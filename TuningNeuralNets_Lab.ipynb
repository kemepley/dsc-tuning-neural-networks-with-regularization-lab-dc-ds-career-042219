{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "data = pd.read_csv('Bank_complaints.csv')\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I have made a payment each month as required t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>This complaint is regarding FedLoan Servicing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I purchased XXXX XXXX, the vehicle was finance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>In the month of XX/XX/2015, my email address (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>the legal dept at lobell is rude the talk over...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                       Consumer complaint narrative\n",
       "0  Bank account or service  I have made a payment each month as required t...\n",
       "1             Student loan  This complaint is regarding FedLoan Servicing,...\n",
       "2            Consumer Loan  I purchased XXXX XXXX, the vehicle was finance...\n",
       "3  Bank account or service  In the month of XX/XX/2015, my email address (...\n",
       "4            Consumer Loan  the legal dept at lobell is rude the talk over..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "sample = data.sample(10000)\n",
    "sample.index = (range(10000))\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "sample_X = sample['Consumer complaint narrative']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sample_X)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(sample_X, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "sample_Y = sample['Product']\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(sample_Y)\n",
    "\n",
    "product_cat = le.transform(sample_Y) \n",
    "\n",
    "product_onehot = to_categorical(product_cat)\n",
    "product_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(units=50, activation='relu'))\n",
    "model.add(layers.Dense(units=25, activation='relu'))\n",
    "model.add(layers.Dense(units=7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:07:24.373028 4761814464 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Yyour code here\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:07:38.866077 4761814464 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0712 15:07:38.876807 4761814464 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0712 15:07:38.934473 4761814464 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0712 15:07:39.055058 4761814464 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0712 15:07:39.106949 4761814464 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.9494 - acc: 0.1508 - val_loss: 1.9367 - val_acc: 0.1540\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9296 - acc: 0.1721 - val_loss: 1.9219 - val_acc: 0.1670\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9136 - acc: 0.1924 - val_loss: 1.9073 - val_acc: 0.1920\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8970 - acc: 0.2104 - val_loss: 1.8907 - val_acc: 0.2000\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8783 - acc: 0.2340 - val_loss: 1.8717 - val_acc: 0.2420\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8570 - acc: 0.2593 - val_loss: 1.8489 - val_acc: 0.2660\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8323 - acc: 0.2833 - val_loss: 1.8226 - val_acc: 0.2960\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8042 - acc: 0.3033 - val_loss: 1.7919 - val_acc: 0.3150\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7725 - acc: 0.3265 - val_loss: 1.7596 - val_acc: 0.3390\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7385 - acc: 0.3541 - val_loss: 1.7235 - val_acc: 0.3570\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7014 - acc: 0.3871 - val_loss: 1.6847 - val_acc: 0.3960\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6613 - acc: 0.4207 - val_loss: 1.6417 - val_acc: 0.4250\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6167 - acc: 0.4565 - val_loss: 1.5935 - val_acc: 0.4700\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5663 - acc: 0.4928 - val_loss: 1.5415 - val_acc: 0.5080\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5134 - acc: 0.5293 - val_loss: 1.4879 - val_acc: 0.5190\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4597 - acc: 0.5508 - val_loss: 1.4360 - val_acc: 0.5420\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4059 - acc: 0.5704 - val_loss: 1.3829 - val_acc: 0.5570\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3521 - acc: 0.5937 - val_loss: 1.3299 - val_acc: 0.5670\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2986 - acc: 0.6085 - val_loss: 1.2791 - val_acc: 0.5990\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2471 - acc: 0.6283 - val_loss: 1.2301 - val_acc: 0.6120\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1973 - acc: 0.6409 - val_loss: 1.1825 - val_acc: 0.6230\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1503 - acc: 0.6525 - val_loss: 1.1399 - val_acc: 0.6290\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1068 - acc: 0.6631 - val_loss: 1.1001 - val_acc: 0.6350\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0666 - acc: 0.6713 - val_loss: 1.0654 - val_acc: 0.6400\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0302 - acc: 0.6819 - val_loss: 1.0331 - val_acc: 0.6540\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9963 - acc: 0.6893 - val_loss: 1.0023 - val_acc: 0.6580\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9655 - acc: 0.6987 - val_loss: 0.9758 - val_acc: 0.6670\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9367 - acc: 0.7049 - val_loss: 0.9511 - val_acc: 0.6750\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9107 - acc: 0.7112 - val_loss: 0.9309 - val_acc: 0.6770\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8864 - acc: 0.7181 - val_loss: 0.9113 - val_acc: 0.6890\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8643 - acc: 0.7204 - val_loss: 0.8901 - val_acc: 0.6840\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8436 - acc: 0.7253 - val_loss: 0.8730 - val_acc: 0.6910\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8247 - acc: 0.7308 - val_loss: 0.8573 - val_acc: 0.6920\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8068 - acc: 0.7367 - val_loss: 0.8437 - val_acc: 0.6850\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7899 - acc: 0.7396 - val_loss: 0.8317 - val_acc: 0.6940\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7745 - acc: 0.7437 - val_loss: 0.8211 - val_acc: 0.6960\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7600 - acc: 0.7463 - val_loss: 0.8070 - val_acc: 0.7000\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7458 - acc: 0.7491 - val_loss: 0.7975 - val_acc: 0.6980\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7329 - acc: 0.7533 - val_loss: 0.7870 - val_acc: 0.7020\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7207 - acc: 0.7567 - val_loss: 0.7776 - val_acc: 0.7070\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7092 - acc: 0.7600 - val_loss: 0.7723 - val_acc: 0.7010\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6983 - acc: 0.7613 - val_loss: 0.7629 - val_acc: 0.7040\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6877 - acc: 0.7635 - val_loss: 0.7555 - val_acc: 0.7090\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6774 - acc: 0.7689 - val_loss: 0.7524 - val_acc: 0.7040\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6682 - acc: 0.7683 - val_loss: 0.7435 - val_acc: 0.7090\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6592 - acc: 0.7728 - val_loss: 0.7359 - val_acc: 0.7090\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.6502 - acc: 0.7744 - val_loss: 0.7315 - val_acc: 0.7100\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6424 - acc: 0.7756 - val_loss: 0.7275 - val_acc: 0.7100\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6341 - acc: 0.7797 - val_loss: 0.7213 - val_acc: 0.7160\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6261 - acc: 0.7828 - val_loss: 0.7162 - val_acc: 0.7120\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6190 - acc: 0.7835 - val_loss: 0.7153 - val_acc: 0.7100\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6119 - acc: 0.7869 - val_loss: 0.7069 - val_acc: 0.7210\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6048 - acc: 0.7904 - val_loss: 0.7031 - val_acc: 0.7150\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5979 - acc: 0.7932 - val_loss: 0.6998 - val_acc: 0.7250\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5917 - acc: 0.7951 - val_loss: 0.6951 - val_acc: 0.7230\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5851 - acc: 0.7967 - val_loss: 0.6976 - val_acc: 0.7200\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5796 - acc: 0.7979 - val_loss: 0.6899 - val_acc: 0.7220\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5731 - acc: 0.8024 - val_loss: 0.6848 - val_acc: 0.7250\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.5673 - acc: 0.8028 - val_loss: 0.6821 - val_acc: 0.7210\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5619 - acc: 0.8047 - val_loss: 0.6812 - val_acc: 0.7280\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5565 - acc: 0.8073 - val_loss: 0.6782 - val_acc: 0.7310\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5514 - acc: 0.8097 - val_loss: 0.6759 - val_acc: 0.7190\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5459 - acc: 0.8115 - val_loss: 0.6745 - val_acc: 0.7240\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5412 - acc: 0.8129 - val_loss: 0.6701 - val_acc: 0.7250\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5358 - acc: 0.8136 - val_loss: 0.6674 - val_acc: 0.7240\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5313 - acc: 0.8157 - val_loss: 0.6668 - val_acc: 0.7280\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5263 - acc: 0.8191 - val_loss: 0.6649 - val_acc: 0.7360\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5217 - acc: 0.8191 - val_loss: 0.6663 - val_acc: 0.7200\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5175 - acc: 0.8192 - val_loss: 0.6599 - val_acc: 0.7330\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5125 - acc: 0.8232 - val_loss: 0.6641 - val_acc: 0.7290\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.5090 - acc: 0.8224 - val_loss: 0.6573 - val_acc: 0.7310\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5037 - acc: 0.8249 - val_loss: 0.6564 - val_acc: 0.7240\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.5003 - acc: 0.8267 - val_loss: 0.6543 - val_acc: 0.7310\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4958 - acc: 0.8285 - val_loss: 0.6529 - val_acc: 0.7310\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4917 - acc: 0.8293 - val_loss: 0.6535 - val_acc: 0.7290\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4878 - acc: 0.8320 - val_loss: 0.6529 - val_acc: 0.7310\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4837 - acc: 0.8325 - val_loss: 0.6518 - val_acc: 0.7280\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4804 - acc: 0.8344 - val_loss: 0.6492 - val_acc: 0.7310\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4756 - acc: 0.8359 - val_loss: 0.6470 - val_acc: 0.7430\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4722 - acc: 0.8368 - val_loss: 0.6439 - val_acc: 0.7350\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4685 - acc: 0.8384 - val_loss: 0.6454 - val_acc: 0.7350\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4649 - acc: 0.8409 - val_loss: 0.6452 - val_acc: 0.7320\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4612 - acc: 0.8404 - val_loss: 0.6451 - val_acc: 0.7420\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4576 - acc: 0.8420 - val_loss: 0.6448 - val_acc: 0.7400\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4545 - acc: 0.8441 - val_loss: 0.6432 - val_acc: 0.7390\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4511 - acc: 0.8456 - val_loss: 0.6423 - val_acc: 0.7340\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4478 - acc: 0.8463 - val_loss: 0.6419 - val_acc: 0.7460\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4443 - acc: 0.8475 - val_loss: 0.6413 - val_acc: 0.7360\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4409 - acc: 0.8485 - val_loss: 0.6421 - val_acc: 0.7380\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4377 - acc: 0.8523 - val_loss: 0.6383 - val_acc: 0.7460\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4344 - acc: 0.8527 - val_loss: 0.6387 - val_acc: 0.7470\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4311 - acc: 0.8529 - val_loss: 0.6367 - val_acc: 0.7470\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4278 - acc: 0.8545 - val_loss: 0.6373 - val_acc: 0.7510\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4245 - acc: 0.8572 - val_loss: 0.6365 - val_acc: 0.7450\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4220 - acc: 0.8564 - val_loss: 0.6374 - val_acc: 0.7420\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4188 - acc: 0.8585 - val_loss: 0.6369 - val_acc: 0.7420\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4158 - acc: 0.8597 - val_loss: 0.6348 - val_acc: 0.7540\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4125 - acc: 0.8609 - val_loss: 0.6328 - val_acc: 0.7540\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4098 - acc: 0.8611 - val_loss: 0.6343 - val_acc: 0.7540\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4068 - acc: 0.8619 - val_loss: 0.6341 - val_acc: 0.7550\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4040 - acc: 0.8627 - val_loss: 0.6334 - val_acc: 0.7550\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4010 - acc: 0.8653 - val_loss: 0.6334 - val_acc: 0.7490\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3981 - acc: 0.8671 - val_loss: 0.6351 - val_acc: 0.7530\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3953 - acc: 0.8669 - val_loss: 0.6345 - val_acc: 0.7510\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3925 - acc: 0.8701 - val_loss: 0.6342 - val_acc: 0.7510\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3899 - acc: 0.8693 - val_loss: 0.6338 - val_acc: 0.7490\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3871 - acc: 0.8725 - val_loss: 0.6354 - val_acc: 0.7460\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.3844 - acc: 0.8707 - val_loss: 0.6341 - val_acc: 0.7480\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3819 - acc: 0.8743 - val_loss: 0.6358 - val_acc: 0.7510\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3792 - acc: 0.8747 - val_loss: 0.6334 - val_acc: 0.7550\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3768 - acc: 0.8757 - val_loss: 0.6323 - val_acc: 0.7540\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3741 - acc: 0.8780 - val_loss: 0.6331 - val_acc: 0.7470\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3715 - acc: 0.8793 - val_loss: 0.6338 - val_acc: 0.7520\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3690 - acc: 0.8804 - val_loss: 0.6354 - val_acc: 0.7520\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.3665 - acc: 0.8797 - val_loss: 0.6367 - val_acc: 0.7470\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.3642 - acc: 0.8811 - val_loss: 0.6379 - val_acc: 0.7470\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3614 - acc: 0.8817 - val_loss: 0.6342 - val_acc: 0.7560\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3592 - acc: 0.8837 - val_loss: 0.6346 - val_acc: 0.7480\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.3564 - acc: 0.8848 - val_loss: 0.6342 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3544 - acc: 0.8856 - val_loss: 0.6361 - val_acc: 0.7550\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 58us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3508249552090963, 0.8862666666348775]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6365994203885397, 0.7706666666666667]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! you remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9426 - acc: 0.1545 - val_loss: 1.9325 - val_acc: 0.1560\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9152 - acc: 0.1999 - val_loss: 1.9095 - val_acc: 0.2060\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8921 - acc: 0.2311 - val_loss: 1.8874 - val_acc: 0.2360\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8673 - acc: 0.2583 - val_loss: 1.8623 - val_acc: 0.2500\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8381 - acc: 0.2848 - val_loss: 1.8325 - val_acc: 0.2680\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8039 - acc: 0.3169 - val_loss: 1.7985 - val_acc: 0.3060\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7652 - acc: 0.3539 - val_loss: 1.7592 - val_acc: 0.3420\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7229 - acc: 0.3915 - val_loss: 1.7170 - val_acc: 0.3800\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6773 - acc: 0.4211 - val_loss: 1.6698 - val_acc: 0.4060\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6285 - acc: 0.4548 - val_loss: 1.6211 - val_acc: 0.4420\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5776 - acc: 0.4864 - val_loss: 1.5700 - val_acc: 0.4720\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5254 - acc: 0.5149 - val_loss: 1.5183 - val_acc: 0.4950\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4723 - acc: 0.5415 - val_loss: 1.4665 - val_acc: 0.5320\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4185 - acc: 0.5704 - val_loss: 1.4142 - val_acc: 0.5550\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3655 - acc: 0.5964 - val_loss: 1.3641 - val_acc: 0.5770\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3141 - acc: 0.6145 - val_loss: 1.3155 - val_acc: 0.6000\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2651 - acc: 0.6260 - val_loss: 1.2702 - val_acc: 0.6160\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2182 - acc: 0.6417 - val_loss: 1.2259 - val_acc: 0.6190\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1744 - acc: 0.6527 - val_loss: 1.1859 - val_acc: 0.6220\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1328 - acc: 0.6688 - val_loss: 1.1480 - val_acc: 0.6370\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0938 - acc: 0.6780 - val_loss: 1.1145 - val_acc: 0.6430\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0572 - acc: 0.6845 - val_loss: 1.0801 - val_acc: 0.6470\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0230 - acc: 0.6952 - val_loss: 1.0491 - val_acc: 0.6560\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9909 - acc: 0.7012 - val_loss: 1.0232 - val_acc: 0.6610\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9610 - acc: 0.7061 - val_loss: 0.9977 - val_acc: 0.6670\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9329 - acc: 0.7151 - val_loss: 0.9735 - val_acc: 0.6700\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9074 - acc: 0.7197 - val_loss: 0.9520 - val_acc: 0.6710\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8833 - acc: 0.7265 - val_loss: 0.9300 - val_acc: 0.6880\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8607 - acc: 0.7313 - val_loss: 0.9126 - val_acc: 0.6890\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8398 - acc: 0.7339 - val_loss: 0.8953 - val_acc: 0.6890\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8202 - acc: 0.7391 - val_loss: 0.8819 - val_acc: 0.6950\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8018 - acc: 0.7440 - val_loss: 0.8672 - val_acc: 0.7010\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7847 - acc: 0.7476 - val_loss: 0.8517 - val_acc: 0.7020\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7690 - acc: 0.7500 - val_loss: 0.8401 - val_acc: 0.7020\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7538 - acc: 0.7548 - val_loss: 0.8279 - val_acc: 0.7030\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7398 - acc: 0.7571 - val_loss: 0.8197 - val_acc: 0.7060\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7264 - acc: 0.7611 - val_loss: 0.8113 - val_acc: 0.7040\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7143 - acc: 0.7640 - val_loss: 0.8007 - val_acc: 0.7040\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7023 - acc: 0.7672 - val_loss: 0.7909 - val_acc: 0.7110\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.6913 - acc: 0.7696 - val_loss: 0.7846 - val_acc: 0.7130\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6801 - acc: 0.7721 - val_loss: 0.7780 - val_acc: 0.7180\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6704 - acc: 0.7729 - val_loss: 0.7697 - val_acc: 0.7190\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6606 - acc: 0.7773 - val_loss: 0.7632 - val_acc: 0.7220\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6519 - acc: 0.7800 - val_loss: 0.7597 - val_acc: 0.7210\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6429 - acc: 0.7812 - val_loss: 0.7562 - val_acc: 0.7220\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6344 - acc: 0.7855 - val_loss: 0.7457 - val_acc: 0.7260\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6264 - acc: 0.7892 - val_loss: 0.7411 - val_acc: 0.7250\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6185 - acc: 0.7889 - val_loss: 0.7365 - val_acc: 0.7250\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6113 - acc: 0.7889 - val_loss: 0.7307 - val_acc: 0.7260\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6037 - acc: 0.7956 - val_loss: 0.7300 - val_acc: 0.7270\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5975 - acc: 0.7943 - val_loss: 0.7257 - val_acc: 0.7250\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.5906 - acc: 0.7967 - val_loss: 0.7203 - val_acc: 0.7280\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5841 - acc: 0.8008 - val_loss: 0.7173 - val_acc: 0.7310\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5779 - acc: 0.8013 - val_loss: 0.7139 - val_acc: 0.7310\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5717 - acc: 0.8043 - val_loss: 0.7121 - val_acc: 0.7300\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5657 - acc: 0.8052 - val_loss: 0.7071 - val_acc: 0.7300\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5598 - acc: 0.8097 - val_loss: 0.7071 - val_acc: 0.7330\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5542 - acc: 0.8123 - val_loss: 0.7022 - val_acc: 0.7380\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5486 - acc: 0.8137 - val_loss: 0.6989 - val_acc: 0.7270\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5436 - acc: 0.8144 - val_loss: 0.6979 - val_acc: 0.7360\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5382511753718058, 0.8168]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.669333552201589, 0.7580000003178914]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=kernel_regulizers.l2(lamda_coeff)` parameter to any model layer. The lambda_coeff parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 2.6002 - acc: 0.1723 - val_loss: 2.5801 - val_acc: 0.2160\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.5714 - acc: 0.2087 - val_loss: 2.5545 - val_acc: 0.2340\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.5456 - acc: 0.2221 - val_loss: 2.5292 - val_acc: 0.2500\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.5187 - acc: 0.2353 - val_loss: 2.5019 - val_acc: 0.2740\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4882 - acc: 0.2547 - val_loss: 2.4698 - val_acc: 0.2950\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4520 - acc: 0.2872 - val_loss: 2.4307 - val_acc: 0.3100\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4097 - acc: 0.3163 - val_loss: 2.3858 - val_acc: 0.3310\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.3634 - acc: 0.3417 - val_loss: 2.3378 - val_acc: 0.3570\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.3132 - acc: 0.3637 - val_loss: 2.2857 - val_acc: 0.3730\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.2612 - acc: 0.3912 - val_loss: 2.2337 - val_acc: 0.3990\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.2088 - acc: 0.4135 - val_loss: 2.1814 - val_acc: 0.4380\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.1566 - acc: 0.4475 - val_loss: 2.1291 - val_acc: 0.4620\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.1049 - acc: 0.4761 - val_loss: 2.0803 - val_acc: 0.5040\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.0546 - acc: 0.5165 - val_loss: 2.0306 - val_acc: 0.5240\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.0052 - acc: 0.5467 - val_loss: 1.9837 - val_acc: 0.5610\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9575 - acc: 0.5692 - val_loss: 1.9380 - val_acc: 0.5850\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9117 - acc: 0.5887 - val_loss: 1.8944 - val_acc: 0.6030\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8677 - acc: 0.6059 - val_loss: 1.8513 - val_acc: 0.6260\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8258 - acc: 0.6245 - val_loss: 1.8122 - val_acc: 0.6320\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7860 - acc: 0.6371 - val_loss: 1.7757 - val_acc: 0.6440\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7482 - acc: 0.6500 - val_loss: 1.7394 - val_acc: 0.6570\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7123 - acc: 0.6597 - val_loss: 1.7071 - val_acc: 0.6620\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6781 - acc: 0.6692 - val_loss: 1.6758 - val_acc: 0.6630\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6460 - acc: 0.6753 - val_loss: 1.6469 - val_acc: 0.6640\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.6154 - acc: 0.6839 - val_loss: 1.6212 - val_acc: 0.6730\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5869 - acc: 0.6924 - val_loss: 1.5929 - val_acc: 0.6810\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5594 - acc: 0.6964 - val_loss: 1.5686 - val_acc: 0.6790\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5337 - acc: 0.7020 - val_loss: 1.5461 - val_acc: 0.6900\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5095 - acc: 0.7087 - val_loss: 1.5257 - val_acc: 0.6960\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4865 - acc: 0.7136 - val_loss: 1.5064 - val_acc: 0.7030\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4645 - acc: 0.7223 - val_loss: 1.4901 - val_acc: 0.7070\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4444 - acc: 0.7240 - val_loss: 1.4705 - val_acc: 0.7090\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4245 - acc: 0.7284 - val_loss: 1.4549 - val_acc: 0.7090\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4065 - acc: 0.7313 - val_loss: 1.4413 - val_acc: 0.7190\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3888 - acc: 0.7373 - val_loss: 1.4284 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3722 - acc: 0.7383 - val_loss: 1.4125 - val_acc: 0.7220\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3565 - acc: 0.7447 - val_loss: 1.3996 - val_acc: 0.7220\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3414 - acc: 0.7496 - val_loss: 1.3872 - val_acc: 0.7290\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3267 - acc: 0.7516 - val_loss: 1.3765 - val_acc: 0.7240\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3128 - acc: 0.7580 - val_loss: 1.3649 - val_acc: 0.7280\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3002 - acc: 0.7587 - val_loss: 1.3549 - val_acc: 0.7260\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2873 - acc: 0.7636 - val_loss: 1.3451 - val_acc: 0.7280\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2754 - acc: 0.7676 - val_loss: 1.3389 - val_acc: 0.7270\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2637 - acc: 0.7695 - val_loss: 1.3276 - val_acc: 0.7300\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2525 - acc: 0.7711 - val_loss: 1.3206 - val_acc: 0.7350\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2413 - acc: 0.7764 - val_loss: 1.3118 - val_acc: 0.7330\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2313 - acc: 0.7785 - val_loss: 1.3038 - val_acc: 0.7360\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2207 - acc: 0.7813 - val_loss: 1.2983 - val_acc: 0.7330\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2115 - acc: 0.7835 - val_loss: 1.2897 - val_acc: 0.7340\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2021 - acc: 0.7848 - val_loss: 1.2841 - val_acc: 0.7360\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1930 - acc: 0.7840 - val_loss: 1.2790 - val_acc: 0.7350\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1842 - acc: 0.7876 - val_loss: 1.2726 - val_acc: 0.7380\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1756 - acc: 0.7907 - val_loss: 1.2633 - val_acc: 0.7390\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1671 - acc: 0.7925 - val_loss: 1.2595 - val_acc: 0.7420\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1591 - acc: 0.7964 - val_loss: 1.2538 - val_acc: 0.7390\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1514 - acc: 0.7943 - val_loss: 1.2490 - val_acc: 0.7400\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1430 - acc: 0.7965 - val_loss: 1.2425 - val_acc: 0.7440\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1361 - acc: 0.8016 - val_loss: 1.2382 - val_acc: 0.7430\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1286 - acc: 0.8041 - val_loss: 1.2314 - val_acc: 0.7430\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1210 - acc: 0.8072 - val_loss: 1.2302 - val_acc: 0.7410\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1141 - acc: 0.8077 - val_loss: 1.2250 - val_acc: 0.7430\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1071 - acc: 0.8096 - val_loss: 1.2191 - val_acc: 0.7500\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1003 - acc: 0.8128 - val_loss: 1.2171 - val_acc: 0.7450\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0939 - acc: 0.8115 - val_loss: 1.2111 - val_acc: 0.7430\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0871 - acc: 0.8137 - val_loss: 1.2051 - val_acc: 0.7500\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0811 - acc: 0.8201 - val_loss: 1.2038 - val_acc: 0.7480\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0744 - acc: 0.8191 - val_loss: 1.1974 - val_acc: 0.7550\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0682 - acc: 0.8232 - val_loss: 1.1948 - val_acc: 0.7540\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0622 - acc: 0.8215 - val_loss: 1.1906 - val_acc: 0.7510\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0562 - acc: 0.8255 - val_loss: 1.1870 - val_acc: 0.7550\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0502 - acc: 0.8257 - val_loss: 1.1825 - val_acc: 0.7500\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0439 - acc: 0.8295 - val_loss: 1.1802 - val_acc: 0.7520\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0393 - acc: 0.8305 - val_loss: 1.1780 - val_acc: 0.7540\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0333 - acc: 0.8292 - val_loss: 1.1721 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0280 - acc: 0.8337 - val_loss: 1.1688 - val_acc: 0.7540\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0224 - acc: 0.8345 - val_loss: 1.1678 - val_acc: 0.7530\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0170 - acc: 0.8363 - val_loss: 1.1616 - val_acc: 0.7550\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0118 - acc: 0.8375 - val_loss: 1.1597 - val_acc: 0.7600\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0065 - acc: 0.8393 - val_loss: 1.1574 - val_acc: 0.7590\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0017 - acc: 0.8409 - val_loss: 1.1519 - val_acc: 0.7550\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9959 - acc: 0.8429 - val_loss: 1.1522 - val_acc: 0.7570\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9913 - acc: 0.8427 - val_loss: 1.1489 - val_acc: 0.7560\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9866 - acc: 0.8448 - val_loss: 1.1464 - val_acc: 0.7570\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9816 - acc: 0.8448 - val_loss: 1.1455 - val_acc: 0.7550\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9769 - acc: 0.8467 - val_loss: 1.1400 - val_acc: 0.7570\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9719 - acc: 0.8480 - val_loss: 1.1376 - val_acc: 0.7590\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9672 - acc: 0.8496 - val_loss: 1.1343 - val_acc: 0.7530\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9626 - acc: 0.8508 - val_loss: 1.1330 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9579 - acc: 0.8516 - val_loss: 1.1315 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9533 - acc: 0.8531 - val_loss: 1.1279 - val_acc: 0.7540\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9489 - acc: 0.8545 - val_loss: 1.1246 - val_acc: 0.7590\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9443 - acc: 0.8545 - val_loss: 1.1219 - val_acc: 0.7590\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9401 - acc: 0.8551 - val_loss: 1.1190 - val_acc: 0.7630\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9360 - acc: 0.8560 - val_loss: 1.1163 - val_acc: 0.7650\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9314 - acc: 0.8583 - val_loss: 1.1133 - val_acc: 0.7600\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9269 - acc: 0.8611 - val_loss: 1.1110 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9228 - acc: 0.8612 - val_loss: 1.1117 - val_acc: 0.7590\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9188 - acc: 0.8607 - val_loss: 1.1068 - val_acc: 0.7620\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9147 - acc: 0.8627 - val_loss: 1.1045 - val_acc: 0.7640\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9101 - acc: 0.8635 - val_loss: 1.1057 - val_acc: 0.7530\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9064 - acc: 0.8635 - val_loss: 1.1007 - val_acc: 0.7650\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9025 - acc: 0.8649 - val_loss: 1.1000 - val_acc: 0.7640\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8982 - acc: 0.8676 - val_loss: 1.0963 - val_acc: 0.7690\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8942 - acc: 0.8668 - val_loss: 1.0943 - val_acc: 0.7650\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8905 - acc: 0.8675 - val_loss: 1.0939 - val_acc: 0.7650\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8867 - acc: 0.8672 - val_loss: 1.0909 - val_acc: 0.7650\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8829 - acc: 0.8687 - val_loss: 1.0885 - val_acc: 0.7630\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8790 - acc: 0.8693 - val_loss: 1.0868 - val_acc: 0.7660\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8756 - acc: 0.8700 - val_loss: 1.0840 - val_acc: 0.7570\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8717 - acc: 0.8721 - val_loss: 1.0826 - val_acc: 0.7670\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8679 - acc: 0.8716 - val_loss: 1.0824 - val_acc: 0.7570\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8643 - acc: 0.8723 - val_loss: 1.0787 - val_acc: 0.7680\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8609 - acc: 0.8733 - val_loss: 1.0819 - val_acc: 0.7610\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8574 - acc: 0.8740 - val_loss: 1.0772 - val_acc: 0.7630\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8535 - acc: 0.8756 - val_loss: 1.0779 - val_acc: 0.7570\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8502 - acc: 0.8751 - val_loss: 1.0739 - val_acc: 0.7660\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8467 - acc: 0.8776 - val_loss: 1.0733 - val_acc: 0.7600\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8433 - acc: 0.8779 - val_loss: 1.0704 - val_acc: 0.7570\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8399 - acc: 0.8781 - val_loss: 1.0660 - val_acc: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8365 - acc: 0.8780 - val_loss: 1.0674 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4FNX6wPHvm96BNNLooYO0JFwQL0hRUKSIDdu1IvZ6r+VnuxbUq/eKDQsqKCrYAREERGlSklBCCQECBNIgISGN1N09vz/OAiEECGXZBM7nefYhM3Nm9p3dZd6Zc86cEaUUhmEYhgHg4uwADMMwjPrDJAXDMAzjMJMUDMMwjMNMUjAMwzAOM0nBMAzDOMwkBcMwDOMwkxTqCRFxFZESEWl+NsvWdyLylYi8aP97gIhsrkvZ03if8+YzM869M/ntNTQmKZwm+wHm0MsmImXVpm861e0ppaxKKT+l1J6zWfZ0iEisiKwVkWIRSRGRwY54n5qUUouVUp3PxrZEZLmI3FZt2w79zC4ENT/TavM7ishsEckVkXwRmScibZ0QonEWmKRwmuwHGD+llB+wB7iq2ryva5YXEbdzH+VpmwTMBgKAK4BM54ZjHI+IuIiIs/8fNwJmAu2BpsB64OdzGUB9/f9VT76fU9Kggm1IROQVEflWRKaLSDFws4j0EZFVIlIgItki8q6IuNvLu4mIEpGW9umv7Mvn2c/YV4pIq1Mta18+TES2iUihiLwnIn/VdsZXjQXYrbSdSqktJ9nX7SIytNq0h/2M8SL7f4ofRGSvfb8Xi0jH42xnsIikVZvuJSLr7fs0HfCstixIRObaz04PiMgvIhJpX/YG0Af4yH7lNrGWz6yx/XPLFZE0EXlaRMS+7C4RWSIib9tj3ikil51g/5+1lykWkc0iMqLG8nvsV1zFIrJJRLrZ57cQkZn2GPaLyDv2+a+IyNRq60eLiKo2vVxEXhaRlcBBoLk95i3299ghInfViOFq+2dZJCKpInKZiIwVkdU1yj0pIj8cb19ro5RapZT6XCmVr5SqAt4GOotIo1o+q34ikln9QCki14rIWvvffxN9lVokIvtE5M3a3vPQb0VEnhGRvcBk+/wRIpJk/96Wi0iXauvEVPs9zRCR7+VI1eVdIrK4Wtmjfi813vu4vz378mO+n1P5PJ3NJAXHGg18gz6T+hZ9sH0YCAYuBoYC95xg/RuB54BA9NXIy6daVkRCge+Af9rfdxcQd5K444H/Hjp41cF0YGy16WFAllJqg316DtAWCAM2AdNOtkER8QRmAZ+j92kWMKpaERf0gaA50AKoAt4BUEo9CawExtuv3B6p5S0mAT5Aa2AgcCdwa7XlfYGNQBD6IPfZCcLdhv4+GwGvAt+ISFP7fowFngVuQl95XQ3kiz6z/RVIBVoCzdDfU13dAtxh32YGsA+40j59N/CeiFxkj6Ev+nN8HGgMXArsxn52L0dX9dxMHb6fk/g7kKGUKqxl2V/o76p/tXk3ov+fALwHvKmUCgCigRMlqCjAD/0buE9EYtG/ibvQ39vnwCz7SYonen8/Rf+efuTo39OpOO5vr5qa30/DoZQyrzN8AWnA4BrzXgH+OMl6TwDf2/92AxTQ0j79FfBRtbIjgE2nUfYOYFm1ZQJkA7cdJ6abgUR0tVEGcJF9/jBg9XHW6QAUAl726W+BZ45TNtgeu2+12F+0/z0YSLP/PRBIB6TauvGHytay3Rggt9r08ur7WP0zA9zRCbpdteX3A7/b/74LSKm2LMC+bnAdfw+bgCvtfy8C7q+lzCXAXsC1lmWvAFOrTUfr/6pH7dvzJ4lhzqH3RSe0N49TbjLwb/vf3YH9gPtxyh71mR6nTHMgC7j2BGVeBz6x/90YKAWi7NMrgOeBoJO8z2CgHPCosS8v1Ci3A52wBwJ7aixbVe23dxewuLbfS83faR1/eyf8furzy1wpOFZ69QkR6SAiv9qrUoqAl9AHyePZW+3vUvRZ0amWjageh9K/2hOduTwMvKuUmos+UC6wn3H2BX6vbQWlVAr6P9+VIuIHDMd+5ie6189/7NUrRegzYzjxfh+KO8Me7yG7D/0hIr4i8qmI7LFv9486bPOQUMC1+vbsf0dWm675ecJxPn8Rua1alUUBOkkeiqUZ+rOpqRk6AVrrGHNNNX9bw0VktehquwLgsjrEAPAF+ioG9AnBt0pXAZ0y+1XpAuAdpdT3Jyj6DTBGdNXpGPTJxqHf5O1AJ2CriMSLyBUn2M4+pVRltekWwJOHvgf75xCO/l4jOPZ3n85pqONv77S2XR+YpOBYNYeg/Rh9Fhmt9OXx8+gzd0fKRl9mAyAiwtEHv5rc0GfRKKVmAU+ik8HNwMQTrHeoCmk0sF4plWaffyv6qmMgunol+lAopxK3XfW62X8BrYA4+2c5sEbZEw3/mwNY0QeR6ts+5QZ1EWkNfAjciz67bQykcGT/0oE2tayaDrQQEddalh1EV20dElZLmeptDN7oapbXgKb2GBbUIQaUUsvt27gY/f2dVtWRiAShfyc/KKXeOFFZpasVs4HLObrqCKXUVqXUDejE/V/gRxHxOt6makyno696Gld7+SilvqP231Ozan/X5TM/5GS/vdpiazBMUji3/NHVLAdFN7aeqD3hbJkD9BSRq+z12A8DISco/z3wooh0tTcGpgCVgDdwvP+coJPCMGAc1f6To/e5AshD/6d7tY5xLwdcROQBe6PftUDPGtstBQ7YD0jP11h/H7q94Bj2M+EfgAki4ie6Uf5RdBXBqfJDHwBy0Tn3LvSVwiGfAv8SkR6itRWRZug2jzx7DD4i4m0/MIPuvdNfRJqJSGPgqZPE4Al42GOwishwYFC15Z8Bd4nIpaIb/qNEpH215dPQie2gUmrVSd7LXUS8qr3c7Q3KC9DVpc+eZP1DpqM/8z5UazcQkVtEJFgpZUP/X1GArY7b/AS4X3SXarF/t1eJiC/69+QqIvfaf09jgF7V1k0CLrL/7r2BF07wPif77TVoJimcW48D/wCK0VcN3zr6DZVS+4Drgf+hD0JtgHXoA3Vt3gC+RHdJzUdfHdyF/k/8q4gEHOd9MtBtEX/j6AbTKeg65ixgM7rOuC5xV6CvOu4GDqAbaGdWK/I/9JVHnn2b82psYiIw1l6N8L9a3uI+dLLbBSxBV6N8WZfYasS5AXgX3d6RjU4Iq6stn47+TL8FioCfgCZKKQu6mq0j+gx3D3CNfbXf0F06N9q3O/skMRSgD7A/o7+za9AnA4eWr0B/ju+iD7R/cvRZ8pdAF+p2lfAJUFbtNdn+fj3Riaf6/TsRJ9jON+gz7IVKqQPV5l8BbBHdY+8t4PoaVUTHpZRajb5i+xD9m9mGvsKt/nsab192HTAX+/8DpVQyMAFYDGwFlp7grU7222vQ5OgqW+N8Z6+uyAKuUUotc3Y8hvPZz6RzgC5KqV3OjudcEZE1wESl1Jn2tjqvmCuFC4CIDBWRRvZuec+h2wzinRyWUX/cD/x1vicE0cOoNLVXH92Jvqpb4Oy46pt6eRegcdb1A75G1ztvBkbZL6eNC5yIZKD72Y90diznQEd0NZ4vujfWGHv1qlGNqT4yDMMwDjPVR4ZhGMZhDa76KDg4WLVs2dLZYRiGYTQoa9as2a+UOlF3dKABJoWWLVuSmJjo7DAMwzAaFBHZffJSpvrIMAzDqMahScHeFXKr6KF6j7krU/TQwYtEZIPoIZVr3oZuGIZhnEMOSwr2m6Q+QA990Al9d2mnGsXeAr5USl2EHhzuNUfFYxiGYZycI68U4oBUpR/SUgnM4Ni+0J3QQwuDvvX+QugrbRiGUW85MilEcvTwsRkcOzpnEnroXNDjkvjbB5g6ioiME5FEEUnMzc11SLCGYRiGY5NCbUMj17xT7gn0aJDr0E9iysQ+bPNRKyn1iVIqRikVExJy0h5VhmEYxmlyZJfUDI4eiTEKPRDbYUqpLPTol9gfzjJG1f4IP8MwDOMccGRSSADa2seqzwRuQD9Q4zARCQby7WOnP41+pqphGMaFraoKsrIgPR2ysyE3F3JyYPhwiIlx6Fs7LCkopSwi8gAwH/3ow8+VUptF5CUgUSk1GxgAvCYiCj1++f2OiscwDMOpKiqgoAAaNQIvLygrg507Yfdu2L8f8vMhJQXi42HjRrAcU5MOTZs6PCk0uAHxYmJilLmj2TCMeisnB5Yvh3XrYPt2SE2FPXv02f4h3t46KdRQ6edNTqcW7O0QRV5YI/KCfSkM9KEisBGVTfwZ0v4KeoT3OK2wRGSNUuqkGaXBDXNhGIbhdLt2wezZ+uy+tFSf6aenQ1oa7NgBgHJ1xdaiOaXNwigaFMeBYF9yPa0U7NtNeW42xZ6wP6IJWcEe/HlwM1keFRR6lqFcUtBPwUU/9LMU3UILNPILPu2kUFcmKRiGYRySlgZr10JgIAQFwapV8N13sHo1tnZtKevaEbVtG34rEg6vUu7hQoG3kBPoSV6QNxtGR7CkmZU/AgspZBf6ia9HBIYG0jmkMz7uPhRVFGGxWRgcdQ+DWw+mU0gnvNy88HTz1P+6euLq4orVZsVis+Dm4vhDtkkKhmFcUCyFB9iy6FvSls7GlpmOt4cvvq5etEjYTlRK1jHld4d48HsHGy32r6XnjLXs94EvBsJXF0FOEw96RcXSonELiiuKKa4sxs/Dj0DvQO70DqZZo2ZEBUQR6htKoHcgob6hhPiEIFJbj/3jc3F1wd3V/Wx9BCdkkoJhGOeNwvJCErMS2VWwi4L9mVTuzcSnoAS/gjICt+wiOmEHnXeW0FVBV8Aq4GpvVk0Mh0+HeZPY3h+vsioCiirIjwrC2v0iogPbkuMbyg7vQJp4N2GgdxDX+QTTMaQjHq4eTt3ns80kBcMw6j2lFLsKdjFv+zyWrv0J67q1ROZVEpVvpVWB0KoQQgstSKWFTgr+VgG+VUdvwyawtVUAC66Pwa/fQLpedjONo7sAUF5ZSg93L2JcXJ2wd/WLSQqGYThNpbWS9MJ0rMoKQIWlgsLSfKxbkvH8YynBy9bgnbmPdM9y9rlXMng/3J93ZH2ri3Ag2I99Id7sivAiICCYYP+mWEKiqApvjntYBISGQmgoLm3a0DEoiI61xOHl6XtudrgBMEnBMAyHsdqsZBVnsbdkLzkHc8gqzmLPrvVE/LoUn13pWIqL8K5UeFrBywIRxdBrP3jbu+inBMH6SC9aWhvTttwVz9hO0H8wxMZCmza4RkUR7OZGsHN387xikoJhGGdEKcW+g/tIzU8lNT+V5NxkNuduZmtuCuVZe4jMt9C8EJoVQkwW/F+KTgAHfdyxevuBjzc2Tw+sHu5URTdi35XRWDt1wHPQZbTsHEsHNy9n7+IFxSQFwzBqZVM2bMqGm4sbldZK1u9dz8r0lWSXZAO6qmdrZhJqTSKtdxfTYT+0zYNupUJohRvBJTY8K61HbdPapDEy7kYYNw7fbt2csVvGSZikYBgGZVVlpOxPYXPuZpL2JpGQlUBiViKW0oP8PcOFXpmKMldFoRcEV7rSKUfRZZ+ieza4W3X3HYuvN5boNnh0aIFLcDAEB0PLlvrVvDk0a4Zr48Zwit0xjXPLJAXDuIAcrDzI5tzN7MjfQVpBGlvztrImK5G8tC1EFNgIL4HWxa48WBJEt9xGtNhegXtlzTF4rBASAp07w41x0LcvxMTgFhGBmzngN3gmKRjGeWp73namrJ9CelE6+4v3cTBtO/n70mhSBl1yoGc2XJ7rTvv9Ct8yW7U1rdCoAjpHw33XwZAh+sCvFBQWgo+P7tFjnJdMUjCMBqrKWsVvqb+RtC+JjKIM8sryaBbQjE4uTdm5dhFJm36nRaFwQ7oXl2yrIKD06Pp9FRiIdO8OV3WA9u2hRQsID4eoKP1vbWf9TZqco70znMUkBcNoAGzKxrLdy8gszqTcUs62vG18kfQFrpl7icmCPvk+jNkntNtTSouC6iMfK4hoDDcMhd69oXFjPXRzhw5I8+amft84hkkKhlGP2JSN+anzmbZhGk28mtAtrBslFcWsmfE/Ll+ShZcFSn2hpYKETF+isu0rShlER6Mu70lhl7Z4dOiMd2QLCAvTDb3m4G/UkUkKhuEkSilmbJrBK8teoXmxK9dtVvil7qGwvIgr3TzBasNmqaJvNjyWCxUBvhAailv2AVwsVuSSS+Cfg3V9f5cu4OuLAI2cvWNGg2aSgmGca6WlZO/ezKfTHsW64i++yPKlZ+pBXBTkNXLH2zMQb3dvcHXF4gKWlkHwn4fwvP56/XAWw3AgkxQMwwGU1cr2+Hms++0LChKW0DK7jA77ISynDM9KK+HAc4DNRZCu0cgLo2HsWILatTtqO+72l2GcKw5NCiIyFHgH/YzmT5VSr9dY3hz4AmhsL/OUUmquI2MyjLPJpmysTF9JYlYismsX4X9toGl8Mp2Sc2h3UNEOsLrA3qZ+bA92YV5rL7zCowhr0Zkuva8iavDVEBDg7N0wjMMclhRExBX4ABiCfphcgojMVkolVyv2LPCdUupDEekEzAVaOiomwzhjlZUUbd1A4rYlrN+2lNyEP4lOK2bYbmiXr4tkN3FnW1wbNl/ch86X3URwbH8ivbyIBAY4M3bDqANHXinEAalKqZ0AIjIDGAlUTwoKOHSa1Ag49rFHhlEfFBeT9daLeL37AYEFFQwEBtoXlTfxR3r3xjrsSlyHXkF427aEm94+RgPlyKQQCaRXm84Aetco8yKwQEQeBHyBwbVtSETGAeMAmjdvftYDNYxD1qStZOOS7+me50b7fTZkRyql2zbjlZpGRJmFP6PdyLv7Sjq16UPbFj1w79wVr6go0+XTOG84MinU9r9E1ZgeC0xVSv1XRPoA00Ski1LKdtRKSn0CfAIQExNTcxuGcdpKi/LYPfMLKn6ZiUf8GrpkltLLfuOvRSCtMexqAru7ulFx8y3ceNc7NPE2d/Ua5y9HJoUMoFm16SiOrR66ExgKoJRaKSJeQDCQ48C4jAtcRt4u1k59Db9vZ/K39bl0rIKD7rCxlS+bxw6m1aWj2R7uyZ8embh4eXNJi0u4NbznefcsXsOojSOTQgLQVkRaAZnADcCNNcrsAQYBU0WkI+AF5DowJuMCoJQiuySb1PxUdhfsxtfDlxDvYEpWLqF86mQuXp7OiFI44OvKxqE9sY24iqirbqJ3aDRirwaKs78M40LjsKSglLKIyAPAfHR308+VUptF5CUgUSk1G3gcmCwij6Krlm5TSpnqIeOUKKVYt3cdv6X+xor0FaxKX0nr1Hyu3gJXbIfwEmhSBm4KKlxhx8Udqbr7QSKvu5PeHubs3zCqc+h9CvZ7DubWmPd8tb+TgYsdGYNx/lFKsTl3M3/t+YsVGStYuGMh2SXZ+FbAk6lN+fQvC2FZYHNzpaxvHGWtotjj44a1VQva3PkEnQKDnL0LhlFvmTuajQZlbfZaHp//GAdWLaFrDsQUeDO+LIi2hS0J3JODy8F9EBcHr4zHZeRIfAMD8QXzYHfDqCOTFIx6SylF2oFdbNm0mKSyXSTkbcDzp9m8/5crnffay7hbkNa+0KoVDLwSbr4Z/vY35wZuNAgWm4UKSwW+Hr7ODqVeMUnBqFcsNgu/bP2FyWsnY/1rGS/PKuGKLLgCsLiAmw2sHaLh5Sfg4ouR6GhwN6MDna9syoZSClcX1zPeVqW1Eg9XD6w2K9M3TeeFxS+QXpjONZ2u4f7Y++kc2vmYdQI8A3ARl8OxrMlaQ1RAFOH+4YfLzNs+j30H93Fb99sOz3tv9XvM3jabSVdMom1Q2xPGtDJ9Jf2a9zsr+3g2mKRg1AtlVWV8vOoDfp39Fq1T9vHQbm+GbizjYHAj0p++mVC/UDyLSqF3b1xHjgQXF2eHbDjIjvwdfJH0BSszVpKQmYCriytPXfwUD8Q9wJrsNTyz6Bk2527mhf4vcH/s/bi6uFJSWcLitMXEZ8azJnsNg1oN4rE+jwH6RGP0t6OZs20OwT7BeLp6klmcSbem3bir5118vfFrpm+aXmssAZ4BxEbEEuEfwYIdC9h3cB9NvJow76Z59I7qzcyUmVzz3TVYlZW80jwe7/s4MzbN4KHfHsJFXOj5SU8+vPJDeoX3Ij4znqKKIu7ocQe+Hr6UVpVy9bdXM3/HfOIi45h81WQuanrRMTGUVpWSWZRJZnEm0YHRRAVEOfTzl4bW2ScmJkYlJiY6OwzjDJVUlrAldRWes3/F99eFuKSkEJVvxd1+26IKCUHGjYOnngI/P+cGex6qsFTw8ZqP+TPtT5r6NiUqIIrBrQfTO7L34W65p6LcUk7S3iQ25WxicOvBtGjcok7r5R7M5bvN31FuKUeh+DPtT+Ztn4eLuNAtrBtxEXHsKtjF/B3zaeTZiMKKQsL8wmgf1J4lu5cQGxFLqG8ov+/8nQprBS7iQqR/JOlF6Xx45YeMjxnPPxf8k7dWvsU9ve4BIK8sj2s6XsO1na/FRVwoqSzh5y0/k1eWd1RsNmVje952ErISSCtI49JWlzK0zVAmLJ9AzsEcnun3DC8ueZEeYT2ICojixy0/8nDvh/ko8SPiIuP4fOTn3D7rdpbvWX7Udls0asF/L/sv76x+h+V7lvNg3INM3zSdA+UHuK3bbVzT6Rr+FvU3ftzyI7/NfAtLyhbmtIMqN5h0xSTujb33lL8fABFZo5SKOWk5kxSMc6qsjDWTX2Lvp28zKLkCLyvsbAy72gYRHTuMFn0u120CbdpccENH7DywkzVZawj3DycqIIoWjVocPkBbbBaWpC2hXVA7mjVqdsy6SikyizNJL0wnq1jfIxrhH0GEfwTh/uF4uHqglCI1P5VFuxbx2vLX2FO4hzZN2lBYUcj+0v0A9ArvxegOo9mat5WErAQsNguR/pEE+QSRezCXzOJMvN28iY2MpUtIF3Yc2EF8Zjwb9m2gylYFgI+7Dy8NeInbe9zO1xu+ZtqGaXRr2o3XB79OkI/u+VVYXsh/V/6Xt1e9TUllyeH9CPML455e9zCu1zgi/CMOz1+6eykfJHxAz7CePNj7QbzdvJmxaQbP/PoYoeWuDIsexpA2Q+gR1gMPV3cenv4Pdmxexk2hg/l9x0IGtRrIrTe+ATEnPSaeVFZxFkOmDSE5N5luTbuxtPu7eO/O5Jnk9/i2aCVerduy8s6VBPkEYbFZ+DLpS1zFlbjIOHJLcxk/Zzxb9m/BzcWNr0Z/xfVdrievNI+5/xrNiqzVfHRRJQhE50HC5640PmilLKgR2TeOwP/Bxwlp2+204jZJwahXClOSKHjj34R8Nwef0ipyA9woHHk5ZdeOxqdvf1oHtjmtM9T6TCl1wn06tFwpxdT1U3lg3gOUVpUeXt6iUQtGdRhFoHcgk9dOJqMoAw9XD+6NuZd7Y+4lNT+V+Mx44rPiSchMOOZMt7pQ31CqrFUcKD8AQGxELK8Neo1BrQcBUFRRxNcbvub9hPdJzk0mzC+MuMg4vN28ySzOJK80jxDfECL9IymqKCI+M57c0lz8PfyJjYwlNiKW3pG9adG4BS8sfoE52+YgCApF19CuJOcmE+gdyEO9HyI+M54FOxZQYa3g2k7X8kL/F2jeSI9p5uPug6tNQVKSfpqcp+fxP+CMDFSfPkhGRt2/lLvvhjfegF274P33YetW6NkTYmMhMFCX8fGBHj2gyfGHM9mfk8ayd59g+O+7cV999PGoZMwI/D6fpodEz8+Hjz6Cffv0wsBAKu66nQ/Tf+KiphcxsNVAUAqeeQZe108W2H39UL65ui0PP/kT3iUVyNtvw4wZMHcuvPce3H9/3fe3GpMUDKdTmZls/uJN8r//kouTDmAT+KGLUH7Ljdz00GQ8PM7Pp4gdrDzIQ/MeOlyd8HjfxwnwPPLMhOTcZG75+RZ2HthJbEQs7q7uzN0+l4GtBjJh4AQKygvYeWAnc1PnsnDHQiqsFQxpPYQ7e9zJwp0LmbJ+Cjb78GAu4kKnkE70juxNr/BetGjcgkj/SAAyizPJLMokqziLzOJMXMSF2IhY4iLj6BLapdaEpZQivyyfQO/Akya03NJcgn2CDzfEVl/205afWJG+grFdxxITEUPS3iTGzRlHfGY8zRs1Z1T7Udze6Sa6q1B9wLTZ9MHxzz/1QTQjAzp0gE8+gUsuOTaAigro3x82b9YH05o3Ifr7s7+JF5MzZ3Nn7D2E+gTr7b79tk40paX64N+tG2zYAAcPHvsebdvCoEEwahT06wcpKbB6NcybBwsX6hjattUH6f79Ye9eWLZMJ52WLeG662DSJCgqgkb2h6QWFemn5z3yCFxxhb4a/uorXe6ee3Qiev11HZvVqj+PPn30ujt3QkgI+Psf93s5kbomBZRSDerVq1cvZdRTVqtSv/+u1KOPqtKObZXS/81VehNX9dfYfmrp8m/UgbIDzo7ylOWU5Kip66aqH5N/VKvSV6mt+7eqbfu3qdS8VFVpqTxczmazqbVZa1XH9zsqeVFUv8/7KV5EBb0RpB6c+6CaljRNfRD/gfJ+xVuF/CdE3TnrTtXjox6q0WuN1MtLXlYWq+WY9y4qL1IZhRlHzUvJTVEfxH+glqQtUcUVxWdnJ4uL9fd3tkycqFSPHkqlpR2eZbFaVNqBNGXLzFTqkksO/z6OeQ0ZotS77yrVsqWeHjtWqV9+Uaqs7Mj2x43Ty3788dTiWrdOqZtv1vEdsP8WLRalkpOVSkjQrwULlJowQakRI5Ty9T02vubNlXr4YaUWL679M1u2TKlmzXTZkSOV2rjxyLKtW5W6/vpjt/n440rZbLrM1KlKhYYq9f33p7ZvJ4EeSeKkx1hzpWCcucJCmD4d3nkHUlKwuLuxuJmVlR18iL7pIa6+9nk83b2cHeUxrDYrybnJzEyZya/bfyXIJ4hR7UcxsNVAiiqKSC9K58ctPzJj0wwqrZW1bsPLzYseYT0I8gliTdYaskuyCfML46vRXzGo9SDWZK3h30v+zaJdiw5XDQ1sNZCvRn91VLdGp9q2TbfjdOkCP/0Ewce51c9qhcceg1WrdN18bKy+PyQiApo3P1LVs2ABDB2qD3dt2uiz53D7vq5bByNGwIED8Oij+ow6LAzc7B0h27SB6Gj998GD8NJL+gy/qEiNxBGdAAAgAElEQVSfPYeE6KuK9HR4+mmYMMGhHw1lZbBoESQmQufO+sbI5s1P3t5VWKivdjof280V0Fcdu3frvxs31tutvk2lznqbmqk+MhyrqgqmToXvv8e2+E9cqiyktPRnUl83Jrc6wNU9b+Tdoe8eblh0BKUU2/O34+nqWWtvl6KKIjblbMLfw5/IgEgKygv4ecvPzNk+h+1529lbshersiIIvaN6s69kH7sKdh21DT8PP/7R7R/c0eMOBCGzOJPC8kJA9zHflLOJ+Kx48krz6BXRi7iIOG7ocgMhviFHbcdis7AldwvZJdkMajWo9j7phYW6WiP8HCaL4mKdELKz9XtHRsKcOdCx49HllIJx4+DTT3UySEnR6x4SGKgP0ldeqataIiNh4kQYOVIfRO+5BxISdNIJCoJfftFVN3VRWamrUebO1Z8RQOvW8H//B671o29/Q2CSguE4f/0F48fDpk1khfvzVatiFvcKpLh7R8L9I7ix642M6jDqrL1dlbWKqeun8try11Ao4iLjCPcLZ17qPLblbQOge1h3Lm9zOWVVZWQWZ7Jl/xa25G5BHfMID+jWtBs9wnsQ6R9J6yatGRY9jHD/cJRSbNi3gdWZqwn2CSbSP5JOIZ3w9zy9OtxTsnatPoOurNSNn4caOXfvhuXLdb2271m+81YpXe/900+6jtzHRx/Ec3KOnKW2a6ffOz8fJk+GZ5+Fl1/WZ+upqbBnD2Rm6ivF+fP1Oo0b6zPrNm1g8WIYNgzKy/UVQf/+OlmEhZ3dfTFOyiQF4+zbu5fKJ5/A48uvyQ3y5v6hVmZ1gGcveZZ/XfwvPN1q7ymSmJWIl5sXXUK7HJ5XZa2iylaFj7vP4XmV1kqKKooI8g5CRNh1YBc/p/zMR4kfsT1/O70je9O8UXPiM+PJLM5kQMsBjGo/inJLOT+n/MyK9BX4efgRGRBJdGA0cRFxdA/rTpmljMyiTNxc3BjebjitmrQ6u5+L1aoP3D//rA+MDz4I991X9/V//lkPz9G4sW6svO8+3cuktFSflScn64bKf/xDV7PEx+uz9IkTYcAAfXD/5BN45RV98AUIDdVVEl26wPbteh2LBa66Sh+kN26E77/XZ+BvvglPPKHX27MHvvhCXwnabLphdfFive4jj8D//nf8ao0lS3RMDzygG2gP2bdPby8y8oLrZlyfmKRgnD1lZRS9+Qqer7+JVFYxsTdMHh7G4K6jeORvj9A+uH2tq1lsFl5c/CITlk1Aobihyw3cH3s/v277lU/XfUp+WT6dQzrTtWlXdh7YybrsdVRYK/Bw9SDIO4jskmxA951/vv/zXNXuqsM9Yqw26zFVMFXWKtxdT3PIi127dL11585H6rdrk5KiD5adOunpPXtg7FhYsULXqUdFQVqaPvO+9FJdxmrV1TNZWfqMOzRUVxGtWqW7Rf7xB/TuDTNn6rPwjz7SVw6TJumD/bvvwsqV+iDu6qq7UObk6JifeUbH9MMP8Pe/6ySglI4rPh5yc3VCiY3VB/Zly3Q8oHvO3H67vkHwRAfrAwd0ErnkEnNQb8BMUjDO2KwN35Pz3muM+H4DTQutzG4Pfz5wJdePerbWO1+VUkzfNJ3tedsBWLhzIX+l/8Xt3W8n3C+ciasnUlpViou4MLL9SLqEdiExK5GNORtp3aQ1cRFxRAZEkl2czd6De7ko9CJGdxxN6yatz+6OpafrA/OhhtGFC2H4cF114+2tG1GHD4fRo3Vf8/h4fSUwa5au2gHo21fXn7/5pj7I/u9/cMMN+oDcu7c+GK9apa8cXnlFJ4XaNG8O994LDz+s3zs/X1fZ+PrqA/uTTx7uv05xMXh56bGeSkr0GfkXX+gkNmECPP740cN/KAX79+s6/EPz8/L0mX/HjvplDvIXDJMUjNNTUACTJpE28wuCkrbhXwnJHYJIGD+CS2599vAB+mDlQZ76/SliI2O5qetNWGwWxs0Zx5dJXx7eVGOvxnxwxQfc2FU/cG9fyT7mbp/LoNaDDt+sdM4tWqR7xrRqBR9/rA+wl12me7z861+wZo2uBlm37uj13Nx0Vc2oUTp5TJqk69R79dI3Fh3qMQM6ccTF6d4zVqtueL3xRn0VERioE0ZmJjRrpvuq17wy+fRTfZNV7976zP5EA/7NmaN7//TsedY+IuP8ZJKCcepWrkSNHYvas5uNoZDdrQ0DH30Hj8uvOOqMstxSzvBvhrNo1yIAOod0xt/Tn1UZq3ix/4s81/85BF3+nN+lnJ2tD+7Z2frAOnr0kRubNm/WZ/gREfrAvnOnPvNu3hyWLoWmTY9sZ/dumD1b14XHxek7XKs39NpsutomOvrYG6cAfvtN168//LBOQqfyOdhs8Nln+mrlXPZEMs5rJikYdWexoF57DfXvF8lq7MqY0VX0GnUv7w17r9Z6+zHfjeGXbb8wZeQUfN19ee7P50grSGPqqKnc0OWGcxNzZaU+8Gdm6gZZ0A2yzz+vG1vDw3XdfliYbvSMidF3s1ZW6sbT4GBdf79sme450+zY8YQM43xSL+5oBoYCW4FU4Klalr8NrLe/tgEFJ9umuaP5LEtKUmXdOisF6psuqJ7/aaNmbJyhqixVanbKbHXfnPtU7CexyuNlD8WLHH59EP/B4U1UWatUXmle3d/TZlPqzz/1q6aDB5X6z3/0XaOjRum7QSsrlfr44yN3iZ7oNXCgUtu26TtN585VaswYpcLD9TIfH6USE8/4IzOMhghn39EsIq72A/0QIANIAMYq/Vzm2so/CPRQSt1xou2aK4WzpLISNWEC6pVX2O9l5Z+jfPj7Y+9yXefr+DDxQyYlTGJ34W78PPyIjYilZ3hP/Dz0ENbdmnZjdMfRp/e+K1boHjNLlujp0aN198uCAt01c9IkfQXw97/D+vW6cTU8XPfc6dMHhgzRVTFubnp+RMSRsWC8vXXdem1VNZmZuqooyDyf2bgw1fVKwZEP2YkDUpVSO+0BzQBGArUmBWAs8IID4zEOWbuW8lvG4pW8jW+6wg/39OO9W6ZTVFFE38/7silnE5e2vJT/Xf4/RrQfgZvLKf5MbDbdo2fxYt3FMyZGN9x+8IG+8a1pU50ISkrg3/+GFi2OdJMcMAC+/VZ3f8zL04OLrV+vG4WvvPL0e8tERp7eeoZxgXFkUogE0qtNZwC9aysoIi2AVsAfx1k+DhgH0Ly5k3qtnCeyvvuMkJvvId/LyhO3+tLvgf/wQ697+GzdZzzy2yP4efgx76Z5DI0eemobLinRfev/+gumTNE3TNUUHa3r9e+++0ij7bXX6gTRoYO+m7Z6w2pQEPznP6e/s4ZhnDJHJoXaTumOV1d1A/CDUspa20Kl1CfAJ6Crj85OeBeWDfs28PObd/DUxDVsCoWv3ryVd0f/l7zSPAZ9OYglu5cwqNUgpo2eVvtAbbt36y6YmZn6rPvQHatK6aEPXn9dXyGAruZ58UXdfXPHDj3mTWSkrvqp+RjNNm10Lx3DMOqHujQ8nM4L6APMrzb9NPD0ccquA/rWZbumofnULdyxUN1+vZeqcEVltI9Q2buTlVJKfbfpO+Xxsodq/HpjNXnNZGW1WZXau1epf/1LqfXr9coVFXpY35oNunffrYdbvvdePX3TTUr9+qtSOTlO3FPDMI6HOjY0OzIpuAE70dVCHkAS0LmWcu2BNOzdY0/2Mknh1Exf86V6u4+LUqDK+8QdHkN++e7lyvNlT9X3s74quzhbF66oUOrii/XPQkSPYx8bq6fvuUepP/5QKiVFqaef1ssbNdLLnnzyyFjwhmHUS3VNCg6rPlJKWUTkAWA+4Ap8rpTaLCIv2YObbS86FphhD9o4i95b9Bpd7nqGG9Ig947rmX13fzqXpOBX6cfIGSNp3qg5s2+YfWR468ce020CH3+s+/i/846+m/aHH2DMmCMbnjBBj+vzwAN63JynnnLG7hmG4QDm5rXzkE3ZeHru4wx8eCKDdwmbXnuMAeozCsoLDpcJ8g5i1V2riG7SRj8MZPp0Pc7O44/DW2/pQvv368qikJDjvJNhGA1FfeiSajhBZlEm9/0ynmvfmMPlO2Dxc7cwpOId2ge1Z8HNC8goymBjzkZGdxhN9JwV8NTfjwzWNmTIkcHX4PhP4DIM47xlksJ5QinFp2s/5V/zH+eNWaXcvBH+uGsQg1y/ZFjrYcy4ZgYBngHERsYyuv1I/dSq11+Hiy/WN5PFxekbv040bLRhGOc9cwQ4Dyil+OfCf/Lhkv+ycG4QfdcX88d1cQyKXMSt3W7lsxGfHbkBrbwcbrpJP23rnnv0PQInGoXTMIwLikkKDVzS3iQem/8Y65L/YNUMLzql5/HitSH8u1M898Xcx3tXvIeL2O8NKCnR9w4sWqTH/3/kETOevmEYRzFJoYHaun8rz/35HN8nf493Jfz+DbTLLufJ+9ux65IuvNtiAA/EPXBk6OrcXJ0QVq3SD2a59Vbn7oBhGPWSSQoN0KyUWYz5bgwu4oKrFf5a2IzumRnId9/x1jXXHF04KUl3LZ0+XY8v9P33cPXVzgncMIx6zySFBmZP4R5un3U7ob6hZBdnszShEz0SkvVgczUTwurVuiHZ01M/9P3BB/UAdYZhGMdhkkIDYrFZuOmnmyitKuVA+QG+yu7DJb+t1DeP3Xff0YXLynQiiIzUj5g03UsNw6gDkxQakJeXvMzyPctxd3Hnn2U9ufGzeD2y6KuvHlv4//5PPyv4999NQjAMo85MUmgg5m2fx8tLX6aRZyPaFbrx+ic7kQ4dYNq0Y0cenTdPjzz6wANHRjM1DMOoA5eTFzGcLTU/lRt/upFA70DKDhby+5xAXBT6wfKHnjoGsGmTfpLZFVdAu3ZH351sGIZRByYp1HMllSVc/e3VWKwW8sryWLyxFwGbtsPUqdC6tb1QCTz6KHTrBn/8oR9In5h45EE2hmEYdWSqj+qxBTsWMH7OeHYX7sbdxZ3ncjrSZ9YanQBGjtSF5szRjcwZGTB+vE4I5jnEhmGcJnOlUA8ppRg/ZzyXf3U5bi5uNAtoRrhLAC98u0+PUfT663oQu+uug6uugoAAPeT1pEkmIRiGcUZMUqiHErIS+HjNx9wbcy8j249kT+Ee5peOwTUvXzcgr14NHTvqNoVXX9XPRu7Tx9lhG4ZxHjDVR/XQpIRJ+Hn48UTfJ7jow4u4pd21tHt8ln6wTWCgTgBhYfDLL9C2rbPDNQzjPGKuFOqZvNI8Zmyawa0X3cqkhEmUWcp4I7Ojri66/37ds8jNDebONQnBMIyzzlwp1DNT1k+hwlrBNZ2u4YpvruDWjmMJ+9cXEBsLb78NWVmwePGRnkeGYRhnkUOvFERkqIhsFZFUEan1Qb4icp2IJIvIZhH5xpHx1Hc2ZePDxA+5pPklzEyZSZW1ijcyOujnJbdsqRuTP/8cevd2dqiGYZynHJYURMQV+AAYBnQCxopIpxpl2gJPAxcrpToDjzgqnoZgwY4F7Dywk2s7XcvHaz7moWbXEvrSf6FrV/jhB7jrLhg71tlhGoZxHnPklUIckKqU2qmUqgRmACNrlLkb+EApdQBAKZXjwHjqNaUUry57lXC/cFakr0BEeGVWEZSWwt690KGDHgLbMAzDgRyZFCKB9GrTGfZ51bUD2onIXyKySkSG1rYhERknIokikpibm+ugcJ1rXuo8lu9Zzi3dbmHG5hl8Yr0Sn1lzoV8/yMuDGTPAx8fZYRqGcZ5zZFKo7TmPqsa0G9AWGACMBT4VkcbHrKTUJ0qpGKVUTEhIyFkP1NlsysYzi56hdZPWrMpYRYRHMDd9slIPW1FQoLugXnSRs8M0DOMC4MikkAE0qzYdBWTVUmaWUqpKKbUL2IpOEheU7zZ/R9K+JEZ3GM3S3Uv53HYVLplZ8MQT+sa04cOdHaJhGBcIRyaFBKCtiLQSEQ/gBmB2jTIzgUsBRCQYXZ2004Ex1TsWm4Xn/nyOrqFdWbRzEW2atGHIgh3QqpVuTwCTFAzDOGcclhSUUhbgAWA+sAX4Tim1WUReEpER9mLzgTwRSQb+BP6plMpzVEz10a/bfiU1P5Xh7Yazft96/hN5Oy5Ll8I99+gb1Fq0MI/QNAzjnBGlalbz128xMTEqMTHR2WGcNcO/Gc7a7LU0C2hGTmkOqTuvwvXDjyA1VY9vdPvt8P77zg7TMIwGTkTWKKViTlbODHPhROmF6cxLnceAlgOIz4rn2V6P4frlNBgzBpKTdfWRqToyDOMcMknBiT5f9zk2ZSM1P5WogChuTfHUvY3Gj9fPSfDxgQEDnB2mYRgXEJMUnMRqs/LZus+Ii4gjISuBf/7tcdz/+7a+e7lHD5g5E4YMAS8vZ4dqGMYFxCQFJ5m/Yz7pRel4unkS4BnA3buDISVFd0O96ip9F/ODDzo7TMMwLjAmKTjJtA3TCPQOZFXGKm676B94v/4WREfDN9/AsmUwbRoMGuTsMA3DuMCYpOAESikW7VxE84DmVNmq+OeBjpCUpLufzp8Pkyebge8Mw3AK8zwFJ9icu5nc0lyqbFVc1noIUe9OhfBwWLQIHn4Y7rzT2SEahnGBMlcKTvDHrj8AKCgv4AU1AOLj9YLmzeGVV5wXmGEYF7w6XSmISBsgQylVISIDgIuAL5VSBY4M7nz1x64/8HbzJsQ3hD4/rNJdT7Oz4ddfwc/P2eEZhnEBq+uVwo+AVUSigc+AVsAF/ZS002W1WVmctphySzn3NhqM/PILVFTA9dfr5y8bhmE4UV2Tgs0+ltFoYKJS6lEg3HFhnb/W711PYUUhCsU/FheAqytYrfDqq84OzTAMo84NzVUiMhb4B3CVfZ67Y0I6vx1qT4hWgYR9Pw88PKB/f2jTxsmRGYZh1P1K4XagD/CqUmqXiLQCvnJcWOev33f+jiC8nhaNlJVBWRncd5+zwzIMwwDqeKWglEoGHgIQkSaAv1LqdUcGdj6qslaxdM9SFIpB6wogIAAaNzZtCYZh1Bt1ulIQkcUiEiAigUASMEVE/ufY0M4/v+/8nXJLOU2rPGm0cTsUFennJri6Ojs0wzAMoO7VR42UUkXA1cAUpVQvYLDjwjo/vbb8NVzEhUcreyFKgZubuVHNMIx6pa5JwU1EwoHrgDkOjOe8tXzPcpbtWYZN2bh9kf3hco88Ak2bOjcwwzCMauqaFF5CPzpzh1IqQURaA9sdF9b557Xlr+Hr7suoLRCydiuEhsIbbzg7LMMwjKPUKSkopb5XSl2klLrXPr1TKTXmZOuJyFAR2SoiqSLyVC3LbxORXBFZb3/ddeq7UP+t37ueudvnEmtpyvSfBAE9LLaLGWXEMIz6pa4NzVEi8rOI5IjIPhH5UUSiTrKOK/ABMAzoBIwVkU61FP1WKdXd/vr0lPegAXhrxVv4e/jTfX02XlX2Z2JffrlzgzIMw6hFXU9VpwCzgQggEvjFPu9E4oBU+1VFJTADGHm6gTZUSil+S/2Nga0GEre9DIuXh+6K2rOns0MzDMM4Rl2TQohSaopSymJ/TQVCTrJOJJBebTrDPq+mMSKyQUR+EJFmtW1IRMaJSKKIJObm5tYx5Pphe/528sryCPDwZ0AauLi4wqWXmm6ohmHUS3VNCvtF5GYRcbW/bgbyTrKO1DJP1Zj+BWiplLoI+B34orYNKaU+UUrFKKViQkJOlovqlxXpKwDw2ZVBeAm4lJaZJ6oZhlFv1TUp3IHujroXyAauQQ99cSIZQPUz/yggq3oBpVSeUqrCPjkZ6FXHeBqMFekraOzVmMDVG47MNEnBMIx6qq69j/YopUYopUKUUqFKqVHoG9lOJAFoKyKtRMQDuAHdLnGY/d6HQ0YAW04h9gZhRfoKeoT1oOuWfKo83SEsDDp2dHZYhmEYtTqTPpGPnWihfajtB9D3N2wBvlNKbRaRl0RkhL3YQyKyWUSS0GMr3XYG8dQ7BeUFbM7dTLB3EAPSwBXRVwlSW82aYRiG853JM5pPemRTSs0F5taY93y1v58Gnj6DGOq1VRmrAAjOyCe8BKDSVB0ZhlGvncmVQs1GY6OGFekrcBEXguI3HZlpkoJhGPXYCa8URKSY2g/+Ang7JKLzyIr0FbQLakenzSlYPNxwa9YCmjd3dliGYRjHdcKkoJTyP1eBnG8sNgurM1fTIagDfdPB1QYMGeLssAzDME7oTNoUjBPYlLOJksoSKjP30KIQwGKqjgzDqPfMiGwO8vOWnxGE5ttzjsy89FLnBWQYhlEHJik4gMVm4fP1n9O6SWt6Z9obZbp1g6AgZ4dmGIZxQiYpOMBvqb+RUZRBhbWCYfsbIyJw2WXODsswDOOkTJuCA0xeO5kg7yAyCjPoussTlILB5umlhmHUf+ZK4SzLLMrk122/0jW0K9H54FFWoUdE7dfP2aEZhmGclEkKZ9mU9VOwKitlljKuLbaPB9ijB/j4ODcwwzCMOjBJ4Sz7IukL+rfoz9rstYzc20TPHDHixCsZhmHUEyYpnEXphemk5qfSNrAtVbYqum6xP3LCNDIbhtFAmKRwFi3dvRSAwopCGosP3unZ4OEBvc67x0QYhnGeMknhLFq6eymNPBuRmJXIba49EZtN35/gZjp5GYbRMJikcBYt2b2EHuE92FWwi1uTPfTMkSOdG5RhGMYpMEnhLNlXso+teVtp5NkIgC5L7A+RGzPGiVEZhmGcGpMUzpJle5YBkFuay6XW5rhnZkOTJtC+vZMjMwzDqDuTFM6SJWlL8HH3ISEzgcf2ROmZV1xhHr1pGEaD4tCkICJDRWSriKSKyFMnKHeNiCgRiXFkPI60dM9Swv3CqbJVMeSvbD3zuuucG5RhGMYpclhSEBFX4ANgGNAJGCsinWop5w88BKx2VCyOll+Wz8Z9G8k5mMNNAf3wTN0FLi5mqGzDMBocR14pxAGpSqmdSqlKYAZQW1ecl4H/AOUOjMWhlu9ZjkJRXFnM0zn2NoS4OPA3D64zDKNhcWRSiATSq01n2OcdJiI9gGZKqTkn2pCIjBORRBFJzM3NPfuRnqGZKTNxFVdaNm5Jp4Xr9cyrr3ZuUIZhGKfBkUmhthZWdXihiAvwNvD4yTaklPpEKRWjlIoJCQk5iyGeudKqUr7d/C1WZeXxzncja9fqBcOGOTcwwzCM0+DIpJABNKs2HQVkVZv2B7oAi0UkDfgbMLuhNTbPSplFaVUp7i7u/ONgW/3shKAg6NzZ2aEZhmGcMkcmhQSgrYi0EhEP4AZg9qGFSqlCpVSwUqqlUqolsAoYoZRKdGBMZ90XSV/gIi5c1e4q/P9K0DOvvNJ0RTUMo0FyWFJQSlmAB4D5wBbgO6XUZhF5SUTOi7Gks4uzWbhzITZl48auN8K8eXrBqFHODcwwDOM0OXSkNqXUXGBujXnPH6fsAEfG4gjTN03Hpmz4uPtwReQASE7WXVEHDXJ2aIZhGKfF3NF8mpRSTF0/FVdxZUzHMXivSQKbDbp0gYAAZ4dnGIZxWkxSOE3fJ3/PxpyNWJWVsV3Gwi+/6AXXXOPcwAzDMM6AGej/NJRUlvD4gsdp7NUYF1wY3HowzH1ELzSjohqG0YCZpHAaJiybQEZRBp6untzW/TbcLTZITdV3MHfs6OzwDOO4qqqqyMjIoLy8wQ4gYJyEl5cXUVFRuLu7n9b6Jimcom1523hrxVvEhMeQmJ3I3T3vhr/+0u0JF19suqIa9VpGRgb+/v60bNkSMb/V845Siry8PDIyMmjVqtVpbcO0KZyiCcsm4OHqQU5pDhc3u5heEb3g3Xf1wltucW5whnES5eXlBAUFmYRwnhIRgoKCzuhK0CSFU3Cw8iA/JP9An2Z92FO4h4d7PwxpabqR2dXVPHrTaBBMQji/nen3a6qPTsHMlJkcrDpIXmkeUQFRjOowCm6/U1cdDR8Ovr7ODtEwDOOMmCuFU/Dlhi+J8I9g3d513B97P+4p2+Drr/XCO+5wbnCG0QDk5eXRvXt3unfvTlhYGJGRkYenKysr67SN22+/na1bt56wzAcffMDXh/5v1jPPPvssEydOPGre7t27GTBgAJ06daJz5868//77TorOXCnUWVZxFr/v/J0eYT3IL8vXDcw33aWrjby9zaiohlEHQUFBrF+vh5d/8cUX8fPz44knnjiqjFIKpRQuLrWfs06ZMuWk73P//fefebDnkLu7OxMnTqR79+4UFRXRo0cPLrvsMtq1a3fOYzFJoY6+2fgNNmVjW942ru54NUGZ+TBzJnh66nsTvLycHaJhnJJHfnuE9XvXn9Vtdg/rzsShE09esIbU1FRGjRpFv379WL16NXPmzOHf//43a9eupaysjOuvv57nn9cj5PTr14/333+fLl26EBwczPjx45k3bx4+Pj7MmjWL0NBQnn32WYKDg3nkkUfo168f/fr1448//qCwsJApU6bQt29fDh48yK233kpqaiqdOnVi+/btfPrpp3Tv3v2o2F544QXmzp1LWVkZ/fr148MPP0RE2LZtG+PHjycvLw9XV1d++uknWrZsyYQJE5g+fTouLi4MHz6cV1999aT7HxERQUREBAABAQF06NCBzMxMpyQFU31UR9M2TCM6MJriymJu63YbfPqpHueoogJuuMHZ4RlGg5ecnMydd97JunXriIyM5PXXXycxMZGkpCQWLlxIcnLyMesUFhbSv39/kpKS6NOnD59//nmt21ZKER8fz5tvvslLL70EwHvvvUdYWBhJSUk89dRTrFu3rtZ1H374YRISEti4cSOFhYX89ttvAIwdO5ZHH32UpKQkVvx/e/ceVmWVNn78e4PnIwoeEmqk3g4qg6iEWttT9DoeGE9ZZFoqaZeWp/n1/n41Dteok3U1KmpmmabjNA0jr2VmlIdRIg/jEUzAcApLnBDGQBFFUA6zfn88mx3iRkHZbpH7c1374jnve7Fgr/2s53nutXcvbdu2JTY2li1btnDw4EGSkpJ4+eXrDhdzlR9++IGjR4/y8MMPV44kLhYAAB6FSURBVHvfmqBnClVwIOMAyaeT6dKmC77NfXnM1wZ/HgcdOkBhITz+uLtDVKrabuQbvSvdd999V3wQrlu3jjVr1lBSUkJmZiapqal07nzlMO+NGzdmsL3rtkePHuzevdvpsUfZR0Ls0aMH6enpAOzZs4dXXnkFgK5du9KlkjFQ4uLiWLhwIZcuXSInJ4cePXrQq1cvcnJy+PWvfw1YD4wB7Nixg4iICBo3bgxA69atq/U7OH/+PE888QRvv/02zZo1q9a+NUUbhSqI2hdFy4YtSc1O5VXbq3h+/gX89JPVdTR+PNzgk4NKqZ81LXf3XlpaGm+99RYHDx7Ey8uLcePGOb33vkGDBo5pT09PSkpKnB67YcOGV21jjHG6bXkFBQVMmzaNw4cP4+vrS2RkpCMOZ7d+GmNu+JbQoqIiRo0axYQJExg2zH2jC2j30XX8kPsDG45toPtd3TEYxncdD++/D61aWV1HeteRUjXu/PnzNG/enBYtWpCVlcW2bdtq/D1sNhvr168HICUlxWn3VGFhIR4eHvj4+HDhwgU2bNgAQKtWrfDx8SHWngjz0qVLFBQUMHDgQNasWUNhYSEAZ8+erVIsxhgmTJhAUFAQM2fOrIni3TBtFK5j6f6leIonp86fopdfLx680AD+/nfr7KBbNwgJcXeISt1xunfvTufOnQkICGDy5Mk8+uijNf4e06dP59SpUwQGBhIVFUVAQAAtW7a8Yhtvb2/Gjx9PQEAAI0eOpGfPno510dHRREVFERgYiM1mIzs7m7CwMAYNGkRwcDBBQUEsWbLE6XvPnTsXPz8//Pz86NixIzt37mTdunVs377dcYuuKxrCqpCqnELdToKDg01Cwq0ZsfNs4VnuXnI3of6hxH4Xy7tD3mXqhpOwcKH1wNqqVTB58i2JRamacOzYMTpp0kYASkpKKCkpoVGjRqSlpTFw4EDS0tKoV6/296o7q2cRSTTGBF9v39pfehd699C7FBQXcFezu/AQD55sOwDeeRj8/ODcOXjmGXeHqJS6Qfn5+YSGhlJSUoIxhpUrV94RDcLNculvQEQGAW8BnsBqY8ybFdZPAV4CSoF84AVjzNUde25w8txJ3tzzJsMeGMbOkzvp94t++Kz8C1y8CJcuwZQpmtZCqVrMy8uLxMREd4dx23HZNQUR8QTeAQYDnYExItK5wmZ/M8b80hgTBCwAFrsqnuowxvDS5pcwGKaFTOPbM98yrsMgePtt+OUvoaTEahSUUuoO48oLzSHAcWPMD8aYIiAGuCKNqDHmfLnZpsBtcYHjo9SP+CLtC+YPmM++jH0IwlPbMqyzhMxMCA2FSu5pVkqp2syVjYIv8GO5+Qz7siuIyEsi8j3WmcIMZwcSkRdEJEFEErKzs10SbJlzl84xY8sMgjsEM6PnDD5O/Zhf+fSi2cq10KMH5OTA7NkujUEppdzFlY2Csyc4rjoTMMa8Y4y5D3gFiHR2IGPMKmNMsDEmuE2bNjUc5pVWHFrB6YuneW/oexw/e5yUn1KYmXMf5OfDqVPQsycMGODSGJRSyl1c2ShkAHeXm/cDMq+xfQwwwoXxXNflksssO7iMX933K3p06MGGY9aDKn0PnwEvL8jKgt/+VofcVOoG9e/f/6r775cuXcqLL754zf3KUj5kZmYyevToSo99vdvVly5dSkFBgWN+yJAhnDt3riqh31JfffUVYWFhVy0fO3YsDz74IAEBAURERFBcXFzj7+3KRuEQcL+I+ItIA+Bp4LPyG4jI/eVmhwJpLoznutYdXce/8//Ny72tJFabvt1EnzbBNNmx02oIunQBe64TpVT1jRkzhpiYmCuWxcTEMGbMmCrt36FDBz7++OMbfv+KjcLmzZvx8vK64ePdamPHjuWf//wnKSkpFBYWsnr16hp/D5fdkmqMKRGRacA2rFtS/2SM+UZE/gAkGGM+A6aJyONAMZALjHdVPFWIl0V7FxHYLpDH732cny7+xKFTh1hXfwwUJEBBASxdamVGVeoO4I7U2aNHjyYyMpLLly/TsGFD0tPTyczMxGazkZ+fz/Dhw8nNzaW4uJj58+czvMIQt+np6YSFhXH06FEKCwuZOHEiqampdOrUyZFaAmDq1KkcOnSIwsJCRo8ezbx581i2bBmZmZkMGDAAHx8f4uPj6dixIwkJCfj4+LB48WJHltVJkyYxa9Ys0tPTGTx4MDabjb179+Lr68umTZscCe/KxMbGMn/+fIqKivD29iY6Opp27dqRn5/P9OnTSUhIQESYM2cOTzzxBFu3bmX27NmUlpbi4+NDXFxclX6/Q4YMcUyHhISQkZFRpf2qw6XPKRhjNgObKyz7fblp9yb5KGfb99v4Jvsb/jLiL4gIW9K2YDA8nnTBSmnRrBk89ZS7w1SqVvP29iYkJIStW7cyfPhwYmJiCA8PR0Ro1KgRGzdupEWLFuTk5NCrVy+GDRtWaYK5FStW0KRJE5KTk0lOTqZ79+6Oda+//jqtW7emtLSU0NBQkpOTmTFjBosXLyY+Ph4fH58rjpWYmMjatWs5cOAAxhh69uxJv379aNWqFWlpaaxbt47333+fp556ig0bNjBu3Lgr9rfZbOzfvx8RYfXq1SxYsICoqChee+01WrZsSUpKCgC5ublkZ2czefJkdu3ahb+/f5XzI5VXXFzMhx9+yFtvvVXtfa9HH9+zW7xvMb7NfQkPCAfgi7Qv8Gvcntbb91jPJYwfrwPpqDuKu1Jnl3UhlTUKZd/OjTHMnj2bXbt24eHhwalTpzh9+jTt27d3epxdu3YxY4Z1w2JgYCCBgYGOdevXr2fVqlWUlJSQlZVFamrqFesr2rNnDyNHjnRkah01ahS7d+9m2LBh+Pv7OwbeKZ96u7yMjAzCw8PJysqiqKgIf39/wEqlXb67rFWrVsTGxtK3b1/HNtVNrw3w4osv0rdvX/r06VPtfa9H+0KA789+z/YftjMleAoNPBtQXFrMtu+3MbOoG5KbC8ZojiOlasiIESOIi4tzjKpW9g0/Ojqa7OxsEhMTOXLkCO3atXOaLrs8Z2cRJ06cYNGiRcTFxZGcnMzQoUOve5xr5YArS7sNlafnnj59OtOmTSMlJYWVK1c63s9ZKu2bSa8NMG/ePLKzs1m82DXP+mqjAKw+vBpP8WRi0EQA9v64l/OXzzMy1VgXmHv1gs4VH8ZWSt2IZs2a0b9/fyIiIq64wJyXl0fbtm2pX78+8fHxnDx58prH6du3L9HR0QAcPXqU5ORkwEq73bRpU1q2bMnp06fZsmWLY5/mzZtz4cIFp8f69NNPKSgo4OLFi2zcuLFa38Lz8vLw9bUew/rggw8cywcOHMjy5csd87m5ufTu3ZudO3dy4sQJoOrptQFWr17Ntm3bHMN9ukKdbxSKS4tZe2QtQx8Yim8Lq1K/SPuCZqX1uPfzf1hnCZrSQqkaNWbMGJKSkni63FC2Y8eOJSEhgeDgYKKjo3nooYeueYypU6eSn59PYGAgCxYsIMSexr5r165069aNLl26EBERcUXa7RdeeIHBgwczoMKzRt27d2fChAmEhITQs2dPJk2aRLdu3apcnrlz5/Lkk0/Sp0+fK65XREZGkpubS0BAAF27diU+Pp42bdqwatUqRo0aRdeuXQkPD3d6zLi4OEd6bT8/P/bt28eUKVM4ffo0vXv3JigoyDG0aE2q86mzNx7byKj1o4gdE0vYA9Z9wV3e7cLYFGH2e99YSe9++gmaNKmx91TKXTR1dt1wM6mz6/yZwqrDq/Br4ceg/xoEWCOtpWan8ty+Aqvr6NlntUFQStUZdbpROHnuJNuObyMiKIJ6HtaNWEv2LaHTWU/8kk5YXUcTJ7o5SqWUunXqdKMw56s51POox/Pdnwcg80Im7x9+n8U/drbOEu6/Hx5+2M1RKqXUrVNnG4W9P+7lg6QPeLn3y9zT8h4AFv5jIVJUzH9/9aN1ljBpkuY5UkrVKXWyUSj9TynTNk/Dr4UfkX2txKyn80+zMnElf/B4DM/cc1ZjUOGpRaWUutPVyUZhVeIqvv7310QNjKJpA+sJxoV7F3K59DIRFx+wNgoNhQ4d3BilUkrdenWuUci7lEdkfCQDOg7gyc5PAnAg4wBL9y/lua7P4f3lfmtDfYJZqRp35swZgoKCCAoKon379vj6+jrmi4qKqnSMiRMn8u23315zm3feecfxYJuqnjqX+2jR3kWcLTxL1MAoRITzl8/zzCfP4NfCjyWPL4LR7a0cRxWyMyqlbp63tzdHjliZWefOnUuzZs34n//5nyu2McZgjKn0id21a9de931eeumlmw+2jqpTjcLp/NMs2b+Ep7o8Rbe7rKcVp22eRvq5dHZN2IXXvq+t5HcDB0K5fCdK3ZFmzYIjNZs6m6AgK8V8NR0/fpwRI0Zgs9k4cOAAn3/+OfPmzXPkRwoPD+f3v7cSLNtsNpYvX05AQAA+Pj5MmTKFLVu20KRJEzZt2kTbtm2JjIzEx8eHWbNmYbPZsNlsfPnll+Tl5bF27VoeeeQRLl68yHPPPcfx48fp3LkzaWlprF692pH8rsycOXPYvHkzhYWF2Gw2VqxYgYjw3XffMWXKFM6cOYOnpyeffPIJHTt25I033nCkoQgLC+P111+vkV/trVKnuo/e2P0Gl0ou8dqA1wD4W8rf+DD5Q+b0m8Oj9zwKy5ZZG778shujVKpuSk1N5fnnn+frr7/G19eXN998k4SEBJKSkti+fTupqalX7ZOXl0e/fv1ISkqid+/ejoyrFRljOHjwIAsXLnSkhnj77bdp3749SUlJvPrqq3z99ddO9505cyaHDh0iJSWFvLw8tm7dClipOn7zm9+QlJTE3r17adu2LbGxsWzZsoWDBw+SlJTEy7Xws6TOnCmkn0vnvcT3mBg0kQe8H+DkuZNM/WIqj9z9CLP7zIbSUoiLgwYNdAxmVTfcwDd6V7rvvvt4uNxzQevWrWPNmjWUlJSQmZlJamoqnSskpmzcuDGDBw8GrLTWu3fvdnrsUaNGObYpS329Z88eXnnlFcDKl9SlSxen+8bFxbFw4UIuXbpETk4OPXr0oFevXuTk5PBr+0iMjexp9Xfs2EFERIRjEJ4bSYvtbnWmUVj79VoEYU7/OZT+p5RnNz6LMYa/jvyr9TTz5s3W6GohIfpsglJuUDaWAUBaWhpvvfUWBw8exMvLi3HjxjlNf92gQQPHdGVpreHn9Nflt6lK3reCggKmTZvG4cOH8fX1JTIy0hGHs/TXN5sW+3ZQZ7qP5vSfw8HJB/Fr4cfCvQvZ/a/dLB+yHP9W1kAXLFli/Rw50n1BKqUAK/118+bNadGiBVlZWWzbtq3G38Nms7F+/XoAUlJSnHZPFRYW4uHhgY+PDxcuXGDDhg2ANViOj48PsbGxAFy6dImCggIGDhzImjVrHEOD3sioau7m0kZBRAaJyLciclxEXnWy/v+ISKqIJItInIj8wlWxeIgHge0CKS4t5o3dbzD8weE8G/istfLAAdixw5ru29dVISilqqh79+507tyZgIAAJk+efEX665oyffp0Tp06RWBgIFFRUQQEBNCyZcsrtvH29mb8+PEEBAQwcuRIevbs6VgXHR1NVFQUgYGB2Gw2srOzCQsLY9CgQQQHBxMUFMSSsi+btUnZ7V81/QI8ge+Be4EGQBLQucI2A4Am9umpwP9e77g9evQwN+Mf//qHYS7mo28++nnhY48Z07ixMfXrG1NYeFPHV+p2lpqa6u4QbhvFxcWm0P7//t1335mOHTua4uJiN0dVM5zVM5BgqvDZ7cprCiHAcWPMDwAiEgMMBxznaMaY+HLb7wdcnlci7oc4BGFAR/vF5B074MsvwcvLup1Ox2FWqk7Iz88nNDSUkpISjDGsXLmSevXqzGXWSrnyN+AL/FhuPgPoWcm2AM8DW5ytEJEXgBcA7rnnnpsKKu5EHEHtg/Bu4m0lvXvpJfDwAE9PcNGYp0qp24+XlxeJiYnuDuO248prCs4uwTu93C8i44BgYKGz9caYVcaYYGNMcJs2bW44oILiAvZl7CPUP9Ra8Mc/wnffQfv2cPCgNRazUkrVYa48U8gA7i437wdkVtxIRB4Hfgf0M8ZcdmE87PnXHopKiwi9N9Q6S1i40DpLSEqCcuOqKqVUXeXKM4VDwP0i4i8iDYCngc/KbyAi3YCVwDBjzE8ujAWwrifU96hPn3v6QGwsnD1r3W2kDYJSSgEubBSMMSXANGAbcAxYb4z5RkT+ICLD7JstBJoBH4nIERH5rJLD1Yi4E3H08utlpct+1X6H7BtvuPItlVKqVnHpcwrGmM3GmAeMMfcZY163L/u9MeYz+/Tjxph2xpgg+2vYtY9443ILczmcddi6nnD4MBw7BnfdpdcRlLqF+vfvf9WDaEuXLuXFF1+85n7NmjUDIDMzk9GjR1d67ISEhGseZ+nSpRQUFDjmhwwZwrlz56oSep1RZ55o/ir9KwzGup4we7a1cOZMTWmh1C00ZswYYmJirlgWExPDmDFjqrR/hw4d+Pjjj2/4/Ss2Cps3b8bLy+uGj3cnqjM35WblZ9GuaTtCLvvAtm3WBebnn3d3WEq5jxtSZ48ePZrIyEguX75Mw4YNSU9PJzMzE5vNRn5+PsOHDyc3N5fi4mLmz5/P8ArjmqSnpxMWFsbRo0cpLCxk4sSJpKam0qlTJ0dqCYCpU6dy6NAhCgsLGT16NPPmzWPZsmVkZmYyYMAAfHx8iI+Pp2PHjiQkJODj48PixYsdWVYnTZrErFmzSE9PZ/DgwdhsNvbu3Yuvry+bNm1yJLwrExsby/z58ykqKsLb25vo6GjatWtHfn4+06dPJyEhARFhzpw5PPHEE2zdupXZs2dTWlqKj48PcXFxNVgJN6fONAovPvwiU4Kn4DHJPqLaoEF6gVmpW8zb25uQkBC2bt3K8OHDiYmJITw8HBGhUaNGbNy4kRYtWpCTk0OvXr0YNmxYpQnmVqxYQZMmTUhOTiY5OZnu3bs71r3++uu0bt2a0tJSQkNDSU5OZsaMGSxevJj4+Hh8KvzvJyYmsnbtWg4cOIAxhp49e9KvXz9atWpFWloa69at4/333+epp55iw4YNjKswfrvNZmP//v2ICKtXr2bBggVERUXx2muv0bJlS1JSUgDIzc0lOzubyZMns2vXLvz9/W+7/Eh1plEA8DhzFv78Z6hX77ZLG6zULeem/4GyLqSyRqHs27kxhtmzZ7Nr1y48PDw4deoUp0+fpn379k6Ps2vXLmbMmAFAYGAggYGBjnXr169n1apVlJSUkJWVRWpq6hXrK9qzZw8jR450ZGodNWoUu3fvZtiwYfj7+zsG3imferu8jIwMwsPDycrKoqioCH9/K9Hmjh07rugua9WqFbGxsfTt29exze2WXrvOXFMA4Jln4D//gTlz4P773R2NUnXSiBEjiIuLc4yqVvYNPzo6muzsbBITEzly5Ajt2rVzmi67PGdnESdOnGDRokXExcWRnJzM0KFDr3scc4002g3LjcJYWXru6dOnM23aNFJSUli5cqXj/YyTVNrOlt1O6k6jEBcH27dbTy9HRro7GqXqrGbNmtG/f38iIiKuuMCcl5dH27ZtqV+/PvHx8Zw8efKax+nbty/R0dEAHD16lOTkZMBKu920aVNatmzJ6dOn2bLl5+w5zZs358KFC06P9emnn1JQUMDFixfZuHEjffr0qXKZ8vLy8PX1BeCDDz5wLB84cCDLly93zOfm5tK7d2927tzJiRMngNsvvXbdaRQ2brR+lg25qZRymzFjxpCUlMTTTz/tWDZ27FgSEhIIDg4mOjqahx566JrHmDp1Kvn5+QQGBrJgwQJCQkIAaxS1bt260aVLFyIiIq5Iu/3CCy8wePBgBlQYXbF79+5MmDCBkJAQevbsyaRJk+jWrVuVyzN37lyefPJJ+vTpc8X1isjISHJzcwkICKBr167Ex8fTpk0bVq1axahRo+jatSvh4eFVfp9bQa512nQ7Cg4ONte7F9mpzz+H1avhk0+sO4+UqoOOHTtGp06d3B2GcjFn9SwiicaY4OvtW3cuNIeFWS+llFKV0q/MSimlHLRRUKqOqW1dxqp6brZ+tVFQqg5p1KgRZ86c0YbhDmWM4cyZMzS6iREk6841BaUUfn5+ZGRkkJ2d7e5QlIs0atQIPz+/G95fGwWl6pD69es7nqRVyhntPlJKKeWgjYJSSikHbRSUUko51LonmkUkG7h2UpSr+QA5LgjHHbQstycty+3rTirPzZTlF8aYNtfbqNY1CjdCRBKq8nh3baBluT1pWW5fd1J5bkVZtPtIKaWUgzYKSimlHOpKo7DK3QHUIC3L7UnLcvu6k8rj8rLUiWsKSimlqqaunCkopZSqAm0UlFJKOdzRjYKIDBKRb0XkuIi86u54qkNE7haReBE5JiLfiMhM+/LWIrJdRNLsP1u5O9aqEhFPEflaRD63z/uLyAF7Wf5XRBq4O8aqEhEvEflYRP5pr6PetbVuROQ39r+xoyKyTkQa1Za6EZE/ichPInK03DKn9SCWZfbPg2QR6e6+yK9WSVkW2v/GkkVko4h4lVv3W3tZvhWRX9VUHHdsoyAinsA7wGCgMzBGRDq7N6pqKQFeNsZ0AnoBL9njfxWIM8bcD8TZ52uLmcCxcvN/BJbYy5ILPO+WqG7MW8BWY8xDQFesctW6uhERX2AGEGyMCQA8gaepPXXzZ2BQhWWV1cNg4H776wVgxS2Ksar+zNVl2Q4EGGMCge+A3wLYPwueBrrY93nX/pl30+7YRgEIAY4bY34wxhQBMcBwN8dUZcaYLGPMYfv0BawPHV+sMnxg3+wDYIR7IqweEfEDhgKr7fMCPAZ8bN+kNpWlBdAXWANgjCkyxpyjltYNVrbkxiJSD2gCZFFL6sYYsws4W2FxZfUwHPiLsewHvETkrlsT6fU5K4sx5u/GmBL77H6gLCf2cCDGGHPZGHMCOI71mXfT7uRGwRf4sdx8hn1ZrSMiHYFuwAGgnTEmC6yGA2jrvsiqZSnw/4D/2Oe9gXPl/uBrU/3cC2QDa+3dYatFpCm1sG6MMaeARcC/sBqDPCCR2ls3UHk91PbPhAhgi33aZWW5kxsFcbKs1t1/KyLNgA3ALGPMeXfHcyNEJAz4yRiTWH6xk01rS/3UA7oDK4wx3YCL1IKuImfs/e3DAX+gA9AUq5ulotpSN9dSa//mROR3WF3K0WWLnGxWI2W5kxuFDODucvN+QKabYrkhIlIfq0GINsZ8Yl98uuyU1/7zJ3fFVw2PAsNEJB2rG+8xrDMHL3uXBdSu+skAMowxB+zzH2M1ErWxbh4HThhjso0xxcAnwCPU3rqByuuhVn4miMh4IAwYa35+sMxlZbmTG4VDwP32uygaYF2U+czNMVWZvc99DXDMGLO43KrPgPH26fHAplsdW3UZY35rjPEzxnTEqocvjTFjgXhgtH2zWlEWAGPMv4EfReRB+6JQIJVaWDdY3Ua9RKSJ/W+urCy1sm7sKquHz4Dn7Hch9QLyyrqZblciMgh4BRhmjCkot+oz4GkRaSgi/lgXzw/WyJsaY+7YFzAE64r998Dv3B1PNWO3YZ0OJgNH7K8hWH3xcUCa/Wdrd8dazXL1Bz63T99r/0M+DnwENHR3fNUoRxCQYK+fT4FWtbVugHnAP4GjwIdAw9pSN8A6rGshxVjfnp+vrB6wulzesX8epGDdceX2MlynLMexrh2UfQa8V27739nL8i0wuKbi0DQXSimlHO7k7iOllFLVpI2CUkopB20UlFJKOWijoJRSykEbBaWUUg7aKChlJyKlInKk3KvGnlIWkY7ls18qdbuqd/1NlKozCo0xQe4OQil30jMFpa5DRNJF5I8ictD++i/78l+ISJw9132ciNxjX97Onvs+yf56xH4oTxF53z52wd9FpLF9+xkikmo/ToybiqkUoI2CUuU1rtB9FF5u3XljTAiwHCtvE/bpvxgr1300sMy+fBmw0xjTFSsn0jf25fcD7xhjugDngCfsy18FutmPM8VVhVOqKvSJZqXsRCTfGNPMyfJ04DFjzA/2JIX/NsZ4i0gOcJcxpti+PMsY4yMi2YCfMeZyuWN0BLYba+AXROQVoL4xZr6IbAXysdJlfGqMyXdxUZWqlJ4pKFU1ppLpyrZx5nK56VJ+vqY3FCsnTw8gsVx2UqVuOW0UlKqa8HI/99mn92JlfQUYC+yxT8cBU8ExLnWLyg4qIh7A3caYeKxBiLyAq85WlLpV9BuJUj9rLCJHys1vNcaU3ZbaUEQOYH2RGmNfNgP4k4j8X6yR2Cbal88EVonI81hnBFOxsl864wn8VURaYmXxXGKsoT2Vcgu9pqDUddivKQQbY3LcHYtSrqbdR0oppRz0TEEppZSDnikopZRy0EZBKaWUgzYKSimlHLRRUEop5aCNglJKKYf/D23XHuLj5r8CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 16.0293 - acc: 0.1481 - val_loss: 15.6090 - val_acc: 0.1740\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 15.2491 - acc: 0.2091 - val_loss: 14.8513 - val_acc: 0.2240\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 14.5004 - acc: 0.2616 - val_loss: 14.1143 - val_acc: 0.2760\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 13.7704 - acc: 0.2997 - val_loss: 13.3926 - val_acc: 0.3070\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.0557 - acc: 0.3440 - val_loss: 12.6864 - val_acc: 0.3350\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 12.3600 - acc: 0.3708 - val_loss: 12.0019 - val_acc: 0.3870\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 11.6848 - acc: 0.4077 - val_loss: 11.3388 - val_acc: 0.4170\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 11.0316 - acc: 0.4375 - val_loss: 10.6971 - val_acc: 0.4360\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 10.4002 - acc: 0.4656 - val_loss: 10.0769 - val_acc: 0.4580\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 9.7913 - acc: 0.4921 - val_loss: 9.4801 - val_acc: 0.4940\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.2068 - acc: 0.5173 - val_loss: 8.9081 - val_acc: 0.5310\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 8.6468 - acc: 0.5379 - val_loss: 8.3625 - val_acc: 0.5530\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 8.1118 - acc: 0.5623 - val_loss: 7.8391 - val_acc: 0.5610\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 7.6009 - acc: 0.5796 - val_loss: 7.3419 - val_acc: 0.5780\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 7.1144 - acc: 0.6039 - val_loss: 6.8693 - val_acc: 0.5800\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.6519 - acc: 0.6147 - val_loss: 6.4195 - val_acc: 0.5990\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 6.2139 - acc: 0.6333 - val_loss: 5.9965 - val_acc: 0.6180\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 5.7996 - acc: 0.6451 - val_loss: 5.5927 - val_acc: 0.6290\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 5.4077 - acc: 0.6563 - val_loss: 5.2143 - val_acc: 0.6390\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 5.0396 - acc: 0.6671 - val_loss: 4.8591 - val_acc: 0.6440\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.6948 - acc: 0.6689 - val_loss: 4.5273 - val_acc: 0.6480\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.3725 - acc: 0.6765 - val_loss: 4.2167 - val_acc: 0.6580\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.0729 - acc: 0.6823 - val_loss: 3.9306 - val_acc: 0.6560\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 3.7960 - acc: 0.6828 - val_loss: 3.6634 - val_acc: 0.6680\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.5409 - acc: 0.6872 - val_loss: 3.4208 - val_acc: 0.6710\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.3082 - acc: 0.6829 - val_loss: 3.1993 - val_acc: 0.6740\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 3.0965 - acc: 0.6876 - val_loss: 3.0014 - val_acc: 0.6710\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.9065 - acc: 0.6851 - val_loss: 2.8213 - val_acc: 0.6800\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.7375 - acc: 0.6908 - val_loss: 2.6621 - val_acc: 0.6750\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.5886 - acc: 0.6905 - val_loss: 2.5250 - val_acc: 0.6740\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.4602 - acc: 0.6895 - val_loss: 2.4070 - val_acc: 0.6740\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.3513 - acc: 0.6851 - val_loss: 2.3061 - val_acc: 0.6710\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.2608 - acc: 0.6857 - val_loss: 2.2255 - val_acc: 0.6740\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.1882 - acc: 0.6867 - val_loss: 2.1627 - val_acc: 0.6750\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.1323 - acc: 0.6877 - val_loss: 2.1123 - val_acc: 0.6760\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.0901 - acc: 0.6875 - val_loss: 2.0777 - val_acc: 0.6680\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0588 - acc: 0.6861 - val_loss: 2.0492 - val_acc: 0.6720\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0335 - acc: 0.6859 - val_loss: 2.0258 - val_acc: 0.6690\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0116 - acc: 0.6855 - val_loss: 2.0095 - val_acc: 0.6730\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9922 - acc: 0.6865 - val_loss: 1.9888 - val_acc: 0.6720\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9738 - acc: 0.6880 - val_loss: 1.9695 - val_acc: 0.6730\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9569 - acc: 0.6892 - val_loss: 1.9534 - val_acc: 0.6710\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9411 - acc: 0.6868 - val_loss: 1.9403 - val_acc: 0.6730\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9261 - acc: 0.6907 - val_loss: 1.9257 - val_acc: 0.6760\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9114 - acc: 0.6881 - val_loss: 1.9091 - val_acc: 0.6790\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8974 - acc: 0.6920 - val_loss: 1.8989 - val_acc: 0.6750\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8835 - acc: 0.6911 - val_loss: 1.8831 - val_acc: 0.6720\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8707 - acc: 0.6909 - val_loss: 1.8744 - val_acc: 0.6780\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8583 - acc: 0.6949 - val_loss: 1.8578 - val_acc: 0.6780\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8454 - acc: 0.6928 - val_loss: 1.8470 - val_acc: 0.6790\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8339 - acc: 0.6943 - val_loss: 1.8369 - val_acc: 0.6790\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8223 - acc: 0.6948 - val_loss: 1.8263 - val_acc: 0.6730\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8112 - acc: 0.6965 - val_loss: 1.8165 - val_acc: 0.6850\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8000 - acc: 0.6967 - val_loss: 1.8013 - val_acc: 0.6750\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7887 - acc: 0.6975 - val_loss: 1.7947 - val_acc: 0.6870\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7784 - acc: 0.6968 - val_loss: 1.7842 - val_acc: 0.6780\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7683 - acc: 0.6956 - val_loss: 1.7724 - val_acc: 0.6810\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7584 - acc: 0.6997 - val_loss: 1.7624 - val_acc: 0.6820\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7482 - acc: 0.7005 - val_loss: 1.7578 - val_acc: 0.6870\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7383 - acc: 0.7021 - val_loss: 1.7487 - val_acc: 0.6840\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.7293 - acc: 0.7013 - val_loss: 1.7334 - val_acc: 0.6830\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7192 - acc: 0.7013 - val_loss: 1.7232 - val_acc: 0.6810\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7100 - acc: 0.7031 - val_loss: 1.7185 - val_acc: 0.6850\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7008 - acc: 0.7037 - val_loss: 1.7092 - val_acc: 0.6860\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6920 - acc: 0.7037 - val_loss: 1.7007 - val_acc: 0.6850\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6832 - acc: 0.7023 - val_loss: 1.6891 - val_acc: 0.6890\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6743 - acc: 0.7047 - val_loss: 1.6815 - val_acc: 0.6870\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.6657 - acc: 0.7035 - val_loss: 1.6726 - val_acc: 0.6940\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6573 - acc: 0.7036 - val_loss: 1.6691 - val_acc: 0.6890\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6488 - acc: 0.7069 - val_loss: 1.6566 - val_acc: 0.6910\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6409 - acc: 0.7052 - val_loss: 1.6500 - val_acc: 0.6860\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6326 - acc: 0.7059 - val_loss: 1.6423 - val_acc: 0.6900\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6247 - acc: 0.7077 - val_loss: 1.6325 - val_acc: 0.6940\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6168 - acc: 0.7084 - val_loss: 1.6266 - val_acc: 0.6900\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6090 - acc: 0.7079 - val_loss: 1.6219 - val_acc: 0.6880\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6014 - acc: 0.7080 - val_loss: 1.6125 - val_acc: 0.6920\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5942 - acc: 0.7095 - val_loss: 1.6045 - val_acc: 0.6970\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5861 - acc: 0.7097 - val_loss: 1.5976 - val_acc: 0.6870\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5788 - acc: 0.7091 - val_loss: 1.5896 - val_acc: 0.6920\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5718 - acc: 0.7109 - val_loss: 1.5829 - val_acc: 0.6950\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5647 - acc: 0.7128 - val_loss: 1.5765 - val_acc: 0.6920\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5578 - acc: 0.7120 - val_loss: 1.5688 - val_acc: 0.6960\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5508 - acc: 0.7115 - val_loss: 1.5613 - val_acc: 0.6970\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5435 - acc: 0.7120 - val_loss: 1.5540 - val_acc: 0.7010\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5367 - acc: 0.7124 - val_loss: 1.5478 - val_acc: 0.6940\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5303 - acc: 0.7147 - val_loss: 1.5413 - val_acc: 0.6980\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5236 - acc: 0.7152 - val_loss: 1.5365 - val_acc: 0.6950\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5167 - acc: 0.7149 - val_loss: 1.5287 - val_acc: 0.6980\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5100 - acc: 0.7157 - val_loss: 1.5219 - val_acc: 0.6980\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5038 - acc: 0.7172 - val_loss: 1.5175 - val_acc: 0.6940\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4971 - acc: 0.7180 - val_loss: 1.5085 - val_acc: 0.7060\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4909 - acc: 0.7177 - val_loss: 1.5057 - val_acc: 0.6970\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4845 - acc: 0.7181 - val_loss: 1.4971 - val_acc: 0.7010\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4783 - acc: 0.7177 - val_loss: 1.4890 - val_acc: 0.7020\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4718 - acc: 0.7185 - val_loss: 1.4862 - val_acc: 0.7010\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4661 - acc: 0.7193 - val_loss: 1.4796 - val_acc: 0.7040\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4598 - acc: 0.7201 - val_loss: 1.4714 - val_acc: 0.7030\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4535 - acc: 0.7187 - val_loss: 1.4684 - val_acc: 0.6980\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4474 - acc: 0.7197 - val_loss: 1.4640 - val_acc: 0.6980\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.4420 - acc: 0.7191 - val_loss: 1.4552 - val_acc: 0.6990\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4364 - acc: 0.7227 - val_loss: 1.4512 - val_acc: 0.7050\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4300 - acc: 0.7204 - val_loss: 1.4445 - val_acc: 0.7020\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4244 - acc: 0.7227 - val_loss: 1.4377 - val_acc: 0.7010\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4185 - acc: 0.7229 - val_loss: 1.4340 - val_acc: 0.7020\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4132 - acc: 0.7227 - val_loss: 1.4291 - val_acc: 0.7000\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4074 - acc: 0.7236 - val_loss: 1.4216 - val_acc: 0.6990\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4021 - acc: 0.7223 - val_loss: 1.4184 - val_acc: 0.6980\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3961 - acc: 0.7244 - val_loss: 1.4125 - val_acc: 0.7000\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3908 - acc: 0.7256 - val_loss: 1.4081 - val_acc: 0.7000\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.3858 - acc: 0.7259 - val_loss: 1.4015 - val_acc: 0.7000\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3804 - acc: 0.7269 - val_loss: 1.3972 - val_acc: 0.7010\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.3751 - acc: 0.7260 - val_loss: 1.3901 - val_acc: 0.7010\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3695 - acc: 0.7279 - val_loss: 1.3866 - val_acc: 0.7020\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3649 - acc: 0.7261 - val_loss: 1.3826 - val_acc: 0.7030\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3595 - acc: 0.7277 - val_loss: 1.3751 - val_acc: 0.7010\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3544 - acc: 0.7277 - val_loss: 1.3728 - val_acc: 0.7000\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3494 - acc: 0.7287 - val_loss: 1.3653 - val_acc: 0.7010\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3442 - acc: 0.7292 - val_loss: 1.3631 - val_acc: 0.7010\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3394 - acc: 0.7300 - val_loss: 1.3577 - val_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3344 - acc: 0.7297 - val_loss: 1.3558 - val_acc: 0.7030\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 15.9898 - acc: 0.1515 - val_loss: 15.5814 - val_acc: 0.1840\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 15.2294 - acc: 0.1841 - val_loss: 14.8386 - val_acc: 0.2100\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 14.4935 - acc: 0.2021 - val_loss: 14.1172 - val_acc: 0.2230\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 13.7807 - acc: 0.2133 - val_loss: 13.4174 - val_acc: 0.2310\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 13.0893 - acc: 0.2311 - val_loss: 12.7379 - val_acc: 0.2490\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 12.4186 - acc: 0.2520 - val_loss: 12.0782 - val_acc: 0.2670\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 11.7680 - acc: 0.2751 - val_loss: 11.4384 - val_acc: 0.2870\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 11.1378 - acc: 0.2964 - val_loss: 10.8186 - val_acc: 0.3010\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 10.5275 - acc: 0.3152 - val_loss: 10.2193 - val_acc: 0.3280\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 9.9368 - acc: 0.3487 - val_loss: 9.6387 - val_acc: 0.3640\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 9.3654 - acc: 0.3769 - val_loss: 9.0762 - val_acc: 0.3990\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 8.8130 - acc: 0.4124 - val_loss: 8.5348 - val_acc: 0.4400\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 8.2806 - acc: 0.4444 - val_loss: 8.0105 - val_acc: 0.4830\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 7.7689 - acc: 0.4789 - val_loss: 7.5084 - val_acc: 0.5000\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 7.2779 - acc: 0.5028 - val_loss: 7.0288 - val_acc: 0.5220\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.8100 - acc: 0.5195 - val_loss: 6.5730 - val_acc: 0.5450\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 6.3666 - acc: 0.5407 - val_loss: 6.1417 - val_acc: 0.5680\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 5.9465 - acc: 0.5549 - val_loss: 5.7337 - val_acc: 0.5870\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.5496 - acc: 0.5729 - val_loss: 5.3483 - val_acc: 0.5910\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 5.1745 - acc: 0.5848 - val_loss: 4.9840 - val_acc: 0.5890\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.8221 - acc: 0.5897 - val_loss: 4.6431 - val_acc: 0.5950\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.4919 - acc: 0.6007 - val_loss: 4.3240 - val_acc: 0.6080\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 4.1844 - acc: 0.6101 - val_loss: 4.0302 - val_acc: 0.6190\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.8993 - acc: 0.6160 - val_loss: 3.7553 - val_acc: 0.6240\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 3.6368 - acc: 0.6217 - val_loss: 3.5064 - val_acc: 0.6290\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.3964 - acc: 0.6279 - val_loss: 3.2757 - val_acc: 0.6330\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 3.1776 - acc: 0.6341 - val_loss: 3.0688 - val_acc: 0.6420\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.9807 - acc: 0.6389 - val_loss: 2.8844 - val_acc: 0.6450\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.8057 - acc: 0.6440 - val_loss: 2.7193 - val_acc: 0.6380\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.6516 - acc: 0.6463 - val_loss: 2.5747 - val_acc: 0.6430\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.5174 - acc: 0.6488 - val_loss: 2.4516 - val_acc: 0.6540\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.4034 - acc: 0.6500 - val_loss: 2.3482 - val_acc: 0.6490\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.3088 - acc: 0.6547 - val_loss: 2.2615 - val_acc: 0.6530\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.2320 - acc: 0.6543 - val_loss: 2.1949 - val_acc: 0.6540\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.1725 - acc: 0.6588 - val_loss: 2.1424 - val_acc: 0.6550\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.1278 - acc: 0.6605 - val_loss: 2.1038 - val_acc: 0.6600\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0943 - acc: 0.6616 - val_loss: 2.0753 - val_acc: 0.6640\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0677 - acc: 0.6619 - val_loss: 2.0493 - val_acc: 0.6640\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0445 - acc: 0.6647 - val_loss: 2.0361 - val_acc: 0.6680\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0238 - acc: 0.6673 - val_loss: 2.0070 - val_acc: 0.6610\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0041 - acc: 0.6656 - val_loss: 1.9902 - val_acc: 0.6640\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9859 - acc: 0.6707 - val_loss: 1.9732 - val_acc: 0.6680\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9687 - acc: 0.6692 - val_loss: 1.9543 - val_acc: 0.6670\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9521 - acc: 0.6723 - val_loss: 1.9382 - val_acc: 0.6660\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9362 - acc: 0.6739 - val_loss: 1.9249 - val_acc: 0.6620\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9214 - acc: 0.6729 - val_loss: 1.9113 - val_acc: 0.6710\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9070 - acc: 0.6751 - val_loss: 1.8941 - val_acc: 0.6650\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8932 - acc: 0.6747 - val_loss: 1.8831 - val_acc: 0.6680\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8797 - acc: 0.6773 - val_loss: 1.8695 - val_acc: 0.6690\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8667 - acc: 0.6800 - val_loss: 1.8570 - val_acc: 0.6660\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8539 - acc: 0.6789 - val_loss: 1.8449 - val_acc: 0.6700\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8415 - acc: 0.6811 - val_loss: 1.8337 - val_acc: 0.6690\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8294 - acc: 0.6824 - val_loss: 1.8212 - val_acc: 0.6720\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8184 - acc: 0.6820 - val_loss: 1.8084 - val_acc: 0.6740\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8067 - acc: 0.6835 - val_loss: 1.8012 - val_acc: 0.6750\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7959 - acc: 0.6841 - val_loss: 1.7885 - val_acc: 0.6770\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7850 - acc: 0.6840 - val_loss: 1.7786 - val_acc: 0.6740\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7745 - acc: 0.6853 - val_loss: 1.7688 - val_acc: 0.6750\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7641 - acc: 0.6859 - val_loss: 1.7594 - val_acc: 0.6790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7543 - acc: 0.6875 - val_loss: 1.7481 - val_acc: 0.6760\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7441 - acc: 0.6857 - val_loss: 1.7394 - val_acc: 0.6790\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7349 - acc: 0.6889 - val_loss: 1.7303 - val_acc: 0.6790\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7256 - acc: 0.6867 - val_loss: 1.7208 - val_acc: 0.6770\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7159 - acc: 0.6895 - val_loss: 1.7123 - val_acc: 0.6790\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.7068 - acc: 0.6899 - val_loss: 1.7022 - val_acc: 0.6810\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6980 - acc: 0.6917 - val_loss: 1.6950 - val_acc: 0.6820\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6895 - acc: 0.6908 - val_loss: 1.6872 - val_acc: 0.6760\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6807 - acc: 0.6917 - val_loss: 1.6810 - val_acc: 0.6790\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6726 - acc: 0.6947 - val_loss: 1.6743 - val_acc: 0.6810\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6646 - acc: 0.6935 - val_loss: 1.6628 - val_acc: 0.6820\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6564 - acc: 0.6931 - val_loss: 1.6568 - val_acc: 0.6850\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6485 - acc: 0.6957 - val_loss: 1.6471 - val_acc: 0.6800\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6411 - acc: 0.6940 - val_loss: 1.6415 - val_acc: 0.6820\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6334 - acc: 0.6941 - val_loss: 1.6341 - val_acc: 0.6820\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6257 - acc: 0.6956 - val_loss: 1.6265 - val_acc: 0.6860\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6182 - acc: 0.6983 - val_loss: 1.6178 - val_acc: 0.6860\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6111 - acc: 0.6983 - val_loss: 1.6137 - val_acc: 0.6830\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6038 - acc: 0.6977 - val_loss: 1.6050 - val_acc: 0.6850\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5964 - acc: 0.6980 - val_loss: 1.5979 - val_acc: 0.6830\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5889 - acc: 0.6995 - val_loss: 1.5941 - val_acc: 0.6830\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5822 - acc: 0.6997 - val_loss: 1.5879 - val_acc: 0.6860\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5754 - acc: 0.6989 - val_loss: 1.5762 - val_acc: 0.6890\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5686 - acc: 0.6997 - val_loss: 1.5734 - val_acc: 0.6830\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5616 - acc: 0.7008 - val_loss: 1.5636 - val_acc: 0.6880\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5549 - acc: 0.7009 - val_loss: 1.5576 - val_acc: 0.6890\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5486 - acc: 0.7020 - val_loss: 1.5547 - val_acc: 0.6810\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5417 - acc: 0.7016 - val_loss: 1.5471 - val_acc: 0.6870\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5352 - acc: 0.7023 - val_loss: 1.5434 - val_acc: 0.6880\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5282 - acc: 0.7043 - val_loss: 1.5319 - val_acc: 0.6910\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5223 - acc: 0.7032 - val_loss: 1.5252 - val_acc: 0.6910\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5152 - acc: 0.7055 - val_loss: 1.5207 - val_acc: 0.6890\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5096 - acc: 0.7032 - val_loss: 1.5154 - val_acc: 0.6880\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5032 - acc: 0.7049 - val_loss: 1.5085 - val_acc: 0.6890\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4973 - acc: 0.7048 - val_loss: 1.5044 - val_acc: 0.6890\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4910 - acc: 0.7064 - val_loss: 1.4962 - val_acc: 0.6910\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4850 - acc: 0.7056 - val_loss: 1.4913 - val_acc: 0.6890\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4790 - acc: 0.7060 - val_loss: 1.4838 - val_acc: 0.6900\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4728 - acc: 0.7057 - val_loss: 1.4792 - val_acc: 0.6940\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4671 - acc: 0.7081 - val_loss: 1.4751 - val_acc: 0.6910\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4618 - acc: 0.7076 - val_loss: 1.4668 - val_acc: 0.6890\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4553 - acc: 0.7092 - val_loss: 1.4632 - val_acc: 0.6910\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4495 - acc: 0.7092 - val_loss: 1.4556 - val_acc: 0.6910\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4436 - acc: 0.7093 - val_loss: 1.4530 - val_acc: 0.6950\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4379 - acc: 0.7112 - val_loss: 1.4533 - val_acc: 0.6900\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4322 - acc: 0.7103 - val_loss: 1.4418 - val_acc: 0.6940\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4270 - acc: 0.7116 - val_loss: 1.4347 - val_acc: 0.6890\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4215 - acc: 0.7093 - val_loss: 1.4301 - val_acc: 0.6910\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4161 - acc: 0.7116 - val_loss: 1.4250 - val_acc: 0.6940\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4110 - acc: 0.7116 - val_loss: 1.4196 - val_acc: 0.6930\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4052 - acc: 0.7139 - val_loss: 1.4150 - val_acc: 0.6950\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4003 - acc: 0.7119 - val_loss: 1.4091 - val_acc: 0.6920\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3948 - acc: 0.7145 - val_loss: 1.4068 - val_acc: 0.6980\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3894 - acc: 0.7156 - val_loss: 1.4004 - val_acc: 0.7010\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3846 - acc: 0.7143 - val_loss: 1.3961 - val_acc: 0.6970\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3792 - acc: 0.7151 - val_loss: 1.3908 - val_acc: 0.7010\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3743 - acc: 0.7147 - val_loss: 1.3830 - val_acc: 0.6950\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3688 - acc: 0.7155 - val_loss: 1.3791 - val_acc: 0.6970\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3636 - acc: 0.7161 - val_loss: 1.3786 - val_acc: 0.6970\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3593 - acc: 0.7169 - val_loss: 1.3739 - val_acc: 0.6980\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3542 - acc: 0.7171 - val_loss: 1.3658 - val_acc: 0.6970\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3488 - acc: 0.7187 - val_loss: 1.3623 - val_acc: 0.7020\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3444 - acc: 0.7175 - val_loss: 1.3565 - val_acc: 0.6990\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3401 - acc: 0.7167 - val_loss: 1.3512 - val_acc: 0.7020\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3350 - acc: 0.7187 - val_loss: 1.3477 - val_acc: 0.6990\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3302 - acc: 0.7188 - val_loss: 1.3445 - val_acc: 0.6990\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3260 - acc: 0.7176 - val_loss: 1.3410 - val_acc: 0.7010\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3217 - acc: 0.7209 - val_loss: 1.3365 - val_acc: 0.7040\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3165 - acc: 0.7203 - val_loss: 1.3297 - val_acc: 0.7010\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3120 - acc: 0.7183 - val_loss: 1.3277 - val_acc: 0.7020\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3079 - acc: 0.7203 - val_loss: 1.3292 - val_acc: 0.6960\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3039 - acc: 0.7215 - val_loss: 1.3196 - val_acc: 0.7010\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2988 - acc: 0.7233 - val_loss: 1.3132 - val_acc: 0.7050\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2950 - acc: 0.7225 - val_loss: 1.3076 - val_acc: 0.6990\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2904 - acc: 0.7225 - val_loss: 1.3101 - val_acc: 0.7000\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2871 - acc: 0.7216 - val_loss: 1.3022 - val_acc: 0.7060\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2820 - acc: 0.7236 - val_loss: 1.2958 - val_acc: 0.7030\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2778 - acc: 0.7240 - val_loss: 1.2974 - val_acc: 0.7000\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2747 - acc: 0.7239 - val_loss: 1.2922 - val_acc: 0.7030\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2705 - acc: 0.7256 - val_loss: 1.2862 - val_acc: 0.7050\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2666 - acc: 0.7243 - val_loss: 1.2815 - val_acc: 0.7030\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2628 - acc: 0.7236 - val_loss: 1.2801 - val_acc: 0.7020\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2587 - acc: 0.7249 - val_loss: 1.2749 - val_acc: 0.7030\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2545 - acc: 0.7269 - val_loss: 1.2706 - val_acc: 0.7030\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2509 - acc: 0.7269 - val_loss: 1.2692 - val_acc: 0.7050\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2477 - acc: 0.7263 - val_loss: 1.2633 - val_acc: 0.7050\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2433 - acc: 0.7283 - val_loss: 1.2590 - val_acc: 0.7060\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2397 - acc: 0.7288 - val_loss: 1.2555 - val_acc: 0.7110\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2367 - acc: 0.7285 - val_loss: 1.2541 - val_acc: 0.7050\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2327 - acc: 0.7289 - val_loss: 1.2494 - val_acc: 0.7050\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2294 - acc: 0.7283 - val_loss: 1.2465 - val_acc: 0.7040\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2262 - acc: 0.7295 - val_loss: 1.2427 - val_acc: 0.7090\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2223 - acc: 0.7301 - val_loss: 1.2456 - val_acc: 0.7090\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2196 - acc: 0.7301 - val_loss: 1.2363 - val_acc: 0.7070\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2157 - acc: 0.7311 - val_loss: 1.2349 - val_acc: 0.7080\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2123 - acc: 0.7313 - val_loss: 1.2325 - val_acc: 0.7080\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2089 - acc: 0.7312 - val_loss: 1.2283 - val_acc: 0.7070\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2062 - acc: 0.7319 - val_loss: 1.2259 - val_acc: 0.7120\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2034 - acc: 0.7315 - val_loss: 1.2224 - val_acc: 0.7070\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1996 - acc: 0.7331 - val_loss: 1.2196 - val_acc: 0.7140\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1968 - acc: 0.7331 - val_loss: 1.2172 - val_acc: 0.7110\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1936 - acc: 0.7316 - val_loss: 1.2155 - val_acc: 0.7150\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1911 - acc: 0.7335 - val_loss: 1.2117 - val_acc: 0.7070\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1877 - acc: 0.7321 - val_loss: 1.2073 - val_acc: 0.7090\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1850 - acc: 0.7305 - val_loss: 1.2081 - val_acc: 0.7140\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1815 - acc: 0.7332 - val_loss: 1.2017 - val_acc: 0.7080\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1789 - acc: 0.7335 - val_loss: 1.2027 - val_acc: 0.7110\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1763 - acc: 0.7340 - val_loss: 1.1978 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1738 - acc: 0.7351 - val_loss: 1.1914 - val_acc: 0.7120\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1701 - acc: 0.7347 - val_loss: 1.1926 - val_acc: 0.7060\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1683 - acc: 0.7345 - val_loss: 1.1896 - val_acc: 0.7120\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1655 - acc: 0.7352 - val_loss: 1.1879 - val_acc: 0.7140\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1625 - acc: 0.7360 - val_loss: 1.1853 - val_acc: 0.7150\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1599 - acc: 0.7363 - val_loss: 1.1808 - val_acc: 0.7090\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1571 - acc: 0.7371 - val_loss: 1.1786 - val_acc: 0.7130\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1552 - acc: 0.7361 - val_loss: 1.1775 - val_acc: 0.7160\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1527 - acc: 0.7356 - val_loss: 1.1769 - val_acc: 0.7190\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1501 - acc: 0.7376 - val_loss: 1.1701 - val_acc: 0.7180\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1475 - acc: 0.7377 - val_loss: 1.1770 - val_acc: 0.7100\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1450 - acc: 0.7352 - val_loss: 1.1669 - val_acc: 0.7170\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1431 - acc: 0.7381 - val_loss: 1.1647 - val_acc: 0.7170\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1408 - acc: 0.7372 - val_loss: 1.1655 - val_acc: 0.7130\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1383 - acc: 0.7372 - val_loss: 1.1615 - val_acc: 0.7140\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1364 - acc: 0.7365 - val_loss: 1.1604 - val_acc: 0.7180\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1345 - acc: 0.7385 - val_loss: 1.1583 - val_acc: 0.7170\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1319 - acc: 0.7380 - val_loss: 1.1612 - val_acc: 0.7150\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1300 - acc: 0.7379 - val_loss: 1.1533 - val_acc: 0.7140\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1275 - acc: 0.7371 - val_loss: 1.1536 - val_acc: 0.7160\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1257 - acc: 0.7377 - val_loss: 1.1491 - val_acc: 0.7180\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1237 - acc: 0.7392 - val_loss: 1.1513 - val_acc: 0.7170\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1223 - acc: 0.7391 - val_loss: 1.1489 - val_acc: 0.7200\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1206 - acc: 0.7399 - val_loss: 1.1435 - val_acc: 0.7160\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1182 - acc: 0.7403 - val_loss: 1.1439 - val_acc: 0.7190\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1166 - acc: 0.7403 - val_loss: 1.1411 - val_acc: 0.7120\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1146 - acc: 0.7396 - val_loss: 1.1400 - val_acc: 0.7180\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1128 - acc: 0.7404 - val_loss: 1.1372 - val_acc: 0.7160\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1106 - acc: 0.7400 - val_loss: 1.1351 - val_acc: 0.7200\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1093 - acc: 0.7413 - val_loss: 1.1370 - val_acc: 0.7150\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1077 - acc: 0.7404 - val_loss: 1.1352 - val_acc: 0.7170\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1057 - acc: 0.7431 - val_loss: 1.1314 - val_acc: 0.7130\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1043 - acc: 0.7427 - val_loss: 1.1342 - val_acc: 0.7190\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1027 - acc: 0.7415 - val_loss: 1.1284 - val_acc: 0.7180\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1007 - acc: 0.7429 - val_loss: 1.1312 - val_acc: 0.7180\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0994 - acc: 0.7419 - val_loss: 1.1263 - val_acc: 0.7140\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0973 - acc: 0.7416 - val_loss: 1.1259 - val_acc: 0.7180\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0960 - acc: 0.7425 - val_loss: 1.1244 - val_acc: 0.7150\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0947 - acc: 0.7431 - val_loss: 1.1258 - val_acc: 0.7210\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0929 - acc: 0.7439 - val_loss: 1.1201 - val_acc: 0.7170\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0908 - acc: 0.7433 - val_loss: 1.1201 - val_acc: 0.7210\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0898 - acc: 0.7433 - val_loss: 1.1189 - val_acc: 0.7180\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0881 - acc: 0.7424 - val_loss: 1.1166 - val_acc: 0.7180\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0861 - acc: 0.7432 - val_loss: 1.1131 - val_acc: 0.7200\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0849 - acc: 0.7449 - val_loss: 1.1163 - val_acc: 0.7190\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0833 - acc: 0.7431 - val_loss: 1.1112 - val_acc: 0.7190\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0817 - acc: 0.7445 - val_loss: 1.1094 - val_acc: 0.7160\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0805 - acc: 0.7428 - val_loss: 1.1054 - val_acc: 0.7190\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0787 - acc: 0.7451 - val_loss: 1.1078 - val_acc: 0.7180\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0775 - acc: 0.7443 - val_loss: 1.1053 - val_acc: 0.7170\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0759 - acc: 0.7468 - val_loss: 1.1072 - val_acc: 0.7170\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0746 - acc: 0.7447 - val_loss: 1.1028 - val_acc: 0.7190\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0731 - acc: 0.7449 - val_loss: 1.1036 - val_acc: 0.7290\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0722 - acc: 0.7467 - val_loss: 1.1028 - val_acc: 0.7150\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0705 - acc: 0.7455 - val_loss: 1.1003 - val_acc: 0.7200\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0686 - acc: 0.7459 - val_loss: 1.1009 - val_acc: 0.7180\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0680 - acc: 0.7452 - val_loss: 1.0974 - val_acc: 0.7220\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0661 - acc: 0.7463 - val_loss: 1.0954 - val_acc: 0.7200\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0646 - acc: 0.7461 - val_loss: 1.0931 - val_acc: 0.7220\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0635 - acc: 0.7453 - val_loss: 1.0989 - val_acc: 0.7190\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0622 - acc: 0.7473 - val_loss: 1.0920 - val_acc: 0.7220\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0610 - acc: 0.7472 - val_loss: 1.0895 - val_acc: 0.7230\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0595 - acc: 0.7465 - val_loss: 1.0896 - val_acc: 0.7210\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0588 - acc: 0.7481 - val_loss: 1.0892 - val_acc: 0.7210\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0576 - acc: 0.7479 - val_loss: 1.0886 - val_acc: 0.7240\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0559 - acc: 0.7467 - val_loss: 1.0909 - val_acc: 0.7240\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0554 - acc: 0.7483 - val_loss: 1.0849 - val_acc: 0.7220\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0534 - acc: 0.7472 - val_loss: 1.0875 - val_acc: 0.7230\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0523 - acc: 0.7483 - val_loss: 1.0849 - val_acc: 0.7260\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0511 - acc: 0.7484 - val_loss: 1.0861 - val_acc: 0.7260\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0507 - acc: 0.7481 - val_loss: 1.0831 - val_acc: 0.7260\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0495 - acc: 0.7488 - val_loss: 1.0807 - val_acc: 0.7300\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0478 - acc: 0.7489 - val_loss: 1.0786 - val_acc: 0.7220\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0462 - acc: 0.7496 - val_loss: 1.0790 - val_acc: 0.7260\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0455 - acc: 0.7489 - val_loss: 1.0758 - val_acc: 0.7270\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0440 - acc: 0.7489 - val_loss: 1.0764 - val_acc: 0.7250\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0428 - acc: 0.7497 - val_loss: 1.0743 - val_acc: 0.7240\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0420 - acc: 0.7501 - val_loss: 1.0781 - val_acc: 0.7270\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0410 - acc: 0.7499 - val_loss: 1.0758 - val_acc: 0.7220\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0401 - acc: 0.7497 - val_loss: 1.0768 - val_acc: 0.7190\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0402 - acc: 0.7515 - val_loss: 1.0703 - val_acc: 0.7240\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0377 - acc: 0.7512 - val_loss: 1.0706 - val_acc: 0.7270\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0371 - acc: 0.7503 - val_loss: 1.0714 - val_acc: 0.7230\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0359 - acc: 0.7501 - val_loss: 1.0675 - val_acc: 0.7280\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0346 - acc: 0.7513 - val_loss: 1.0700 - val_acc: 0.7250\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0343 - acc: 0.7520 - val_loss: 1.0660 - val_acc: 0.7280\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0330 - acc: 0.7507 - val_loss: 1.0653 - val_acc: 0.7270\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0322 - acc: 0.7503 - val_loss: 1.0732 - val_acc: 0.7300\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0322 - acc: 0.7512 - val_loss: 1.0617 - val_acc: 0.7280\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0300 - acc: 0.7512 - val_loss: 1.0627 - val_acc: 0.7250\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0301 - acc: 0.7499 - val_loss: 1.0615 - val_acc: 0.7270\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0280 - acc: 0.7523 - val_loss: 1.0608 - val_acc: 0.7290\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0272 - acc: 0.7517 - val_loss: 1.0604 - val_acc: 0.7230\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0263 - acc: 0.7520 - val_loss: 1.0586 - val_acc: 0.7270\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0255 - acc: 0.7520 - val_loss: 1.0700 - val_acc: 0.7210\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0253 - acc: 0.7515 - val_loss: 1.0602 - val_acc: 0.7220\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0237 - acc: 0.7513 - val_loss: 1.0562 - val_acc: 0.7260\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0228 - acc: 0.7523 - val_loss: 1.0567 - val_acc: 0.7290\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0217 - acc: 0.7529 - val_loss: 1.0590 - val_acc: 0.7280\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0213 - acc: 0.7516 - val_loss: 1.0553 - val_acc: 0.7290\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0202 - acc: 0.7520 - val_loss: 1.0579 - val_acc: 0.7290\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0198 - acc: 0.7536 - val_loss: 1.0536 - val_acc: 0.7280\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0184 - acc: 0.7544 - val_loss: 1.0598 - val_acc: 0.7200\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0182 - acc: 0.7549 - val_loss: 1.0495 - val_acc: 0.7240\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0167 - acc: 0.7527 - val_loss: 1.0513 - val_acc: 0.7260\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0157 - acc: 0.7539 - val_loss: 1.0536 - val_acc: 0.7290\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0154 - acc: 0.7532 - val_loss: 1.0557 - val_acc: 0.7220\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0143 - acc: 0.7535 - val_loss: 1.0509 - val_acc: 0.7320\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0134 - acc: 0.7544 - val_loss: 1.0566 - val_acc: 0.7280\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0135 - acc: 0.7540 - val_loss: 1.0471 - val_acc: 0.7310\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0120 - acc: 0.7547 - val_loss: 1.0448 - val_acc: 0.7290\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0105 - acc: 0.7543 - val_loss: 1.0471 - val_acc: 0.7280\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0104 - acc: 0.7547 - val_loss: 1.0556 - val_acc: 0.7230\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0101 - acc: 0.7536 - val_loss: 1.0492 - val_acc: 0.7280\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0089 - acc: 0.7549 - val_loss: 1.0427 - val_acc: 0.7330\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0078 - acc: 0.7551 - val_loss: 1.0483 - val_acc: 0.7180\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0082 - acc: 0.7543 - val_loss: 1.0441 - val_acc: 0.7310\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0067 - acc: 0.7556 - val_loss: 1.0424 - val_acc: 0.7340\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0059 - acc: 0.7533 - val_loss: 1.0414 - val_acc: 0.7300\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0052 - acc: 0.7565 - val_loss: 1.0416 - val_acc: 0.7320\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0042 - acc: 0.7552 - val_loss: 1.0421 - val_acc: 0.7320\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0033 - acc: 0.7563 - val_loss: 1.0498 - val_acc: 0.7300\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0035 - acc: 0.7548 - val_loss: 1.0361 - val_acc: 0.7330\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0014 - acc: 0.7548 - val_loss: 1.0399 - val_acc: 0.7280\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0008 - acc: 0.7548 - val_loss: 1.0353 - val_acc: 0.7350\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0004 - acc: 0.7567 - val_loss: 1.0338 - val_acc: 0.7310\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9992 - acc: 0.7576 - val_loss: 1.0352 - val_acc: 0.7320\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9991 - acc: 0.7553 - val_loss: 1.0348 - val_acc: 0.7310\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9990 - acc: 0.7567 - val_loss: 1.0336 - val_acc: 0.7300\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9971 - acc: 0.7565 - val_loss: 1.0352 - val_acc: 0.7260\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9971 - acc: 0.7564 - val_loss: 1.0338 - val_acc: 0.7310\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9958 - acc: 0.7559 - val_loss: 1.0321 - val_acc: 0.7300\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9946 - acc: 0.7575 - val_loss: 1.0345 - val_acc: 0.7330\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9941 - acc: 0.7549 - val_loss: 1.0324 - val_acc: 0.7290\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9944 - acc: 0.7576 - val_loss: 1.0317 - val_acc: 0.7280\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9934 - acc: 0.7580 - val_loss: 1.0361 - val_acc: 0.7310\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9933 - acc: 0.7573 - val_loss: 1.0325 - val_acc: 0.7340\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9919 - acc: 0.7580 - val_loss: 1.0299 - val_acc: 0.7310\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9912 - acc: 0.7573 - val_loss: 1.0297 - val_acc: 0.7290\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9907 - acc: 0.7568 - val_loss: 1.0266 - val_acc: 0.7330\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9897 - acc: 0.7585 - val_loss: 1.0277 - val_acc: 0.7300\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9890 - acc: 0.7587 - val_loss: 1.0277 - val_acc: 0.7350\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9887 - acc: 0.7573 - val_loss: 1.0252 - val_acc: 0.7300\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9876 - acc: 0.7587 - val_loss: 1.0242 - val_acc: 0.7320\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9869 - acc: 0.7580 - val_loss: 1.0280 - val_acc: 0.7310\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9868 - acc: 0.7576 - val_loss: 1.0253 - val_acc: 0.7340\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9859 - acc: 0.7595 - val_loss: 1.0224 - val_acc: 0.7310\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9848 - acc: 0.7587 - val_loss: 1.0237 - val_acc: 0.7290\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9844 - acc: 0.7603 - val_loss: 1.0229 - val_acc: 0.7310\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9843 - acc: 0.7581 - val_loss: 1.0238 - val_acc: 0.7310\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9838 - acc: 0.7577 - val_loss: 1.0197 - val_acc: 0.7320\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9820 - acc: 0.7591 - val_loss: 1.0180 - val_acc: 0.7310\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9815 - acc: 0.7599 - val_loss: 1.0210 - val_acc: 0.7330\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9812 - acc: 0.7595 - val_loss: 1.0189 - val_acc: 0.7330\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9806 - acc: 0.7597 - val_loss: 1.0242 - val_acc: 0.7320\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9804 - acc: 0.7592 - val_loss: 1.0168 - val_acc: 0.7320\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9797 - acc: 0.7595 - val_loss: 1.0195 - val_acc: 0.7350\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9786 - acc: 0.7600 - val_loss: 1.0188 - val_acc: 0.7290\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9788 - acc: 0.7605 - val_loss: 1.0207 - val_acc: 0.7300\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9779 - acc: 0.7588 - val_loss: 1.0173 - val_acc: 0.7320\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9770 - acc: 0.7611 - val_loss: 1.0157 - val_acc: 0.7320\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9769 - acc: 0.7597 - val_loss: 1.0191 - val_acc: 0.7320\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9758 - acc: 0.7600 - val_loss: 1.0152 - val_acc: 0.7320\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9747 - acc: 0.7604 - val_loss: 1.0149 - val_acc: 0.7330\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9747 - acc: 0.7600 - val_loss: 1.0134 - val_acc: 0.7300\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9738 - acc: 0.7605 - val_loss: 1.0108 - val_acc: 0.7340\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9731 - acc: 0.7603 - val_loss: 1.0149 - val_acc: 0.7290\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9727 - acc: 0.7609 - val_loss: 1.0157 - val_acc: 0.7320\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9723 - acc: 0.7620 - val_loss: 1.0109 - val_acc: 0.7340\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9712 - acc: 0.7617 - val_loss: 1.0216 - val_acc: 0.7290\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9707 - acc: 0.7605 - val_loss: 1.0096 - val_acc: 0.7310\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9697 - acc: 0.7615 - val_loss: 1.0113 - val_acc: 0.7330\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9694 - acc: 0.7599 - val_loss: 1.0104 - val_acc: 0.7320\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9694 - acc: 0.7611 - val_loss: 1.0148 - val_acc: 0.7300\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9691 - acc: 0.7609 - val_loss: 1.0067 - val_acc: 0.7330\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9681 - acc: 0.7619 - val_loss: 1.0179 - val_acc: 0.7280\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9681 - acc: 0.7616 - val_loss: 1.0120 - val_acc: 0.7320\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9668 - acc: 0.7601 - val_loss: 1.0110 - val_acc: 0.7310\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9667 - acc: 0.7608 - val_loss: 1.0052 - val_acc: 0.7360\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9654 - acc: 0.7623 - val_loss: 1.0139 - val_acc: 0.7350\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9648 - acc: 0.7632 - val_loss: 1.0082 - val_acc: 0.7350\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9647 - acc: 0.7615 - val_loss: 1.0059 - val_acc: 0.7360\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9640 - acc: 0.7612 - val_loss: 1.0040 - val_acc: 0.7340\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9639 - acc: 0.7620 - val_loss: 1.0083 - val_acc: 0.7320\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9634 - acc: 0.7625 - val_loss: 1.0072 - val_acc: 0.7350\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9627 - acc: 0.7617 - val_loss: 1.0083 - val_acc: 0.7310\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9622 - acc: 0.7632 - val_loss: 1.0038 - val_acc: 0.7350\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9611 - acc: 0.7625 - val_loss: 1.0051 - val_acc: 0.7330\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9610 - acc: 0.7609 - val_loss: 1.0043 - val_acc: 0.7350\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9598 - acc: 0.7625 - val_loss: 1.0021 - val_acc: 0.7320\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9596 - acc: 0.7627 - val_loss: 1.0042 - val_acc: 0.7380\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9596 - acc: 0.7620 - val_loss: 1.0011 - val_acc: 0.7320\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9593 - acc: 0.7631 - val_loss: 1.0016 - val_acc: 0.7360\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9584 - acc: 0.7632 - val_loss: 1.0020 - val_acc: 0.7350\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9584 - acc: 0.7629 - val_loss: 1.0021 - val_acc: 0.7390\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9575 - acc: 0.7616 - val_loss: 0.9998 - val_acc: 0.7390\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9568 - acc: 0.7629 - val_loss: 1.0009 - val_acc: 0.7380\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9570 - acc: 0.7623 - val_loss: 1.0000 - val_acc: 0.7370\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9556 - acc: 0.7648 - val_loss: 0.9983 - val_acc: 0.7380\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9553 - acc: 0.7628 - val_loss: 0.9998 - val_acc: 0.7340\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9544 - acc: 0.7645 - val_loss: 1.0053 - val_acc: 0.7340\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9545 - acc: 0.7633 - val_loss: 0.9977 - val_acc: 0.7330\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9544 - acc: 0.7633 - val_loss: 1.0054 - val_acc: 0.7320\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9536 - acc: 0.7644 - val_loss: 0.9997 - val_acc: 0.7350\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9537 - acc: 0.7637 - val_loss: 0.9951 - val_acc: 0.7340\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9525 - acc: 0.7632 - val_loss: 0.9949 - val_acc: 0.7370\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9524 - acc: 0.7648 - val_loss: 1.0035 - val_acc: 0.7410\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9516 - acc: 0.7645 - val_loss: 0.9936 - val_acc: 0.7400\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9504 - acc: 0.7648 - val_loss: 0.9966 - val_acc: 0.7380\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9511 - acc: 0.7635 - val_loss: 0.9943 - val_acc: 0.7340\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9499 - acc: 0.7640 - val_loss: 0.9940 - val_acc: 0.7390\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9496 - acc: 0.7656 - val_loss: 0.9974 - val_acc: 0.7370\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9496 - acc: 0.7643 - val_loss: 0.9979 - val_acc: 0.7320\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9493 - acc: 0.7659 - val_loss: 0.9927 - val_acc: 0.7370\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9479 - acc: 0.7636 - val_loss: 1.0028 - val_acc: 0.7390\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9487 - acc: 0.7641 - val_loss: 0.9939 - val_acc: 0.7380\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9470 - acc: 0.7635 - val_loss: 0.9936 - val_acc: 0.7350\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9470 - acc: 0.7645 - val_loss: 0.9938 - val_acc: 0.7340\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9468 - acc: 0.7651 - val_loss: 0.9887 - val_acc: 0.7380\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9460 - acc: 0.7624 - val_loss: 0.9920 - val_acc: 0.7380\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9450 - acc: 0.7637 - val_loss: 0.9884 - val_acc: 0.7340\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9443 - acc: 0.7640 - val_loss: 0.9893 - val_acc: 0.7330\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9442 - acc: 0.7661 - val_loss: 0.9922 - val_acc: 0.7380\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9435 - acc: 0.7644 - val_loss: 0.9907 - val_acc: 0.7370\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9442 - acc: 0.7645 - val_loss: 0.9910 - val_acc: 0.7370\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9422 - acc: 0.7655 - val_loss: 0.9888 - val_acc: 0.7400\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9423 - acc: 0.7663 - val_loss: 0.9870 - val_acc: 0.7370\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9424 - acc: 0.7645 - val_loss: 0.9908 - val_acc: 0.7430\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9419 - acc: 0.7659 - val_loss: 0.9883 - val_acc: 0.7380\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9411 - acc: 0.7648 - val_loss: 0.9907 - val_acc: 0.7400\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9405 - acc: 0.7652 - val_loss: 0.9858 - val_acc: 0.7390\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9405 - acc: 0.7665 - val_loss: 0.9857 - val_acc: 0.7420\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9410 - acc: 0.7643 - val_loss: 0.9880 - val_acc: 0.7410\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9392 - acc: 0.7663 - val_loss: 0.9919 - val_acc: 0.7370\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9395 - acc: 0.7668 - val_loss: 0.9882 - val_acc: 0.7390\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9391 - acc: 0.7656 - val_loss: 0.9897 - val_acc: 0.7370\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9374 - acc: 0.7689 - val_loss: 0.9831 - val_acc: 0.7380\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9369 - acc: 0.7661 - val_loss: 0.9883 - val_acc: 0.7420\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9375 - acc: 0.7668 - val_loss: 0.9830 - val_acc: 0.7380\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9367 - acc: 0.7681 - val_loss: 0.9976 - val_acc: 0.7340\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9382 - acc: 0.7659 - val_loss: 0.9838 - val_acc: 0.7370\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9362 - acc: 0.7664 - val_loss: 0.9836 - val_acc: 0.7370\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9352 - acc: 0.7668 - val_loss: 0.9832 - val_acc: 0.7360\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9346 - acc: 0.7676 - val_loss: 0.9822 - val_acc: 0.7450\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9342 - acc: 0.7671 - val_loss: 0.9818 - val_acc: 0.7350\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9342 - acc: 0.7665 - val_loss: 0.9830 - val_acc: 0.7430\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9333 - acc: 0.7669 - val_loss: 0.9828 - val_acc: 0.7440\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9333 - acc: 0.7664 - val_loss: 0.9834 - val_acc: 0.7410\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9326 - acc: 0.7676 - val_loss: 0.9896 - val_acc: 0.7380\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9327 - acc: 0.7676 - val_loss: 0.9857 - val_acc: 0.7370\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9320 - acc: 0.7684 - val_loss: 0.9825 - val_acc: 0.7430\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9320 - acc: 0.7665 - val_loss: 0.9834 - val_acc: 0.7390\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9316 - acc: 0.7667 - val_loss: 0.9846 - val_acc: 0.7360\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9309 - acc: 0.7668 - val_loss: 0.9800 - val_acc: 0.7390\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9307 - acc: 0.7672 - val_loss: 0.9758 - val_acc: 0.7390\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9298 - acc: 0.7681 - val_loss: 0.9811 - val_acc: 0.7360\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9300 - acc: 0.7681 - val_loss: 0.9824 - val_acc: 0.7440\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9292 - acc: 0.7668 - val_loss: 0.9846 - val_acc: 0.7330\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9289 - acc: 0.7681 - val_loss: 0.9880 - val_acc: 0.7390\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9292 - acc: 0.7687 - val_loss: 0.9789 - val_acc: 0.7410\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9285 - acc: 0.7683 - val_loss: 0.9744 - val_acc: 0.7400\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9275 - acc: 0.7685 - val_loss: 0.9833 - val_acc: 0.7430\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9276 - acc: 0.7693 - val_loss: 0.9786 - val_acc: 0.7370\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9266 - acc: 0.7680 - val_loss: 0.9762 - val_acc: 0.7420\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9258 - acc: 0.7703 - val_loss: 0.9778 - val_acc: 0.7410\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9260 - acc: 0.7691 - val_loss: 0.9753 - val_acc: 0.7370\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9254 - acc: 0.7681 - val_loss: 0.9797 - val_acc: 0.7370\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9254 - acc: 0.7704 - val_loss: 0.9749 - val_acc: 0.7390\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9256 - acc: 0.7679 - val_loss: 0.9752 - val_acc: 0.7430\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9241 - acc: 0.7699 - val_loss: 0.9740 - val_acc: 0.7380\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9247 - acc: 0.7695 - val_loss: 0.9754 - val_acc: 0.7380\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9237 - acc: 0.7709 - val_loss: 0.9718 - val_acc: 0.7380\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9234 - acc: 0.7700 - val_loss: 0.9711 - val_acc: 0.7420\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9233 - acc: 0.7688 - val_loss: 0.9744 - val_acc: 0.7420\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9219 - acc: 0.7695 - val_loss: 0.9971 - val_acc: 0.7320\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9230 - acc: 0.7701 - val_loss: 0.9788 - val_acc: 0.7410\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9225 - acc: 0.7679 - val_loss: 0.9735 - val_acc: 0.7420\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9209 - acc: 0.7703 - val_loss: 0.9713 - val_acc: 0.7410\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9209 - acc: 0.7717 - val_loss: 0.9741 - val_acc: 0.7420\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9199 - acc: 0.7712 - val_loss: 0.9700 - val_acc: 0.7400\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9200 - acc: 0.7701 - val_loss: 0.9691 - val_acc: 0.7400\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9209 - acc: 0.7677 - val_loss: 0.9733 - val_acc: 0.7430\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9193 - acc: 0.7705 - val_loss: 0.9673 - val_acc: 0.7380\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9187 - acc: 0.7699 - val_loss: 0.9702 - val_acc: 0.7420\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9180 - acc: 0.7701 - val_loss: 0.9732 - val_acc: 0.7400\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9180 - acc: 0.7699 - val_loss: 0.9723 - val_acc: 0.7410\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9177 - acc: 0.7695 - val_loss: 0.9694 - val_acc: 0.7410\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9177 - acc: 0.7704 - val_loss: 0.9706 - val_acc: 0.7380\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9171 - acc: 0.7716 - val_loss: 0.9727 - val_acc: 0.7380\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9167 - acc: 0.7692 - val_loss: 0.9701 - val_acc: 0.7410\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9153 - acc: 0.7713 - val_loss: 0.9777 - val_acc: 0.7390\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9162 - acc: 0.7703 - val_loss: 0.9679 - val_acc: 0.7430\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9154 - acc: 0.7696 - val_loss: 0.9683 - val_acc: 0.7400\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9160 - acc: 0.7705 - val_loss: 0.9749 - val_acc: 0.7360\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9154 - acc: 0.7699 - val_loss: 0.9763 - val_acc: 0.7360\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9141 - acc: 0.7713 - val_loss: 0.9705 - val_acc: 0.7370\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9155 - acc: 0.7696 - val_loss: 0.9730 - val_acc: 0.7410\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9138 - acc: 0.7709 - val_loss: 0.9708 - val_acc: 0.7430\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9135 - acc: 0.7723 - val_loss: 0.9702 - val_acc: 0.7410\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9129 - acc: 0.7696 - val_loss: 0.9730 - val_acc: 0.7380\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9133 - acc: 0.7715 - val_loss: 0.9641 - val_acc: 0.7400\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9137 - acc: 0.7716 - val_loss: 0.9641 - val_acc: 0.7440\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9123 - acc: 0.7715 - val_loss: 0.9696 - val_acc: 0.7370\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9125 - acc: 0.7719 - val_loss: 0.9669 - val_acc: 0.7390\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9110 - acc: 0.7717 - val_loss: 0.9662 - val_acc: 0.7420\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9104 - acc: 0.7697 - val_loss: 0.9654 - val_acc: 0.7400\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9098 - acc: 0.7719 - val_loss: 0.9667 - val_acc: 0.7400\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9095 - acc: 0.7708 - val_loss: 0.9681 - val_acc: 0.7440\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9091 - acc: 0.7723 - val_loss: 0.9647 - val_acc: 0.7440\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9093 - acc: 0.7727 - val_loss: 0.9614 - val_acc: 0.7410\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9082 - acc: 0.7721 - val_loss: 0.9639 - val_acc: 0.7370\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9081 - acc: 0.7707 - val_loss: 0.9651 - val_acc: 0.7430\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9080 - acc: 0.7720 - val_loss: 0.9659 - val_acc: 0.7460\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9073 - acc: 0.7709 - val_loss: 0.9734 - val_acc: 0.7430\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9069 - acc: 0.7716 - val_loss: 0.9731 - val_acc: 0.7400\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9072 - acc: 0.7725 - val_loss: 0.9642 - val_acc: 0.7430\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9061 - acc: 0.7724 - val_loss: 0.9651 - val_acc: 0.7440\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9064 - acc: 0.7727 - val_loss: 0.9684 - val_acc: 0.7430\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9060 - acc: 0.7707 - val_loss: 0.9663 - val_acc: 0.7400\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9057 - acc: 0.7715 - val_loss: 0.9621 - val_acc: 0.7380\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9043 - acc: 0.7728 - val_loss: 0.9685 - val_acc: 0.7360\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9049 - acc: 0.7723 - val_loss: 0.9614 - val_acc: 0.7380\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9036 - acc: 0.7717 - val_loss: 0.9611 - val_acc: 0.7430\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9034 - acc: 0.7740 - val_loss: 0.9572 - val_acc: 0.7410\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9032 - acc: 0.7740 - val_loss: 0.9606 - val_acc: 0.7410\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9033 - acc: 0.7717 - val_loss: 0.9558 - val_acc: 0.7420\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9024 - acc: 0.7748 - val_loss: 0.9584 - val_acc: 0.7390\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9026 - acc: 0.7721 - val_loss: 0.9659 - val_acc: 0.7380\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9012 - acc: 0.7713 - val_loss: 0.9631 - val_acc: 0.7380\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9022 - acc: 0.7733 - val_loss: 0.9638 - val_acc: 0.7400\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9009 - acc: 0.7737 - val_loss: 0.9583 - val_acc: 0.7410\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9008 - acc: 0.7731 - val_loss: 0.9628 - val_acc: 0.7420\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9007 - acc: 0.7731 - val_loss: 0.9625 - val_acc: 0.7360\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8998 - acc: 0.7735 - val_loss: 0.9608 - val_acc: 0.7370\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8996 - acc: 0.7727 - val_loss: 0.9571 - val_acc: 0.7470\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8984 - acc: 0.7735 - val_loss: 0.9582 - val_acc: 0.7390\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8988 - acc: 0.7732 - val_loss: 0.9546 - val_acc: 0.7430\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8988 - acc: 0.7732 - val_loss: 0.9729 - val_acc: 0.7420\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8985 - acc: 0.7724 - val_loss: 0.9562 - val_acc: 0.7490\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8983 - acc: 0.7744 - val_loss: 0.9633 - val_acc: 0.7420\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8972 - acc: 0.7745 - val_loss: 0.9507 - val_acc: 0.7410\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8969 - acc: 0.7739 - val_loss: 0.9587 - val_acc: 0.7410\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8957 - acc: 0.7743 - val_loss: 0.9671 - val_acc: 0.7400\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8971 - acc: 0.7723 - val_loss: 0.9615 - val_acc: 0.7360\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8964 - acc: 0.7733 - val_loss: 0.9642 - val_acc: 0.7350\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8948 - acc: 0.7753 - val_loss: 0.9571 - val_acc: 0.7320\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8963 - acc: 0.7747 - val_loss: 0.9536 - val_acc: 0.7430\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8957 - acc: 0.7759 - val_loss: 0.9565 - val_acc: 0.7450\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8941 - acc: 0.7739 - val_loss: 0.9620 - val_acc: 0.7380\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8943 - acc: 0.7744 - val_loss: 0.9520 - val_acc: 0.7430\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8937 - acc: 0.7751 - val_loss: 0.9546 - val_acc: 0.7430\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8949 - acc: 0.7735 - val_loss: 0.9525 - val_acc: 0.7430\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8929 - acc: 0.7756 - val_loss: 0.9540 - val_acc: 0.7430\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8932 - acc: 0.7731 - val_loss: 0.9485 - val_acc: 0.7400\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8929 - acc: 0.7748 - val_loss: 0.9526 - val_acc: 0.7420\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8931 - acc: 0.7736 - val_loss: 0.9577 - val_acc: 0.7460\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8915 - acc: 0.7755 - val_loss: 0.9512 - val_acc: 0.7370\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8917 - acc: 0.7735 - val_loss: 0.9518 - val_acc: 0.7390\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8917 - acc: 0.7757 - val_loss: 0.9492 - val_acc: 0.7430\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8907 - acc: 0.7749 - val_loss: 0.9573 - val_acc: 0.7390\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8911 - acc: 0.7753 - val_loss: 0.9557 - val_acc: 0.7430\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8904 - acc: 0.7763 - val_loss: 0.9547 - val_acc: 0.7420\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8907 - acc: 0.7756 - val_loss: 0.9492 - val_acc: 0.7420\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8897 - acc: 0.7743 - val_loss: 0.9525 - val_acc: 0.7450\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8893 - acc: 0.7747 - val_loss: 0.9584 - val_acc: 0.7300\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8896 - acc: 0.7756 - val_loss: 0.9488 - val_acc: 0.7400\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8880 - acc: 0.7765 - val_loss: 0.9470 - val_acc: 0.7410\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8876 - acc: 0.7764 - val_loss: 0.9492 - val_acc: 0.7390\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8883 - acc: 0.7756 - val_loss: 0.9563 - val_acc: 0.7390\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8877 - acc: 0.7759 - val_loss: 0.9543 - val_acc: 0.7410\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8868 - acc: 0.7767 - val_loss: 0.9477 - val_acc: 0.7390\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8871 - acc: 0.7761 - val_loss: 0.9465 - val_acc: 0.7440\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8871 - acc: 0.7771 - val_loss: 0.9499 - val_acc: 0.7400\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8876 - acc: 0.7747 - val_loss: 0.9471 - val_acc: 0.7430\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8860 - acc: 0.7781 - val_loss: 0.9472 - val_acc: 0.7420\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8874 - acc: 0.7761 - val_loss: 0.9445 - val_acc: 0.7430\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8849 - acc: 0.7763 - val_loss: 0.9443 - val_acc: 0.7390\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8844 - acc: 0.7783 - val_loss: 0.9535 - val_acc: 0.7430\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8843 - acc: 0.7767 - val_loss: 0.9516 - val_acc: 0.7440\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8840 - acc: 0.7773 - val_loss: 0.9523 - val_acc: 0.7390\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8835 - acc: 0.7765 - val_loss: 0.9432 - val_acc: 0.7410\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8837 - acc: 0.7764 - val_loss: 0.9437 - val_acc: 0.7440\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8837 - acc: 0.7767 - val_loss: 0.9511 - val_acc: 0.7400\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8836 - acc: 0.7760 - val_loss: 0.9485 - val_acc: 0.7440\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8833 - acc: 0.7775 - val_loss: 0.9466 - val_acc: 0.7340\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8820 - acc: 0.7772 - val_loss: 0.9463 - val_acc: 0.7440\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8831 - acc: 0.7775 - val_loss: 0.9442 - val_acc: 0.7420\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8827 - acc: 0.7760 - val_loss: 0.9430 - val_acc: 0.7420\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8819 - acc: 0.7772 - val_loss: 0.9413 - val_acc: 0.7390\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8803 - acc: 0.7776 - val_loss: 0.9436 - val_acc: 0.7380\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8810 - acc: 0.7784 - val_loss: 0.9487 - val_acc: 0.7410\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8821 - acc: 0.7768 - val_loss: 0.9479 - val_acc: 0.7410\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8797 - acc: 0.7772 - val_loss: 0.9454 - val_acc: 0.7430\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8811 - acc: 0.7767 - val_loss: 0.9420 - val_acc: 0.7420\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8797 - acc: 0.7783 - val_loss: 0.9471 - val_acc: 0.7380\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8785 - acc: 0.7800 - val_loss: 0.9498 - val_acc: 0.7400\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8785 - acc: 0.7783 - val_loss: 0.9409 - val_acc: 0.7410\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8788 - acc: 0.7771 - val_loss: 0.9497 - val_acc: 0.7460\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8783 - acc: 0.7779 - val_loss: 0.9491 - val_acc: 0.7370\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8792 - acc: 0.7780 - val_loss: 0.9398 - val_acc: 0.7410\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8767 - acc: 0.7805 - val_loss: 0.9401 - val_acc: 0.7410\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8776 - acc: 0.7807 - val_loss: 0.9435 - val_acc: 0.7410\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8769 - acc: 0.7773 - val_loss: 0.9465 - val_acc: 0.7440\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8767 - acc: 0.7781 - val_loss: 0.9403 - val_acc: 0.7380\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8762 - acc: 0.7760 - val_loss: 0.9381 - val_acc: 0.7430\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8753 - acc: 0.7805 - val_loss: 0.9511 - val_acc: 0.7390\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8757 - acc: 0.7783 - val_loss: 0.9542 - val_acc: 0.7450\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8760 - acc: 0.7761 - val_loss: 0.9394 - val_acc: 0.7440\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8755 - acc: 0.7768 - val_loss: 0.9461 - val_acc: 0.7420\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8749 - acc: 0.7771 - val_loss: 0.9385 - val_acc: 0.7400\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8745 - acc: 0.7807 - val_loss: 0.9431 - val_acc: 0.7420\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8741 - acc: 0.7783 - val_loss: 0.9392 - val_acc: 0.7420\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8742 - acc: 0.7796 - val_loss: 0.9377 - val_acc: 0.7400\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8730 - acc: 0.7807 - val_loss: 0.9392 - val_acc: 0.7420\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8732 - acc: 0.7781 - val_loss: 0.9576 - val_acc: 0.7390\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8743 - acc: 0.7772 - val_loss: 0.9426 - val_acc: 0.7420\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8724 - acc: 0.7793 - val_loss: 0.9404 - val_acc: 0.7400\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8720 - acc: 0.7803 - val_loss: 0.9350 - val_acc: 0.7410\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8715 - acc: 0.7795 - val_loss: 0.9400 - val_acc: 0.7470\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8719 - acc: 0.7781 - val_loss: 0.9363 - val_acc: 0.7360\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8710 - acc: 0.7821 - val_loss: 0.9397 - val_acc: 0.7390\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8709 - acc: 0.7783 - val_loss: 0.9400 - val_acc: 0.7470\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8699 - acc: 0.7808 - val_loss: 0.9557 - val_acc: 0.7340\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8704 - acc: 0.7795 - val_loss: 0.9651 - val_acc: 0.7370\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8718 - acc: 0.7796 - val_loss: 0.9335 - val_acc: 0.7420\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8693 - acc: 0.7801 - val_loss: 0.9365 - val_acc: 0.7390\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8698 - acc: 0.7800 - val_loss: 0.9338 - val_acc: 0.7440\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8690 - acc: 0.7784 - val_loss: 0.9372 - val_acc: 0.7380\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8688 - acc: 0.7804 - val_loss: 0.9402 - val_acc: 0.7460\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8692 - acc: 0.7800 - val_loss: 0.9441 - val_acc: 0.7480\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8690 - acc: 0.7784 - val_loss: 0.9389 - val_acc: 0.7380\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8694 - acc: 0.7789 - val_loss: 0.9408 - val_acc: 0.7420\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8683 - acc: 0.7781 - val_loss: 0.9482 - val_acc: 0.7410\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8686 - acc: 0.7811 - val_loss: 0.9436 - val_acc: 0.7320\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8676 - acc: 0.7797 - val_loss: 0.9432 - val_acc: 0.7440\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7819 - val_loss: 0.9349 - val_acc: 0.7400\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8667 - acc: 0.7800 - val_loss: 0.9383 - val_acc: 0.7440\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8671 - acc: 0.7789 - val_loss: 0.9352 - val_acc: 0.7420\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8647 - acc: 0.7807 - val_loss: 0.9507 - val_acc: 0.7360\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8668 - acc: 0.7795 - val_loss: 0.9333 - val_acc: 0.7380\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8646 - acc: 0.7805 - val_loss: 0.9370 - val_acc: 0.7440\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8658 - acc: 0.7815 - val_loss: 0.9503 - val_acc: 0.7390\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8663 - acc: 0.7801 - val_loss: 0.9396 - val_acc: 0.7380\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8647 - acc: 0.7797 - val_loss: 0.9517 - val_acc: 0.7440\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8656 - acc: 0.7795 - val_loss: 0.9422 - val_acc: 0.7460\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8654 - acc: 0.7808 - val_loss: 0.9346 - val_acc: 0.7460\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8651 - acc: 0.7799 - val_loss: 0.9346 - val_acc: 0.7430\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8636 - acc: 0.7809 - val_loss: 0.9362 - val_acc: 0.7430\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8628 - acc: 0.7821 - val_loss: 0.9380 - val_acc: 0.7360\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8641 - acc: 0.7803 - val_loss: 0.9364 - val_acc: 0.7410\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8630 - acc: 0.7816 - val_loss: 0.9334 - val_acc: 0.7450\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8628 - acc: 0.7796 - val_loss: 0.9404 - val_acc: 0.7420\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8623 - acc: 0.7799 - val_loss: 0.9355 - val_acc: 0.7350\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8625 - acc: 0.7812 - val_loss: 0.9325 - val_acc: 0.7440\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8625 - acc: 0.7817 - val_loss: 0.9294 - val_acc: 0.7410\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8626 - acc: 0.7799 - val_loss: 0.9331 - val_acc: 0.7390\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8639 - acc: 0.7831 - val_loss: 0.9566 - val_acc: 0.7390\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8621 - acc: 0.7832 - val_loss: 0.9360 - val_acc: 0.7440\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8611 - acc: 0.7819 - val_loss: 0.9318 - val_acc: 0.7360\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8611 - acc: 0.7803 - val_loss: 0.9332 - val_acc: 0.7430\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8601 - acc: 0.7831 - val_loss: 0.9334 - val_acc: 0.7440\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8598 - acc: 0.7820 - val_loss: 0.9335 - val_acc: 0.7400\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8603 - acc: 0.7804 - val_loss: 0.9301 - val_acc: 0.7410\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8586 - acc: 0.7821 - val_loss: 0.9296 - val_acc: 0.7400\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8595 - acc: 0.7804 - val_loss: 0.9344 - val_acc: 0.7460\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8594 - acc: 0.7805 - val_loss: 0.9320 - val_acc: 0.7450\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8592 - acc: 0.7817 - val_loss: 0.9348 - val_acc: 0.7440\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8586 - acc: 0.7804 - val_loss: 0.9338 - val_acc: 0.7440\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8586 - acc: 0.7823 - val_loss: 0.9411 - val_acc: 0.7470\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8575 - acc: 0.7836 - val_loss: 0.9351 - val_acc: 0.7360\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8575 - acc: 0.7839 - val_loss: 0.9355 - val_acc: 0.7400\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8573 - acc: 0.7821 - val_loss: 0.9320 - val_acc: 0.7450\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8587 - acc: 0.7819 - val_loss: 0.9313 - val_acc: 0.7410\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8580 - acc: 0.7820 - val_loss: 0.9324 - val_acc: 0.7440\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8570 - acc: 0.7832 - val_loss: 0.9307 - val_acc: 0.7360\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8560 - acc: 0.7837 - val_loss: 0.9381 - val_acc: 0.7350\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8577 - acc: 0.7828 - val_loss: 0.9340 - val_acc: 0.7390\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8568 - acc: 0.7812 - val_loss: 0.9336 - val_acc: 0.7390\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8560 - acc: 0.7807 - val_loss: 0.9301 - val_acc: 0.7370\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8586 - acc: 0.7803 - val_loss: 0.9340 - val_acc: 0.7390\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8553 - acc: 0.7819 - val_loss: 0.9341 - val_acc: 0.7460\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8561 - acc: 0.7815 - val_loss: 0.9303 - val_acc: 0.7390\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8553 - acc: 0.7812 - val_loss: 0.9299 - val_acc: 0.7410\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8549 - acc: 0.7815 - val_loss: 0.9456 - val_acc: 0.7410\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8558 - acc: 0.7829 - val_loss: 0.9278 - val_acc: 0.7410\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8547 - acc: 0.7825 - val_loss: 0.9340 - val_acc: 0.7390\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8547 - acc: 0.7821 - val_loss: 0.9340 - val_acc: 0.7410\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8539 - acc: 0.7845 - val_loss: 0.9318 - val_acc: 0.7400\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8544 - acc: 0.7839 - val_loss: 0.9369 - val_acc: 0.7430\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8535 - acc: 0.7821 - val_loss: 0.9500 - val_acc: 0.7400\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8542 - acc: 0.7820 - val_loss: 0.9253 - val_acc: 0.7410\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.8533 - acc: 0.7827 - val_loss: 0.9239 - val_acc: 0.7410\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8525 - acc: 0.7835 - val_loss: 0.9293 - val_acc: 0.7380\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8531 - acc: 0.7823 - val_loss: 0.9262 - val_acc: 0.7360\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8518 - acc: 0.7821 - val_loss: 0.9278 - val_acc: 0.7360\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8519 - acc: 0.7835 - val_loss: 0.9256 - val_acc: 0.7370\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8530 - acc: 0.7836 - val_loss: 0.9287 - val_acc: 0.7350\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8532 - acc: 0.7823 - val_loss: 0.9285 - val_acc: 0.7420\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8523 - acc: 0.7808 - val_loss: 0.9288 - val_acc: 0.7410\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8519 - acc: 0.7820 - val_loss: 0.9342 - val_acc: 0.7470\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8507 - acc: 0.7836 - val_loss: 0.9356 - val_acc: 0.7390\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8507 - acc: 0.7839 - val_loss: 0.9438 - val_acc: 0.7420\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8515 - acc: 0.7841 - val_loss: 0.9412 - val_acc: 0.7410\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8512 - acc: 0.7813 - val_loss: 0.9274 - val_acc: 0.7420\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8492 - acc: 0.7844 - val_loss: 0.9306 - val_acc: 0.7370\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8507 - acc: 0.7847 - val_loss: 0.9454 - val_acc: 0.7480\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8490 - acc: 0.7835 - val_loss: 0.9274 - val_acc: 0.7470\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8494 - acc: 0.7836 - val_loss: 0.9272 - val_acc: 0.7380\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8489 - acc: 0.7835 - val_loss: 0.9284 - val_acc: 0.7370\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8501 - acc: 0.7828 - val_loss: 0.9375 - val_acc: 0.7380\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8482 - acc: 0.7851 - val_loss: 0.9240 - val_acc: 0.7400\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8490 - acc: 0.7837 - val_loss: 0.9440 - val_acc: 0.7440\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8493 - acc: 0.7815 - val_loss: 0.9404 - val_acc: 0.7410\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8496 - acc: 0.7836 - val_loss: 0.9391 - val_acc: 0.7370\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8511 - acc: 0.7825 - val_loss: 0.9437 - val_acc: 0.7330\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8496 - acc: 0.7833 - val_loss: 0.9231 - val_acc: 0.7420\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8484 - acc: 0.7828 - val_loss: 0.9282 - val_acc: 0.7400\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8477 - acc: 0.7837 - val_loss: 0.9250 - val_acc: 0.7420\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8465 - acc: 0.7840 - val_loss: 0.9284 - val_acc: 0.7390\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8467 - acc: 0.7833 - val_loss: 0.9750 - val_acc: 0.7280\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8486 - acc: 0.7832 - val_loss: 0.9295 - val_acc: 0.7410\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8467 - acc: 0.7855 - val_loss: 0.9321 - val_acc: 0.7430\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8466 - acc: 0.7844 - val_loss: 0.9301 - val_acc: 0.7380\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8456 - acc: 0.7857 - val_loss: 0.9274 - val_acc: 0.7410\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8461 - acc: 0.7824 - val_loss: 0.9521 - val_acc: 0.7410\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8465 - acc: 0.7857 - val_loss: 0.9381 - val_acc: 0.7440\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8468 - acc: 0.7848 - val_loss: 0.9452 - val_acc: 0.7310\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8465 - acc: 0.7843 - val_loss: 0.9365 - val_acc: 0.7360\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8462 - acc: 0.7837 - val_loss: 0.9273 - val_acc: 0.7450\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8459 - acc: 0.7848 - val_loss: 0.9359 - val_acc: 0.7440\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8439 - acc: 0.7848 - val_loss: 0.9375 - val_acc: 0.7400\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8449 - acc: 0.7837 - val_loss: 0.9368 - val_acc: 0.7480\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8457 - acc: 0.7828 - val_loss: 0.9330 - val_acc: 0.7410\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8445 - acc: 0.7839 - val_loss: 0.9411 - val_acc: 0.7400\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8417 - acc: 0.7853 - val_loss: 0.9222 - val_acc: 0.7440\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8439 - acc: 0.7836 - val_loss: 0.9276 - val_acc: 0.7410\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8433 - acc: 0.7857 - val_loss: 0.9447 - val_acc: 0.7410\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8443 - acc: 0.7845 - val_loss: 0.9376 - val_acc: 0.7430\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8436 - acc: 0.7855 - val_loss: 0.9222 - val_acc: 0.7380\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8428 - acc: 0.7845 - val_loss: 0.9274 - val_acc: 0.7450\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8421 - acc: 0.7845 - val_loss: 0.9304 - val_acc: 0.7430\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8425 - acc: 0.7864 - val_loss: 0.9345 - val_acc: 0.7350\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8428 - acc: 0.7853 - val_loss: 0.9435 - val_acc: 0.7390\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8403 - acc: 0.7844 - val_loss: 0.9696 - val_acc: 0.7320\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8440 - acc: 0.7844 - val_loss: 0.9204 - val_acc: 0.7420\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8415 - acc: 0.7873 - val_loss: 0.9262 - val_acc: 0.7440\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8415 - acc: 0.7853 - val_loss: 0.9328 - val_acc: 0.7400\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8409 - acc: 0.7845 - val_loss: 0.9870 - val_acc: 0.7240\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8423 - acc: 0.7839 - val_loss: 0.9359 - val_acc: 0.7440\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8396 - acc: 0.7851 - val_loss: 0.9314 - val_acc: 0.7410\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8408 - acc: 0.7855 - val_loss: 0.9224 - val_acc: 0.7410\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8405 - acc: 0.7848 - val_loss: 0.9203 - val_acc: 0.7450\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8401 - acc: 0.7879 - val_loss: 0.9327 - val_acc: 0.7420\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8400 - acc: 0.7865 - val_loss: 0.9185 - val_acc: 0.7460\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8397 - acc: 0.7876 - val_loss: 0.9488 - val_acc: 0.7400\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8412 - acc: 0.7868 - val_loss: 0.9241 - val_acc: 0.7450\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8390 - acc: 0.7863 - val_loss: 0.9184 - val_acc: 0.7390\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8377 - acc: 0.7857 - val_loss: 0.9228 - val_acc: 0.7480\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8374 - acc: 0.7861 - val_loss: 0.9253 - val_acc: 0.7320\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8384 - acc: 0.7861 - val_loss: 0.9250 - val_acc: 0.7320\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8376 - acc: 0.7869 - val_loss: 0.9452 - val_acc: 0.7370\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8406 - acc: 0.7855 - val_loss: 0.9197 - val_acc: 0.7390\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8397 - acc: 0.7844 - val_loss: 0.9178 - val_acc: 0.7470\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8386 - acc: 0.7861 - val_loss: 0.9224 - val_acc: 0.7450\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8392 - acc: 0.7851 - val_loss: 0.9410 - val_acc: 0.7340\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8394 - acc: 0.7861 - val_loss: 0.9241 - val_acc: 0.7460\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8377 - acc: 0.7856 - val_loss: 0.9225 - val_acc: 0.7480\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8383 - acc: 0.7869 - val_loss: 0.9249 - val_acc: 0.7460\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8379 - acc: 0.7841 - val_loss: 0.9197 - val_acc: 0.7360\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8362 - acc: 0.7849 - val_loss: 0.9437 - val_acc: 0.7360\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8370 - acc: 0.7856 - val_loss: 0.9443 - val_acc: 0.7320\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8377 - acc: 0.7876 - val_loss: 0.9182 - val_acc: 0.7420\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8352 - acc: 0.7883 - val_loss: 0.9305 - val_acc: 0.7300\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8362 - acc: 0.7859 - val_loss: 0.9257 - val_acc: 0.7440\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8365 - acc: 0.7860 - val_loss: 0.9216 - val_acc: 0.7460\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8349 - acc: 0.7880 - val_loss: 0.9399 - val_acc: 0.7440\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8374 - acc: 0.7861 - val_loss: 0.9212 - val_acc: 0.7400\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8347 - acc: 0.7864 - val_loss: 0.9454 - val_acc: 0.7340\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8364 - acc: 0.7861 - val_loss: 0.9401 - val_acc: 0.7440\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8350 - acc: 0.7876 - val_loss: 0.9181 - val_acc: 0.7480\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8334 - acc: 0.7877 - val_loss: 0.9230 - val_acc: 0.7410\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8354 - acc: 0.7888 - val_loss: 0.9165 - val_acc: 0.7380\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8313 - acc: 0.7892 - val_loss: 0.9318 - val_acc: 0.7450\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8339 - acc: 0.7856 - val_loss: 0.9212 - val_acc: 0.7450\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8335 - acc: 0.7876 - val_loss: 0.9274 - val_acc: 0.7380\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8340 - acc: 0.7879 - val_loss: 0.9456 - val_acc: 0.7340\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8329 - acc: 0.7879 - val_loss: 0.9227 - val_acc: 0.7370\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8334 - acc: 0.7876 - val_loss: 0.9294 - val_acc: 0.7380\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8338 - acc: 0.7857 - val_loss: 0.9414 - val_acc: 0.7430\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8333 - acc: 0.7861 - val_loss: 0.9175 - val_acc: 0.7380\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8311 - acc: 0.7888 - val_loss: 0.9164 - val_acc: 0.7400\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8314 - acc: 0.7871 - val_loss: 0.9236 - val_acc: 0.7390\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8310 - acc: 0.7868 - val_loss: 0.9198 - val_acc: 0.7420\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8314 - acc: 0.7859 - val_loss: 0.9267 - val_acc: 0.7400\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8323 - acc: 0.7880 - val_loss: 0.9227 - val_acc: 0.7410\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8327 - acc: 0.7856 - val_loss: 0.9482 - val_acc: 0.7350\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8312 - acc: 0.7861 - val_loss: 0.9255 - val_acc: 0.7420\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8308 - acc: 0.7891 - val_loss: 0.9285 - val_acc: 0.7420\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8307 - acc: 0.7885 - val_loss: 0.9769 - val_acc: 0.7360\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8341 - acc: 0.7868 - val_loss: 0.9292 - val_acc: 0.7400\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8305 - acc: 0.7904 - val_loss: 0.9529 - val_acc: 0.7350\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8325 - acc: 0.7860 - val_loss: 0.9210 - val_acc: 0.7450\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8300 - acc: 0.7885 - val_loss: 0.9251 - val_acc: 0.7360\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8313 - acc: 0.7867 - val_loss: 0.9225 - val_acc: 0.7370\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8303 - acc: 0.7875 - val_loss: 0.9250 - val_acc: 0.7450\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8320 - acc: 0.7867 - val_loss: 0.9208 - val_acc: 0.7370\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8289 - acc: 0.7884 - val_loss: 0.9200 - val_acc: 0.7400\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8294 - acc: 0.7881 - val_loss: 0.9226 - val_acc: 0.7440\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8289 - acc: 0.7891 - val_loss: 0.9221 - val_acc: 0.7420\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8292 - acc: 0.7891 - val_loss: 0.9190 - val_acc: 0.7390\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8293 - acc: 0.7884 - val_loss: 0.9280 - val_acc: 0.7330\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8278 - acc: 0.7876 - val_loss: 0.9178 - val_acc: 0.7410\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8303 - acc: 0.7872 - val_loss: 0.9187 - val_acc: 0.7450\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8284 - acc: 0.7896 - val_loss: 0.9245 - val_acc: 0.7400\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8289 - acc: 0.7875 - val_loss: 0.9163 - val_acc: 0.7420\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8266 - acc: 0.7891 - val_loss: 0.9398 - val_acc: 0.7400\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8292 - acc: 0.7876 - val_loss: 0.9230 - val_acc: 0.7410\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8263 - acc: 0.7903 - val_loss: 0.9191 - val_acc: 0.7470\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8278 - acc: 0.7875 - val_loss: 0.9194 - val_acc: 0.7450\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8271 - acc: 0.7887 - val_loss: 0.9242 - val_acc: 0.7430\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8272 - acc: 0.7884 - val_loss: 0.9342 - val_acc: 0.7340\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8273 - acc: 0.7893 - val_loss: 0.9249 - val_acc: 0.7460\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8258 - acc: 0.7889 - val_loss: 0.9207 - val_acc: 0.7370\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8250 - acc: 0.7888 - val_loss: 0.9236 - val_acc: 0.7450\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8288 - acc: 0.7881 - val_loss: 0.9222 - val_acc: 0.7400\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8263 - acc: 0.7879 - val_loss: 0.9187 - val_acc: 0.7440\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8262 - acc: 0.7881 - val_loss: 0.9206 - val_acc: 0.7440\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8256 - acc: 0.7891 - val_loss: 0.9111 - val_acc: 0.7420\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8266 - acc: 0.7879 - val_loss: 0.9130 - val_acc: 0.7480\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8249 - acc: 0.7877 - val_loss: 0.9238 - val_acc: 0.7470\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8243 - acc: 0.7893 - val_loss: 0.9201 - val_acc: 0.7350\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8263 - acc: 0.7875 - val_loss: 0.9286 - val_acc: 0.7470\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8297 - acc: 0.7871 - val_loss: 0.9453 - val_acc: 0.7360\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8252 - acc: 0.7887 - val_loss: 0.9205 - val_acc: 0.7430\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8254 - acc: 0.7879 - val_loss: 0.9126 - val_acc: 0.7430\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8249 - acc: 0.7889 - val_loss: 0.9099 - val_acc: 0.7430\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8228 - acc: 0.7896 - val_loss: 0.9146 - val_acc: 0.7430\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8235 - acc: 0.7872 - val_loss: 0.9147 - val_acc: 0.7430\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8227 - acc: 0.7909 - val_loss: 0.9105 - val_acc: 0.7430\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8256 - acc: 0.7865 - val_loss: 0.9289 - val_acc: 0.7330\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8239 - acc: 0.7887 - val_loss: 0.9171 - val_acc: 0.7400\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8235 - acc: 0.7899 - val_loss: 0.9131 - val_acc: 0.7430\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8245 - acc: 0.7893 - val_loss: 0.9242 - val_acc: 0.7450\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8245 - acc: 0.7879 - val_loss: 0.9330 - val_acc: 0.7360\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8225 - acc: 0.7884 - val_loss: 0.9219 - val_acc: 0.7440\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8228 - acc: 0.7907 - val_loss: 0.9222 - val_acc: 0.7450\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8236 - acc: 0.7875 - val_loss: 0.9160 - val_acc: 0.7380\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8220 - acc: 0.7892 - val_loss: 0.9155 - val_acc: 0.7350\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8219 - acc: 0.7909 - val_loss: 0.9183 - val_acc: 0.7440\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8206 - acc: 0.7880 - val_loss: 0.9190 - val_acc: 0.7360\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8219 - acc: 0.7876 - val_loss: 0.9487 - val_acc: 0.7340\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8236 - acc: 0.7915 - val_loss: 0.9267 - val_acc: 0.7450\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8268 - acc: 0.7887 - val_loss: 0.9100 - val_acc: 0.7460\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8208 - acc: 0.7893 - val_loss: 0.9422 - val_acc: 0.7430\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8218 - acc: 0.7899 - val_loss: 0.9295 - val_acc: 0.7450\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8235 - acc: 0.7895 - val_loss: 0.9123 - val_acc: 0.7470\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8194 - acc: 0.7884 - val_loss: 0.9280 - val_acc: 0.7420\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8220 - acc: 0.7883 - val_loss: 0.9217 - val_acc: 0.7370\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8182 - acc: 0.7895 - val_loss: 0.9254 - val_acc: 0.7390\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8201 - acc: 0.7883 - val_loss: 0.9083 - val_acc: 0.7420\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8195 - acc: 0.7913 - val_loss: 0.9151 - val_acc: 0.7390\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8228 - acc: 0.7900 - val_loss: 0.9118 - val_acc: 0.7400\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8212 - acc: 0.7879 - val_loss: 0.9123 - val_acc: 0.7410\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8207 - acc: 0.7893 - val_loss: 0.9374 - val_acc: 0.7460\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8187 - acc: 0.7899 - val_loss: 0.9142 - val_acc: 0.7410\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8195 - acc: 0.7889 - val_loss: 0.9206 - val_acc: 0.7440\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8185 - acc: 0.7888 - val_loss: 0.9198 - val_acc: 0.7410\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8178 - acc: 0.7908 - val_loss: 0.9505 - val_acc: 0.7320\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8201 - acc: 0.7904 - val_loss: 0.9248 - val_acc: 0.7310\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8188 - acc: 0.7892 - val_loss: 0.9143 - val_acc: 0.7410\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8178 - acc: 0.7889 - val_loss: 0.9171 - val_acc: 0.7400\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8184 - acc: 0.7883 - val_loss: 0.9143 - val_acc: 0.7440\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8177 - acc: 0.7889 - val_loss: 0.9220 - val_acc: 0.7440\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8182 - acc: 0.7911 - val_loss: 0.9407 - val_acc: 0.7380\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8185 - acc: 0.7901 - val_loss: 0.9127 - val_acc: 0.7440\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8158 - acc: 0.7919 - val_loss: 0.9116 - val_acc: 0.7380\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8166 - acc: 0.7912 - val_loss: 0.9409 - val_acc: 0.7370\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8202 - acc: 0.7899 - val_loss: 0.9126 - val_acc: 0.7430\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8191 - acc: 0.7900 - val_loss: 0.9173 - val_acc: 0.7370\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8181 - acc: 0.7897 - val_loss: 0.9277 - val_acc: 0.7400\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8157 - acc: 0.7899 - val_loss: 0.9171 - val_acc: 0.7390\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8156 - acc: 0.7900 - val_loss: 0.9179 - val_acc: 0.7440\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8161 - acc: 0.7908 - val_loss: 0.9158 - val_acc: 0.7370\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8183 - acc: 0.7911 - val_loss: 0.9123 - val_acc: 0.7470\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8154 - acc: 0.7903 - val_loss: 0.9477 - val_acc: 0.7310\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8173 - acc: 0.7899 - val_loss: 0.9232 - val_acc: 0.7450\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8157 - acc: 0.7913 - val_loss: 0.9247 - val_acc: 0.7390\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8169 - acc: 0.7883 - val_loss: 0.9263 - val_acc: 0.7430\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8142 - acc: 0.7891 - val_loss: 0.9216 - val_acc: 0.7350\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8167 - acc: 0.7899 - val_loss: 0.9105 - val_acc: 0.7430\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8161 - acc: 0.7903 - val_loss: 0.9187 - val_acc: 0.7400\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8153 - acc: 0.7901 - val_loss: 0.9184 - val_acc: 0.7400\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8147 - acc: 0.7929 - val_loss: 0.9112 - val_acc: 0.7440\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8146 - acc: 0.7920 - val_loss: 0.9494 - val_acc: 0.7390\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8152 - acc: 0.7919 - val_loss: 0.9161 - val_acc: 0.7370\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8155 - acc: 0.7905 - val_loss: 0.9143 - val_acc: 0.7410\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8144 - acc: 0.7907 - val_loss: 0.9133 - val_acc: 0.7380\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8129 - acc: 0.7900 - val_loss: 0.9224 - val_acc: 0.7490\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8134 - acc: 0.7899 - val_loss: 0.9307 - val_acc: 0.7300\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8134 - acc: 0.7904 - val_loss: 0.9230 - val_acc: 0.7440\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8135 - acc: 0.7913 - val_loss: 0.9078 - val_acc: 0.7440\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8146 - acc: 0.7917 - val_loss: 0.9217 - val_acc: 0.7310\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8141 - acc: 0.7897 - val_loss: 0.9152 - val_acc: 0.7340\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8131 - acc: 0.7915 - val_loss: 0.9223 - val_acc: 0.7350\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8131 - acc: 0.7912 - val_loss: 0.9313 - val_acc: 0.7440\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8132 - acc: 0.7911 - val_loss: 0.9142 - val_acc: 0.7350\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8114 - acc: 0.7929 - val_loss: 0.9106 - val_acc: 0.7360\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8105 - acc: 0.7912 - val_loss: 0.9159 - val_acc: 0.7380\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8141 - acc: 0.7913 - val_loss: 0.9097 - val_acc: 0.7420\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8113 - acc: 0.7904 - val_loss: 0.9164 - val_acc: 0.7480\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8138 - acc: 0.7895 - val_loss: 0.9149 - val_acc: 0.7370\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8130 - acc: 0.7907 - val_loss: 0.9243 - val_acc: 0.7360\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8126 - acc: 0.7905 - val_loss: 0.9072 - val_acc: 0.7390\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8094 - acc: 0.7933 - val_loss: 0.9110 - val_acc: 0.7430\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8113 - acc: 0.7911 - val_loss: 0.9398 - val_acc: 0.7350\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8159 - acc: 0.7899 - val_loss: 0.9101 - val_acc: 0.7370\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8096 - acc: 0.7909 - val_loss: 0.9171 - val_acc: 0.7440\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8095 - acc: 0.7928 - val_loss: 0.9215 - val_acc: 0.7420\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8096 - acc: 0.7929 - val_loss: 0.9145 - val_acc: 0.7350\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8121 - acc: 0.7913 - val_loss: 0.9644 - val_acc: 0.7340\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8098 - acc: 0.7917 - val_loss: 0.9100 - val_acc: 0.7450\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8104 - acc: 0.7925 - val_loss: 0.9149 - val_acc: 0.7440\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8097 - acc: 0.7923 - val_loss: 0.9194 - val_acc: 0.7410\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8098 - acc: 0.7920 - val_loss: 0.9205 - val_acc: 0.7430\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8106 - acc: 0.7913 - val_loss: 0.9123 - val_acc: 0.7390\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8103 - acc: 0.7920 - val_loss: 0.9163 - val_acc: 0.7430\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8088 - acc: 0.7932 - val_loss: 0.9324 - val_acc: 0.7350\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8114 - acc: 0.7915 - val_loss: 0.9472 - val_acc: 0.7350\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8093 - acc: 0.7937 - val_loss: 0.9115 - val_acc: 0.7370\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8088 - acc: 0.7957 - val_loss: 0.9195 - val_acc: 0.7420\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8105 - acc: 0.7909 - val_loss: 0.9210 - val_acc: 0.7400\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8086 - acc: 0.7923 - val_loss: 0.9133 - val_acc: 0.7470\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8099 - acc: 0.7924 - val_loss: 0.9503 - val_acc: 0.7380\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8094 - acc: 0.7913 - val_loss: 0.9145 - val_acc: 0.7420\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8096 - acc: 0.7900 - val_loss: 0.9120 - val_acc: 0.7430\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8070 - acc: 0.7928 - val_loss: 0.9177 - val_acc: 0.7350\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8088 - acc: 0.7923 - val_loss: 0.9092 - val_acc: 0.7400\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8078 - acc: 0.7921 - val_loss: 0.9102 - val_acc: 0.7380\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8093 - acc: 0.7931 - val_loss: 0.9070 - val_acc: 0.7420\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8069 - acc: 0.7911 - val_loss: 0.9044 - val_acc: 0.7400\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8062 - acc: 0.7956 - val_loss: 0.9114 - val_acc: 0.7400\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8057 - acc: 0.7941 - val_loss: 0.9109 - val_acc: 0.7440\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8084 - acc: 0.7944 - val_loss: 0.9108 - val_acc: 0.7340\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8057 - acc: 0.7920 - val_loss: 0.9144 - val_acc: 0.7390\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8055 - acc: 0.7935 - val_loss: 0.9185 - val_acc: 0.7350\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8060 - acc: 0.7941 - val_loss: 0.9089 - val_acc: 0.7470\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8078 - acc: 0.7921 - val_loss: 0.9260 - val_acc: 0.7290\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8058 - acc: 0.7921 - val_loss: 0.9089 - val_acc: 0.7420\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8074 - acc: 0.7940 - val_loss: 0.9214 - val_acc: 0.7390\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8074 - acc: 0.7945 - val_loss: 0.9189 - val_acc: 0.7450\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8083 - acc: 0.7947 - val_loss: 0.9502 - val_acc: 0.7380\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8060 - acc: 0.7923 - val_loss: 0.9206 - val_acc: 0.7470\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8053 - acc: 0.7945 - val_loss: 0.9160 - val_acc: 0.7430\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8082 - acc: 0.7935 - val_loss: 0.9197 - val_acc: 0.7390\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8068 - acc: 0.7925 - val_loss: 0.9215 - val_acc: 0.7310\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8039 - acc: 0.7940 - val_loss: 0.9158 - val_acc: 0.7410\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8086 - acc: 0.7920 - val_loss: 0.9267 - val_acc: 0.7480\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8039 - acc: 0.7949 - val_loss: 0.9406 - val_acc: 0.7460\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8042 - acc: 0.7937 - val_loss: 0.9130 - val_acc: 0.7430\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8042 - acc: 0.7936 - val_loss: 0.9158 - val_acc: 0.7460\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8069 - acc: 0.7959 - val_loss: 0.9153 - val_acc: 0.7460\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8048 - acc: 0.7940 - val_loss: 0.9155 - val_acc: 0.7470\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8031 - acc: 0.7933 - val_loss: 0.9118 - val_acc: 0.7450\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8045 - acc: 0.7956 - val_loss: 0.9387 - val_acc: 0.7450\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8054 - acc: 0.7939 - val_loss: 0.9221 - val_acc: 0.7430\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8030 - acc: 0.7961 - val_loss: 0.9261 - val_acc: 0.7410\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8041 - acc: 0.7924 - val_loss: 0.9138 - val_acc: 0.7470\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8047 - acc: 0.7935 - val_loss: 0.9345 - val_acc: 0.7380\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8054 - acc: 0.7932 - val_loss: 0.9029 - val_acc: 0.7470\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8037 - acc: 0.7933 - val_loss: 0.9267 - val_acc: 0.7500\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8017 - acc: 0.7957 - val_loss: 0.9270 - val_acc: 0.7370\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8080 - acc: 0.7925 - val_loss: 0.9181 - val_acc: 0.7370\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8069 - acc: 0.7959 - val_loss: 0.9129 - val_acc: 0.7410\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8015 - acc: 0.7952 - val_loss: 0.9377 - val_acc: 0.7430\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8027 - acc: 0.7939 - val_loss: 0.9175 - val_acc: 0.7420\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8064 - acc: 0.7949 - val_loss: 0.9216 - val_acc: 0.7440\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8012 - acc: 0.7956 - val_loss: 0.9240 - val_acc: 0.7480\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8057 - acc: 0.7931 - val_loss: 0.9455 - val_acc: 0.7350\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8042 - acc: 0.7968 - val_loss: 1.0000 - val_acc: 0.7150\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8078 - acc: 0.7947 - val_loss: 0.9092 - val_acc: 0.7440\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8019 - acc: 0.7940 - val_loss: 0.9324 - val_acc: 0.7420\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8037 - acc: 0.7936 - val_loss: 0.9142 - val_acc: 0.7390\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8044 - acc: 0.7947 - val_loss: 0.9161 - val_acc: 0.7460\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8031 - acc: 0.7965 - val_loss: 0.9129 - val_acc: 0.7400\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8003 - acc: 0.7940 - val_loss: 0.9080 - val_acc: 0.7450\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8028 - acc: 0.7943 - val_loss: 0.9218 - val_acc: 0.7390\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8021 - acc: 0.7943 - val_loss: 0.9121 - val_acc: 0.7320\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8017 - acc: 0.7933 - val_loss: 0.9269 - val_acc: 0.7360\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7991 - acc: 0.7975 - val_loss: 0.9601 - val_acc: 0.7300\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8010 - acc: 0.7948 - val_loss: 0.9317 - val_acc: 0.7420\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8015 - acc: 0.7947 - val_loss: 0.9218 - val_acc: 0.7300\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8026 - acc: 0.7949 - val_loss: 0.9269 - val_acc: 0.7390\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7997 - acc: 0.7964 - val_loss: 0.9081 - val_acc: 0.7450\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7990 - acc: 0.7968 - val_loss: 0.9029 - val_acc: 0.7450\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8021 - acc: 0.7957 - val_loss: 0.9329 - val_acc: 0.7360\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7996 - acc: 0.7987 - val_loss: 0.9322 - val_acc: 0.7340\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8001 - acc: 0.7983 - val_loss: 0.9071 - val_acc: 0.7440\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7966 - acc: 0.7989 - val_loss: 0.9284 - val_acc: 0.7420\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8013 - acc: 0.7949 - val_loss: 0.9610 - val_acc: 0.7260\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8019 - acc: 0.7932 - val_loss: 0.9107 - val_acc: 0.7430\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7978 - acc: 0.8001 - val_loss: 0.9896 - val_acc: 0.7280\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8024 - acc: 0.7949 - val_loss: 0.9095 - val_acc: 0.7510\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8006 - acc: 0.7979 - val_loss: 0.9190 - val_acc: 0.7410\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8013 - acc: 0.7985 - val_loss: 0.9085 - val_acc: 0.7500\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7980 - acc: 0.7984 - val_loss: 1.0335 - val_acc: 0.7200\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8077 - acc: 0.7928 - val_loss: 0.9326 - val_acc: 0.7410\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8063 - acc: 0.7952 - val_loss: 0.9070 - val_acc: 0.7430\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7983 - acc: 0.7967 - val_loss: 0.9065 - val_acc: 0.7460\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7959 - acc: 0.7995 - val_loss: 0.9158 - val_acc: 0.7340\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7980 - acc: 0.7961 - val_loss: 0.9208 - val_acc: 0.7330\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7984 - acc: 0.7993 - val_loss: 0.9264 - val_acc: 0.7440\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7980 - acc: 0.7984 - val_loss: 0.9561 - val_acc: 0.7380\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8007 - acc: 0.7957 - val_loss: 0.9008 - val_acc: 0.7440\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7969 - acc: 0.7988 - val_loss: 0.9057 - val_acc: 0.7440\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7966 - acc: 0.7993 - val_loss: 0.9087 - val_acc: 0.7450\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7971 - acc: 0.7989 - val_loss: 0.9080 - val_acc: 0.7450\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7951 - acc: 0.7988 - val_loss: 0.9227 - val_acc: 0.7420\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7987 - acc: 0.7977 - val_loss: 0.9295 - val_acc: 0.7350\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7944 - acc: 0.7971 - val_loss: 0.9630 - val_acc: 0.7330\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7980 - acc: 0.7977 - val_loss: 0.9040 - val_acc: 0.7540\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7979 - acc: 0.8003 - val_loss: 0.9328 - val_acc: 0.7420\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7965 - acc: 0.7999 - val_loss: 0.9077 - val_acc: 0.7410\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7944 - acc: 0.7991 - val_loss: 0.9459 - val_acc: 0.7430\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7980 - acc: 0.8017 - val_loss: 0.9134 - val_acc: 0.7460\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7975 - acc: 0.7971 - val_loss: 0.9197 - val_acc: 0.7520\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7951 - acc: 0.8025 - val_loss: 0.9059 - val_acc: 0.7390\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7971 - acc: 0.7992 - val_loss: 0.9044 - val_acc: 0.7420\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7982 - acc: 0.7985 - val_loss: 0.9294 - val_acc: 0.7330\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7977 - acc: 0.8003 - val_loss: 0.9102 - val_acc: 0.7500\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7953 - acc: 0.8012 - val_loss: 0.9082 - val_acc: 0.7390\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7928 - acc: 0.7995 - val_loss: 0.9372 - val_acc: 0.7470\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7943 - acc: 0.8008 - val_loss: 0.9152 - val_acc: 0.7360\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7964 - acc: 0.7988 - val_loss: 0.9109 - val_acc: 0.7360\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step\n",
      "1500/1500 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7904860157648722, 0.8013333333015442]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.903479238986969, 0.7566666666666667]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 15:28:58.006814 4761814464 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 2.0016 - acc: 0.1449 - val_loss: 1.9429 - val_acc: 0.1590\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9620 - acc: 0.1581 - val_loss: 1.9272 - val_acc: 0.1820\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9333 - acc: 0.1753 - val_loss: 1.9159 - val_acc: 0.2090\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9270 - acc: 0.1864 - val_loss: 1.9061 - val_acc: 0.2160\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9177 - acc: 0.1901 - val_loss: 1.8967 - val_acc: 0.2270\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9108 - acc: 0.1985 - val_loss: 1.8872 - val_acc: 0.2470\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8984 - acc: 0.2072 - val_loss: 1.8751 - val_acc: 0.2490\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8833 - acc: 0.2137 - val_loss: 1.8614 - val_acc: 0.2630\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8814 - acc: 0.2133 - val_loss: 1.8486 - val_acc: 0.2810\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8709 - acc: 0.2355 - val_loss: 1.8330 - val_acc: 0.2900\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8556 - acc: 0.2393 - val_loss: 1.8170 - val_acc: 0.3040\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8402 - acc: 0.2473 - val_loss: 1.7973 - val_acc: 0.3120\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8329 - acc: 0.2495 - val_loss: 1.7784 - val_acc: 0.3390\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8161 - acc: 0.2596 - val_loss: 1.7584 - val_acc: 0.3510\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7968 - acc: 0.2767 - val_loss: 1.7352 - val_acc: 0.3630\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7887 - acc: 0.2777 - val_loss: 1.7136 - val_acc: 0.3810\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7664 - acc: 0.2959 - val_loss: 1.6911 - val_acc: 0.3930\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.7511 - acc: 0.2985 - val_loss: 1.6669 - val_acc: 0.4230\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.7410 - acc: 0.3069 - val_loss: 1.6429 - val_acc: 0.4400\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.7230 - acc: 0.3151 - val_loss: 1.6191 - val_acc: 0.4600\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.7088 - acc: 0.3252 - val_loss: 1.5968 - val_acc: 0.4790\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.6870 - acc: 0.3379 - val_loss: 1.5707 - val_acc: 0.4940\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6680 - acc: 0.3427 - val_loss: 1.5468 - val_acc: 0.5080\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6537 - acc: 0.3561 - val_loss: 1.5213 - val_acc: 0.5110\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.6322 - acc: 0.3707 - val_loss: 1.4994 - val_acc: 0.5320\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.6194 - acc: 0.3708 - val_loss: 1.4744 - val_acc: 0.5430\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6094 - acc: 0.3799 - val_loss: 1.4518 - val_acc: 0.5540\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5811 - acc: 0.3929 - val_loss: 1.4260 - val_acc: 0.5660\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.5635 - acc: 0.4025 - val_loss: 1.4041 - val_acc: 0.5760\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.5409 - acc: 0.4117 - val_loss: 1.3788 - val_acc: 0.5780\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5364 - acc: 0.4073 - val_loss: 1.3608 - val_acc: 0.5970\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5139 - acc: 0.4281 - val_loss: 1.3383 - val_acc: 0.6080\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.4946 - acc: 0.4304 - val_loss: 1.3159 - val_acc: 0.6150\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4849 - acc: 0.4360 - val_loss: 1.2947 - val_acc: 0.6280\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4660 - acc: 0.4453 - val_loss: 1.2722 - val_acc: 0.6330\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4498 - acc: 0.4528 - val_loss: 1.2531 - val_acc: 0.6460\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4329 - acc: 0.4612 - val_loss: 1.2291 - val_acc: 0.6560\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4169 - acc: 0.4684 - val_loss: 1.2132 - val_acc: 0.6650\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4006 - acc: 0.4852 - val_loss: 1.1890 - val_acc: 0.6670\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.3859 - acc: 0.4743 - val_loss: 1.1738 - val_acc: 0.6690\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3795 - acc: 0.4781 - val_loss: 1.1565 - val_acc: 0.6730\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3578 - acc: 0.4923 - val_loss: 1.1373 - val_acc: 0.6780\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3405 - acc: 0.4969 - val_loss: 1.1190 - val_acc: 0.6770\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3266 - acc: 0.5023 - val_loss: 1.1029 - val_acc: 0.6840\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3076 - acc: 0.5095 - val_loss: 1.0852 - val_acc: 0.6830\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2939 - acc: 0.5189 - val_loss: 1.0695 - val_acc: 0.6910\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2842 - acc: 0.5213 - val_loss: 1.0540 - val_acc: 0.6920\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2748 - acc: 0.5265 - val_loss: 1.0375 - val_acc: 0.6960\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.2631 - acc: 0.5243 - val_loss: 1.0280 - val_acc: 0.6930\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2522 - acc: 0.5451 - val_loss: 1.0166 - val_acc: 0.6910\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2441 - acc: 0.5356 - val_loss: 1.0052 - val_acc: 0.7010\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2248 - acc: 0.5451 - val_loss: 0.9919 - val_acc: 0.6990\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.2187 - acc: 0.5509 - val_loss: 0.9826 - val_acc: 0.7020\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.2156 - acc: 0.5487 - val_loss: 0.9729 - val_acc: 0.7010\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2094 - acc: 0.5525 - val_loss: 0.9645 - val_acc: 0.7030\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1874 - acc: 0.5589 - val_loss: 0.9529 - val_acc: 0.7070\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.1746 - acc: 0.5611 - val_loss: 0.9401 - val_acc: 0.7000\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.1823 - acc: 0.5643 - val_loss: 0.9366 - val_acc: 0.7030\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1703 - acc: 0.5661 - val_loss: 0.9281 - val_acc: 0.7040\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.1621 - acc: 0.5740 - val_loss: 0.9172 - val_acc: 0.7040\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1463 - acc: 0.5769 - val_loss: 0.9120 - val_acc: 0.7020\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1396 - acc: 0.5705 - val_loss: 0.8999 - val_acc: 0.7050\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1310 - acc: 0.5849 - val_loss: 0.8914 - val_acc: 0.7060\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1217 - acc: 0.5840 - val_loss: 0.8831 - val_acc: 0.7100\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1108 - acc: 0.5847 - val_loss: 0.8756 - val_acc: 0.7090\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1113 - acc: 0.5980 - val_loss: 0.8723 - val_acc: 0.7100\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1003 - acc: 0.5883 - val_loss: 0.8657 - val_acc: 0.7100\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1000 - acc: 0.5887 - val_loss: 0.8602 - val_acc: 0.7090\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.0894 - acc: 0.5943 - val_loss: 0.8522 - val_acc: 0.7100\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0872 - acc: 0.5948 - val_loss: 0.8468 - val_acc: 0.7090\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0677 - acc: 0.6016 - val_loss: 0.8405 - val_acc: 0.7090\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0661 - acc: 0.5997 - val_loss: 0.8330 - val_acc: 0.7160\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0568 - acc: 0.6108 - val_loss: 0.8268 - val_acc: 0.7170\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0537 - acc: 0.6087 - val_loss: 0.8210 - val_acc: 0.7140\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0446 - acc: 0.6065 - val_loss: 0.8186 - val_acc: 0.7220\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0394 - acc: 0.6152 - val_loss: 0.8135 - val_acc: 0.7200\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0280 - acc: 0.6132 - val_loss: 0.8091 - val_acc: 0.7190\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0326 - acc: 0.6144 - val_loss: 0.8052 - val_acc: 0.7260\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0218 - acc: 0.6153 - val_loss: 0.7992 - val_acc: 0.7230\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0209 - acc: 0.6268 - val_loss: 0.7949 - val_acc: 0.7260\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0242 - acc: 0.6259 - val_loss: 0.7920 - val_acc: 0.7240\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0070 - acc: 0.6257 - val_loss: 0.7873 - val_acc: 0.7230\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9919 - acc: 0.6361 - val_loss: 0.7805 - val_acc: 0.7270\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0049 - acc: 0.6336 - val_loss: 0.7761 - val_acc: 0.7250\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9974 - acc: 0.6307 - val_loss: 0.7728 - val_acc: 0.7260\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9878 - acc: 0.6332 - val_loss: 0.7678 - val_acc: 0.7290\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9894 - acc: 0.6303 - val_loss: 0.7657 - val_acc: 0.7290\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9933 - acc: 0.6293 - val_loss: 0.7643 - val_acc: 0.7280\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9824 - acc: 0.6395 - val_loss: 0.7606 - val_acc: 0.7310\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9681 - acc: 0.6443 - val_loss: 0.7581 - val_acc: 0.7310\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9681 - acc: 0.6431 - val_loss: 0.7545 - val_acc: 0.7310\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9736 - acc: 0.6391 - val_loss: 0.7512 - val_acc: 0.7340\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9640 - acc: 0.6428 - val_loss: 0.7462 - val_acc: 0.7320\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9621 - acc: 0.6400 - val_loss: 0.7443 - val_acc: 0.7330\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9562 - acc: 0.6451 - val_loss: 0.7404 - val_acc: 0.7370\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9527 - acc: 0.6423 - val_loss: 0.7399 - val_acc: 0.7390\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9369 - acc: 0.6551 - val_loss: 0.7324 - val_acc: 0.7340\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9233 - acc: 0.6515 - val_loss: 0.7292 - val_acc: 0.7370\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9351 - acc: 0.6536 - val_loss: 0.7271 - val_acc: 0.7340\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9358 - acc: 0.6495 - val_loss: 0.7251 - val_acc: 0.7380\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9338 - acc: 0.6591 - val_loss: 0.7238 - val_acc: 0.7350\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9238 - acc: 0.6576 - val_loss: 0.7217 - val_acc: 0.7380\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9256 - acc: 0.6551 - val_loss: 0.7169 - val_acc: 0.7360\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9291 - acc: 0.6540 - val_loss: 0.7162 - val_acc: 0.7400\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9125 - acc: 0.6633 - val_loss: 0.7117 - val_acc: 0.7390\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9236 - acc: 0.6520 - val_loss: 0.7104 - val_acc: 0.7410\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9139 - acc: 0.6565 - val_loss: 0.7083 - val_acc: 0.7400\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9043 - acc: 0.6637 - val_loss: 0.7068 - val_acc: 0.7410\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8874 - acc: 0.6725 - val_loss: 0.7019 - val_acc: 0.7400\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9059 - acc: 0.6557 - val_loss: 0.7024 - val_acc: 0.7420\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8870 - acc: 0.6740 - val_loss: 0.7005 - val_acc: 0.7420\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8909 - acc: 0.6731 - val_loss: 0.6961 - val_acc: 0.7420\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9010 - acc: 0.6643 - val_loss: 0.6984 - val_acc: 0.7440\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8880 - acc: 0.6695 - val_loss: 0.6941 - val_acc: 0.7430\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8793 - acc: 0.6736 - val_loss: 0.6924 - val_acc: 0.7450\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8811 - acc: 0.6711 - val_loss: 0.6918 - val_acc: 0.7430\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8796 - acc: 0.6713 - val_loss: 0.6874 - val_acc: 0.7430\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8803 - acc: 0.6751 - val_loss: 0.6864 - val_acc: 0.7410\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8716 - acc: 0.6753 - val_loss: 0.6825 - val_acc: 0.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8682 - acc: 0.6800 - val_loss: 0.6824 - val_acc: 0.7440\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8598 - acc: 0.6775 - val_loss: 0.6809 - val_acc: 0.7420\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8646 - acc: 0.6776 - val_loss: 0.6777 - val_acc: 0.7430\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8545 - acc: 0.6880 - val_loss: 0.6759 - val_acc: 0.7410\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8691 - acc: 0.6811 - val_loss: 0.6763 - val_acc: 0.7410\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8621 - acc: 0.6835 - val_loss: 0.6732 - val_acc: 0.7430\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8567 - acc: 0.6843 - val_loss: 0.6709 - val_acc: 0.7440\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8458 - acc: 0.6883 - val_loss: 0.6690 - val_acc: 0.7430\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8544 - acc: 0.6815 - val_loss: 0.6694 - val_acc: 0.7430\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8358 - acc: 0.6901 - val_loss: 0.6678 - val_acc: 0.7450\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8363 - acc: 0.6912 - val_loss: 0.6679 - val_acc: 0.7450\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8494 - acc: 0.6895 - val_loss: 0.6661 - val_acc: 0.7460\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8398 - acc: 0.6875 - val_loss: 0.6623 - val_acc: 0.7410\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8345 - acc: 0.6921 - val_loss: 0.6619 - val_acc: 0.7420\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8279 - acc: 0.6955 - val_loss: 0.6619 - val_acc: 0.7420\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8400 - acc: 0.6896 - val_loss: 0.6598 - val_acc: 0.7420\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8168 - acc: 0.7035 - val_loss: 0.6567 - val_acc: 0.7430\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8253 - acc: 0.7016 - val_loss: 0.6570 - val_acc: 0.7420\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8349 - acc: 0.6959 - val_loss: 0.6573 - val_acc: 0.7420\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8221 - acc: 0.6897 - val_loss: 0.6533 - val_acc: 0.7430\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8332 - acc: 0.6901 - val_loss: 0.6529 - val_acc: 0.7420\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8157 - acc: 0.6965 - val_loss: 0.6512 - val_acc: 0.7470\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8175 - acc: 0.7033 - val_loss: 0.6487 - val_acc: 0.7450\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8218 - acc: 0.6983 - val_loss: 0.6494 - val_acc: 0.7450\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8166 - acc: 0.6981 - val_loss: 0.6462 - val_acc: 0.7430\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7986 - acc: 0.7045 - val_loss: 0.6449 - val_acc: 0.7450\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7952 - acc: 0.7025 - val_loss: 0.6446 - val_acc: 0.7430\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7991 - acc: 0.7061 - val_loss: 0.6434 - val_acc: 0.7470\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8066 - acc: 0.6949 - val_loss: 0.6427 - val_acc: 0.7440\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8002 - acc: 0.699 - 1s 69us/step - loss: 0.8029 - acc: 0.6985 - val_loss: 0.6425 - val_acc: 0.7460\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8010 - acc: 0.7037 - val_loss: 0.6422 - val_acc: 0.7470\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8015 - acc: 0.6984 - val_loss: 0.6405 - val_acc: 0.7440\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7932 - acc: 0.7084 - val_loss: 0.6396 - val_acc: 0.7440\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7942 - acc: 0.7032 - val_loss: 0.6393 - val_acc: 0.7430\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8018 - acc: 0.7027 - val_loss: 0.6381 - val_acc: 0.7430\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7850 - acc: 0.7092 - val_loss: 0.6358 - val_acc: 0.7450\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7799 - acc: 0.7113 - val_loss: 0.6347 - val_acc: 0.7450\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7783 - acc: 0.7105 - val_loss: 0.6349 - val_acc: 0.7460\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7636 - acc: 0.7172 - val_loss: 0.6309 - val_acc: 0.7440\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7752 - acc: 0.7028 - val_loss: 0.6309 - val_acc: 0.7460\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7682 - acc: 0.7147 - val_loss: 0.6287 - val_acc: 0.7420\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7706 - acc: 0.7159 - val_loss: 0.6298 - val_acc: 0.7460\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7747 - acc: 0.7165 - val_loss: 0.6279 - val_acc: 0.7480\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7813 - acc: 0.7120 - val_loss: 0.6275 - val_acc: 0.7450\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7820 - acc: 0.7128 - val_loss: 0.6276 - val_acc: 0.7450\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7726 - acc: 0.7141 - val_loss: 0.6251 - val_acc: 0.7490\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7685 - acc: 0.7089 - val_loss: 0.6257 - val_acc: 0.7490\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7650 - acc: 0.7123 - val_loss: 0.6230 - val_acc: 0.7490\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7645 - acc: 0.7149 - val_loss: 0.6254 - val_acc: 0.7470\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.7702 - acc: 0.7115 - val_loss: 0.6222 - val_acc: 0.7500\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7624 - acc: 0.7189 - val_loss: 0.6242 - val_acc: 0.7510\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7591 - acc: 0.7172 - val_loss: 0.6221 - val_acc: 0.7510\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7560 - acc: 0.7167 - val_loss: 0.6207 - val_acc: 0.7480\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7644 - acc: 0.7139 - val_loss: 0.6226 - val_acc: 0.7490\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7416 - acc: 0.7244 - val_loss: 0.6195 - val_acc: 0.7470\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7510 - acc: 0.7161 - val_loss: 0.6192 - val_acc: 0.7540\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7537 - acc: 0.7196 - val_loss: 0.6187 - val_acc: 0.7480\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7492 - acc: 0.7193 - val_loss: 0.6165 - val_acc: 0.7490\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7455 - acc: 0.7204 - val_loss: 0.6173 - val_acc: 0.7500\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7517 - acc: 0.7179 - val_loss: 0.6160 - val_acc: 0.7520\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7469 - acc: 0.7228 - val_loss: 0.6137 - val_acc: 0.7540\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7361 - acc: 0.7269 - val_loss: 0.6128 - val_acc: 0.7550\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7440 - acc: 0.7277 - val_loss: 0.6133 - val_acc: 0.7560\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7437 - acc: 0.7152 - val_loss: 0.6133 - val_acc: 0.7560\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7366 - acc: 0.7307 - val_loss: 0.6132 - val_acc: 0.7580\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.7296 - acc: 0.7285 - val_loss: 0.6110 - val_acc: 0.7580\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7278 - acc: 0.7340 - val_loss: 0.6092 - val_acc: 0.7620\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7403 - acc: 0.7227 - val_loss: 0.6100 - val_acc: 0.7570\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7279 - acc: 0.7243 - val_loss: 0.6076 - val_acc: 0.7650\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.7259 - acc: 0.7336 - val_loss: 0.6079 - val_acc: 0.7620\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7253 - acc: 0.7259 - val_loss: 0.6072 - val_acc: 0.7590\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7157 - acc: 0.7292 - val_loss: 0.6073 - val_acc: 0.7650\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7203 - acc: 0.7309 - val_loss: 0.6058 - val_acc: 0.7620\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7200 - acc: 0.7308 - val_loss: 0.6052 - val_acc: 0.7690\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.7191 - acc: 0.7317 - val_loss: 0.6055 - val_acc: 0.7670\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7140 - acc: 0.7325 - val_loss: 0.6040 - val_acc: 0.7650\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7228 - acc: 0.7307 - val_loss: 0.6055 - val_acc: 0.7660\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7164 - acc: 0.7356 - val_loss: 0.6045 - val_acc: 0.7640\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7270 - acc: 0.7308 - val_loss: 0.6070 - val_acc: 0.7670\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7208 - acc: 0.7325 - val_loss: 0.6040 - val_acc: 0.7660\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7292 - acc: 0.7233 - val_loss: 0.6032 - val_acc: 0.7660\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step\n",
      "1500/1500 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44293760016759237, 0.8466666666984558]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5944667511781057, 0.7713333331743876]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 1.9363 - acc: 0.1832 - val_loss: 1.9011 - val_acc: 0.2323\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.8565 - acc: 0.2769 - val_loss: 1.8025 - val_acc: 0.3393\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.7242 - acc: 0.3686 - val_loss: 1.6381 - val_acc: 0.4030\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 1.5369 - acc: 0.4359 - val_loss: 1.4382 - val_acc: 0.4867\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 1.3406 - acc: 0.5268 - val_loss: 1.2559 - val_acc: 0.5743\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 1.1778 - acc: 0.6045 - val_loss: 1.1132 - val_acc: 0.6293\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 1.0514 - acc: 0.6479 - val_loss: 1.0043 - val_acc: 0.6653\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.9533 - acc: 0.6777 - val_loss: 0.9228 - val_acc: 0.6877\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.8777 - acc: 0.6982 - val_loss: 0.8593 - val_acc: 0.7013\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.8191 - acc: 0.7141 - val_loss: 0.8090 - val_acc: 0.7193\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.7731 - acc: 0.7265 - val_loss: 0.7702 - val_acc: 0.7290\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.7365 - acc: 0.7372 - val_loss: 0.7402 - val_acc: 0.7370\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.7067 - acc: 0.7465 - val_loss: 0.7148 - val_acc: 0.7383\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.6820 - acc: 0.7547 - val_loss: 0.6932 - val_acc: 0.7490\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.6609 - acc: 0.7613 - val_loss: 0.6780 - val_acc: 0.7530\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6432 - acc: 0.7658 - val_loss: 0.6622 - val_acc: 0.7570\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.6275 - acc: 0.7718 - val_loss: 0.6493 - val_acc: 0.7603\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.6137 - acc: 0.7764 - val_loss: 0.6390 - val_acc: 0.7623\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.6012 - acc: 0.7794 - val_loss: 0.6279 - val_acc: 0.7690\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5902 - acc: 0.7839 - val_loss: 0.6199 - val_acc: 0.7703\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5798 - acc: 0.7865 - val_loss: 0.6117 - val_acc: 0.7707\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5707 - acc: 0.7899 - val_loss: 0.6057 - val_acc: 0.7767\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.5620 - acc: 0.7939 - val_loss: 0.5984 - val_acc: 0.7770\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.5539 - acc: 0.7971 - val_loss: 0.5929 - val_acc: 0.7773\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.5463 - acc: 0.8007 - val_loss: 0.5883 - val_acc: 0.7807\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.5393 - acc: 0.8020 - val_loss: 0.5839 - val_acc: 0.7793\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.5328 - acc: 0.8053 - val_loss: 0.5785 - val_acc: 0.7873\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.5266 - acc: 0.8077 - val_loss: 0.5749 - val_acc: 0.7897\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5208 - acc: 0.8102 - val_loss: 0.5719 - val_acc: 0.7883\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5151 - acc: 0.8126 - val_loss: 0.5670 - val_acc: 0.7937\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5095 - acc: 0.8140 - val_loss: 0.5656 - val_acc: 0.7943\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.5044 - acc: 0.8155 - val_loss: 0.5608 - val_acc: 0.7983\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4996 - acc: 0.8188 - val_loss: 0.5578 - val_acc: 0.7993\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4948 - acc: 0.8207 - val_loss: 0.5547 - val_acc: 0.7993\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4904 - acc: 0.8228 - val_loss: 0.5541 - val_acc: 0.8000\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4861 - acc: 0.8249 - val_loss: 0.5504 - val_acc: 0.8023\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4819 - acc: 0.8258 - val_loss: 0.5481 - val_acc: 0.8033\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4779 - acc: 0.8282 - val_loss: 0.5468 - val_acc: 0.8040\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.4737 - acc: 0.8285 - val_loss: 0.5467 - val_acc: 0.8043\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.4701 - acc: 0.8304 - val_loss: 0.5437 - val_acc: 0.8073\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.4666 - acc: 0.8317 - val_loss: 0.5410 - val_acc: 0.8077\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4628 - acc: 0.8333 - val_loss: 0.5405 - val_acc: 0.8063\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4592 - acc: 0.8349 - val_loss: 0.5393 - val_acc: 0.8073\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4562 - acc: 0.8360 - val_loss: 0.5386 - val_acc: 0.8067\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4529 - acc: 0.8372 - val_loss: 0.5372 - val_acc: 0.8080\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4495 - acc: 0.8386 - val_loss: 0.5356 - val_acc: 0.8080\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4467 - acc: 0.8393 - val_loss: 0.5341 - val_acc: 0.8073\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4438 - acc: 0.8409 - val_loss: 0.5353 - val_acc: 0.8073\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4406 - acc: 0.8413 - val_loss: 0.5330 - val_acc: 0.8087\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4377 - acc: 0.8426 - val_loss: 0.5328 - val_acc: 0.8063\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4350 - acc: 0.8445 - val_loss: 0.5330 - val_acc: 0.8100\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4323 - acc: 0.8452 - val_loss: 0.5329 - val_acc: 0.8110\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4299 - acc: 0.8469 - val_loss: 0.5305 - val_acc: 0.8103\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4270 - acc: 0.8469 - val_loss: 0.5300 - val_acc: 0.8113\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4242 - acc: 0.8487 - val_loss: 0.5306 - val_acc: 0.8100\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.4220 - acc: 0.8491 - val_loss: 0.5305 - val_acc: 0.8143\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4195 - acc: 0.8510 - val_loss: 0.5311 - val_acc: 0.8123\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4173 - acc: 0.8513 - val_loss: 0.5308 - val_acc: 0.8133\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4146 - acc: 0.8530 - val_loss: 0.5294 - val_acc: 0.8140\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.4122 - acc: 0.8540 - val_loss: 0.5263 - val_acc: 0.8130\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4103 - acc: 0.8552 - val_loss: 0.5272 - val_acc: 0.8130\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4079 - acc: 0.8555 - val_loss: 0.5284 - val_acc: 0.8133\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4057 - acc: 0.8563 - val_loss: 0.5274 - val_acc: 0.8140\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4036 - acc: 0.8571 - val_loss: 0.5304 - val_acc: 0.8117\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4017 - acc: 0.8581 - val_loss: 0.5284 - val_acc: 0.8150\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3993 - acc: 0.8595 - val_loss: 0.5294 - val_acc: 0.8160\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3972 - acc: 0.8600 - val_loss: 0.5315 - val_acc: 0.8117\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 67us/step - loss: 0.3955 - acc: 0.8598 - val_loss: 0.5291 - val_acc: 0.8147\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3937 - acc: 0.8615 - val_loss: 0.5281 - val_acc: 0.8153\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3916 - acc: 0.8623 - val_loss: 0.5311 - val_acc: 0.8153\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.3895 - acc: 0.8626 - val_loss: 0.5289 - val_acc: 0.8147\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3876 - acc: 0.8628 - val_loss: 0.5288 - val_acc: 0.8147\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3858 - acc: 0.8648 - val_loss: 0.5294 - val_acc: 0.8163\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3840 - acc: 0.8651 - val_loss: 0.5303 - val_acc: 0.8140\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3823 - acc: 0.8645 - val_loss: 0.5310 - val_acc: 0.8160\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3807 - acc: 0.8661 - val_loss: 0.5343 - val_acc: 0.8160\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 67us/step - loss: 0.3785 - acc: 0.8663 - val_loss: 0.5340 - val_acc: 0.8127\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 73us/step - loss: 0.3770 - acc: 0.8674 - val_loss: 0.5333 - val_acc: 0.8137\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 70us/step - loss: 0.3752 - acc: 0.8681 - val_loss: 0.5319 - val_acc: 0.8157\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3736 - acc: 0.8696 - val_loss: 0.5330 - val_acc: 0.8160\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3721 - acc: 0.8694 - val_loss: 0.5357 - val_acc: 0.8150\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3702 - acc: 0.8699 - val_loss: 0.5351 - val_acc: 0.8173\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3685 - acc: 0.8711 - val_loss: 0.5347 - val_acc: 0.8160\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3673 - acc: 0.8702 - val_loss: 0.5341 - val_acc: 0.8150\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3654 - acc: 0.8720 - val_loss: 0.5346 - val_acc: 0.8160\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3639 - acc: 0.8730 - val_loss: 0.5368 - val_acc: 0.8177\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3624 - acc: 0.8724 - val_loss: 0.5356 - val_acc: 0.8150\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3608 - acc: 0.8737 - val_loss: 0.5380 - val_acc: 0.8170\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3594 - acc: 0.8738 - val_loss: 0.5411 - val_acc: 0.8150\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3580 - acc: 0.8745 - val_loss: 0.5376 - val_acc: 0.8153\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3566 - acc: 0.8755 - val_loss: 0.5410 - val_acc: 0.8157\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3550 - acc: 0.8752 - val_loss: 0.5412 - val_acc: 0.8100\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3535 - acc: 0.8761 - val_loss: 0.5389 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3522 - acc: 0.8772 - val_loss: 0.5401 - val_acc: 0.8127\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3505 - acc: 0.8771 - val_loss: 0.5415 - val_acc: 0.8150\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3494 - acc: 0.8774 - val_loss: 0.5412 - val_acc: 0.8133\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3477 - acc: 0.8785 - val_loss: 0.5428 - val_acc: 0.8140\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3465 - acc: 0.8789 - val_loss: 0.5447 - val_acc: 0.8143\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3449 - acc: 0.8796 - val_loss: 0.5441 - val_acc: 0.8120\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3438 - acc: 0.8792 - val_loss: 0.5456 - val_acc: 0.8113\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3425 - acc: 0.8811 - val_loss: 0.5459 - val_acc: 0.8137\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3411 - acc: 0.8813 - val_loss: 0.5473 - val_acc: 0.8120\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3398 - acc: 0.8811 - val_loss: 0.5509 - val_acc: 0.8137\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3391 - acc: 0.8810 - val_loss: 0.5502 - val_acc: 0.8120\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3370 - acc: 0.8825 - val_loss: 0.5512 - val_acc: 0.8117\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3358 - acc: 0.8831 - val_loss: 0.5498 - val_acc: 0.8107\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3347 - acc: 0.8828 - val_loss: 0.5541 - val_acc: 0.8100\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3337 - acc: 0.8839 - val_loss: 0.5521 - val_acc: 0.8087\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3319 - acc: 0.8844 - val_loss: 0.5528 - val_acc: 0.8103\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3314 - acc: 0.8848 - val_loss: 0.5544 - val_acc: 0.8103\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3300 - acc: 0.8844 - val_loss: 0.5534 - val_acc: 0.8113\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3285 - acc: 0.8854 - val_loss: 0.5555 - val_acc: 0.8110\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3273 - acc: 0.8860 - val_loss: 0.5600 - val_acc: 0.8107\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3260 - acc: 0.8868 - val_loss: 0.5589 - val_acc: 0.8103\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3250 - acc: 0.8870 - val_loss: 0.5584 - val_acc: 0.8127\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3239 - acc: 0.8872 - val_loss: 0.5601 - val_acc: 0.8093\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3226 - acc: 0.8885 - val_loss: 0.5598 - val_acc: 0.8107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3212 - acc: 0.8882 - val_loss: 0.5626 - val_acc: 0.8100\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3205 - acc: 0.8877 - val_loss: 0.5634 - val_acc: 0.8083\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3193 - acc: 0.8887 - val_loss: 0.5630 - val_acc: 0.8097\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 32us/step\n",
      "4000/4000 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31457229739246945, 0.8911818181818182]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5957522695064544, 0.80025]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
